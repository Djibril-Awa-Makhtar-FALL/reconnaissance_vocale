Statistique
La théorie et ses applications

Deuxième édition
avec exercices corrigés

Springer
Paris
Berlin
Heidelberg
New York
Hong Kong
Londres
Milan
Tokyo

Michel Lejeune

Statistique
La théorie et ses applications

Deuxième édition
avec exercices corrigés

Michel Lejeune
Professeur émérite
Université de Grenoble 2
IUT 2 département statistique
BP 47
38040 Grenoble cedex 9

ISBN : 978-2-8178-0156-8 Springer Paris Berlin Heidelberg New York

© Springer-Verlag France, Paris, 2010
Imprimé en France
Springer-Verlag France est membre du groupe Springer Science + Business
Media
Cet ouvrage est soumis au copyright. Tous droits réservés, notamment la reproduction et la représentation,
la traduction, la réimpression, l’exposé, la reproduction des illustrations et des tableaux, la transmission par
voie d’enregistrement sonore ou visuel, la reproduction par microfilm ou tout autre moyen ainsi que la
conservation des banques de données. La loi française sur le copyright du 9 septembre 1965 dans la version
en vigueur n’autorise une reproduction intégrale ou partielle que dans certains cas, et en principe moyennant
le paiement de droits. Toute représentation, reproduction, contrefaçon ou conservation dans une banque de
données par quelque procédé que ce soit est sanctionnée par la loi pénale sur le copyright.
L’utilisation dans cet ouvrage de désignations, dénominations commerciales, marques de fabrique, etc.
même sans spécification ne signifie pas que ces termes soient libres de la législation sur les marques de
fabrique et la protection des marques et qu’ils puissent être utilisés par chacun.
La maison d’édition décline toute responsabilité quant à l’exactitude des indications de dosage et des modes
d’emploi. Dans chaque cas, il incombe à l’usager de vérifier les informations données par comparaison à la
littérature existante.

Maquette de couverture : Jean-François Montmarché
c détail du tableau " Chrysler impressions " de Philippe Lejeune (1990)

Collection
Statistique et probabilités appliquées
dirigée par Yadolah Dodge
Professeur Honoraire
Université de Neuchâtel
Suisse
yadolah.dodge@unine.ch

Comité éditorial :
Christian Genest
Département de Mathématiques
et de statistique
Université Laval
Québec GIK 7P4
Canada

Stephan Morgenthaler
École Polytechnique Fédérale
de Lausanne
Département des Mathématiques
1015 Lausanne
Suisse

Marc Hallin
Université libre de Bruxelles
Campus de la Plaine CP 210
1050 Bruxelles
Belgique

Gilbert Saporta
Conservatoire national
des arts et métiers
292, rue Saint-Martin
75141 Paris Cedex 3
France

Ludovic Lebart
Télécom-ParisTech
46, rue Barrault
75634 Paris Cedex 13
France

Aurore Delaigle
Departement of Mathematics and statistics
Richard Berry Building
The university of Melbourne
VIC, 3010
Australia

Christian Mazza
Département de mathématiques
Université de Fribourg
Chemin du Musée 23
CH-1700 Fribourg
Suisse

Louis-Paul Rivest
Département de mathématiques et de statistique
Université Laval
1045, rue de la Médecine
Québec G1V OA6
Canada

Dans la même collection :
– Statistique. La théorie et ses applications
Michel Lejeune, avril 2004
– Optimisation appliquée
Yadolah Dodge, octobre 2004
– Le choix bayésien. Principes et pratique
Christian P. Robert, novembre 2005
– Maîtriser l’aléatoire. Exercices résolus de probabilités et statistique
Eva Cantoni, Philippe Huber, Elvezio Ronchetti, novembre 2006

– Régression. Théorie et applications
Pierre-André Cornillon, Éric Matzner-Løber, janvier 2007
– Le raisonnement bayésien. Modélisation et inférence
Éric Parent, Jacques Bernier, juillet 2007
– Premiers pas en simulation
Yadolah Dodge, Giuseppe Melfi, juin 2008
– Génétique statistique
Stephan Morgenthaler, juillet 2008
– Pratique du calcul bayésien
Jean-Jacques Boreux, Éric Parent, 2009
– Maîtriser l'aléatoire
Eva Cantoni, Philippe Huber, Elvezio Ronchetti, septembre 2009

À paraître :
– Le logiciel R
Pierre Lafaye de Micheaux, Rémi Drouilhet, Benoit Liquet, 2010

AVANT-PROPOS
L’objectif de cet ouvrage est de rendre accessibles les fondements théoriques
de la statistique à un public de niveau mathématique moyen : étudiants du
premier cycle des ﬁlières scientiﬁques, élèves ingénieurs, chercheurs dans les
domaines appliqués (économie, gestion, biologie, médecine, géographie, sciences
de la vie, psychologie. . . ) et, plus généralement, tous les chercheurs désireux
d’approfondir leur compréhension des résultats utilisés dans la pratique. Pour
ces derniers un minimum de connaissance de l’arrière-plan théorique apportera
une vision plus claire et plus critique des méthodes qu’ils emploient et permettra
d’éviter bien des écueils.
Les prérequis principaux sont la maı̂trise de la dérivation, de l’intégration
et de bases minimales du calcul des probabilités. Sur le plan purement mathématique, nous pensons que l’essentiel de l’exposé est accessible à quiconque
aurait parfaitement assimilé le programme d’un bac scientiﬁque. Il reste cependant quelques notions qui ne sont abordées qu’en premier cycle supérieur, notamment les approximations par développement de Taylor, les développements
en série entière, les fonctions de plusieurs variables (dérivation et intégration)
et, très marginalement, le calcul matriciel. Mais ces notions n’interviennent
le plus souvent que dans les aspects techniques de démonstration, ce qui ne
devrait pas nuire à la compréhension des concepts. Pour satisfaire la curiosité de mathématiciens qui voudraient, par la lecture de cet ouvrage, s’initier
sans peine à la science statistique, mention sera faite ici ou là de résultats ou
démonstrations exigeant des connaissances plus approfondies d’analyse. Ces
éléments seront consignés en petits caractères, généralement dans des «notes»
détachées que l’on pourra ignorer totalement. Quelques exercices plus diﬃciles,
repérés par un astérisque, leur sont également proposés.
Notons que les premiers chapitres concernent la théorie des probabilités qui,
toutefois, est abordée non comme une ﬁn en soi mais de façon simpliﬁée dans la
perspective de ce qui est nécessaire pour la théorie statistique de l’estimation
et des tests.
Pour atteindre l’objectif ﬁxé nous avons pris le parti de toujours privilégier
la facilité de compréhension au détriment éventuel de la pureté formelle (si
tant est qu’elle existe). Nous sommes d’avis que trop de formalisme nuit à
l’assimilation des concepts et qu’il faut s’eﬀorcer sans cesse de s’en tenir à un
niveau compatible avec celui des connaissances du public visé. Ceci a été un
souci constant dans la rédaction. Cela ne signiﬁe pas que nous ayons renoncé
à la rigueur du propos, c’est-à-dire à la cohérence des éléments apportés tout
au long de l’ouvrage.
Par ailleurs, nous faisons partie de ceux qui pensent que la statistique ne
relève pas uniquement de la mathématique qui n’est qu’un instrument. Sa

viii

Statistique - La théorie et ses applications

raison d’être consiste à appréhender le monde réel à partir des observations
que l’on en fait. C’est pourquoi la discipline est rangée dans le domaine des
mathématiques appliquées, ce terme ne devant pas, à notre sens, rester un vain
mot. Fidèle à cette vision nous avons tenté de commenter le plus largement
possible les concepts et résultats de façon concrète pour montrer leur utilité
dans l’approche du réel. Dans les chapitres débouchant immédiatement sur des
méthodes usuelles nous avons également introduit des exercices «appliqués»
pour illustrer l’intérêt et la mise en oeuvre des principes théoriques. L’ouvrage
n’est donc pas uniquement un traité mathématique. Cela a motivé le choix
de son sous-titre « La théorie et ses applications» pour marquer la distinction,
même si son objectif premier reste l’exposé de la théorie.
L’essentiel de l’apport de cette nouvelle édition est constitué des corrigés
détaillés des exercices proposés. Cette demande m’a été faite de façon récurrente
et il est vrai que ces corrigés doivent permettre d’améliorer nettement l’assimilation de la matière.
Je remercie mes collègues Alain Latour et Pierre Lafaye de Micheaux pour
leur aide technique précieuse ainsi qu’Alain Catalano, Yves-Alain Gerber,
Jérôme Hennet, Alexandre Junod, Julien Junod, Vincent Voirol et Mathieu
Vuilleumier pour leurs appréciations.
J’adresse des remerciements particuliers à Yadolah Dodge, directeur de cette
collection « Statistique et probabilités appliquées », sans les encouragements
duquel cet ouvrage n’aurait sans doute pas abouti.

Michel Lejeune

Grenoble, juin 2010

Table des matières
1 Variables aléatoires
1.1 Notion de variable aléatoire . . . . .
1.2 Fonction de répartition . . . . . . .
1.3 Cas des variables aléatoires discrètes
1.4 Cas des variables aléatoires continues
1.5 Notion essentielle de quantile . . . .
1.6 Fonction d’une variable aléatoire . .
1.7 Exercices . . . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

1
1
4
6
6
9
11
12

2 Espérance mathématique et moments
2.1 Introduction et déﬁnition . . . . . . . . . . . . . . . . . . . . .
2.2 Espérance d’une fonction d’une variable aléatoire . . . . . . . .
2.3 Linéarité de l’opérateur E(.), moments, variance . . . . . . . .
2.4 Tirage aléatoire dans une population ﬁnie : distribution empirique et distribution probabiliste . . . . . . . . . . . . . . . . .
2.5 Fonction génératrice des moments . . . . . . . . . . . . . . . .
2.6 Formules d’approximation de l’espérance et de la variance d’une
fonction d’une v.a. . . . . . . . . . . . . . . . . . . . . . . . . .
2.7 Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15
15
16
18

3 Couples et n-uplets de variables aléatoires
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . .
3.2 Couples de v.a. . . . . . . . . . . . . . . . . . . . .
3.3 Indépendance de deux variables aléatoires . . . . .
3.4 Espérance mathématique, covariance, corrélation .
3.5 Somme de deux v.a. . . . . . . . . . . . . . . . . .
3.6 Les n-uplets de v.a. ; somme de n v.a. . . . . . . .
3.7 Sondage aléatoire dans une population et v.a. i.i.d.
3.8 Notation matricielle des vecteurs aléatoires . . . .
3.9 Loi de Gauss multivariée . . . . . . . . . . . . . .
3.10 Exercices . . . . . . . . . . . . . . . . . . . . . . .

27
27
28
31
32
36
37
38
39
40
43

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

21
21
24
25

x

Statistique - La théorie et ses applications

4 Les lois de probabilités usuelles
4.1 Les lois discrètes . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.1 La loi uniforme discrète . . . . . . . . . . . . . . . . . .
4.1.2 Loi de Bernoulli B(p) . . . . . . . . . . . . . . . . . . .
4.1.3 Le processus de Bernoulli et la loi binomiale B(n, p) . .
4.1.4 Les lois géométrique G(p) et binomiale négative BN (r, p)
4.1.5 La loi hypergéométrique H(N, M, n) . . . . . . . . . . .
4.1.6 La loi multinomiale . . . . . . . . . . . . . . . . . . . .
4.1.7 Le processus et la loi de Poisson P(λ) . . . . . . . . . .
4.2 Les lois continues . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.1 La loi continue uniforme U[a, b] . . . . . . . . . . . . .
4.2.2 La loi exponentielle E(λ) . . . . . . . . . . . . . . . . .
4.2.3 La loi gamma Γ(r, λ) . . . . . . . . . . . . . . . . . . .
4.2.4 La loi de Gauss ou loi normale N (μ, σ2 ) . . . . . . . .
4.2.5 La loi lognormale LN (μ, σ 2 ) . . . . . . . . . . . . . . .
4.2.6 La loi de Pareto . . . . . . . . . . . . . . . . . . . . . .
4.2.7 La loi de Weibull W (λ, α) . . . . . . . . . . . . . . . .
4.2.8 La loi de Gumbel . . . . . . . . . . . . . . . . . . . . .
4.2.9 La loi bêta Beta(α, β) . . . . . . . . . . . . . . . . . . .
4.3 Génération de nombres issus d’une loi donnée . . . . . . . . . .
4.4 Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

45
45
45
46
47
49
50
51
51
54
54
55
56
57
60
61
61
62
63
63
64

5 Lois fondamentales de l’échantillonnage
5.1 Phénomènes et échantillons aléatoires . . . . . .
5.2 Moyenne, variance, moments empiriques . . . . .
5.3 Loi du Khi-deux . . . . . . . . . . . . . . . . . .
5.4 Loi de Student . . . . . . . . . . . . . . . . . . .
5.5 Loi de Fisher-Snedecor . . . . . . . . . . . . . .
5.6 Statistiques d’ordre . . . . . . . . . . . . . . . .
5.7 Fonction de répartition empirique . . . . . . . .
5.8 Convergence, approximations gaussiennes, grands
5.8.1 Les modes de convergence aléatoires . . .
5.8.2 Lois des grands nombres . . . . . . . . .
5.8.3 Le théorème central limite . . . . . . . .
5.9 Exercices . . . . . . . . . . . . . . . . . . . . . .

67
67
69
72
74
76
77
78
79
79
81
82
86

. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
échantillons
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.

6 Théorie de l’estimation paramétrique ponctuelle
91
6.1 Cadre général de l’estimation . . . . . . . . . . . . . . . . . . .
91
6.2 Cadre de l’estimation paramétrique . . . . . . . . . . . . . . . .
92
6.3 La classe exponentielle de lois . . . . . . . . . . . . . . . . . .
94
6.4 Une approche intuitive de l’estimation : la méthode des moments 96
6.5 Qualités des estimateurs . . . . . . . . . . . . . . . . . . . . . .
98
6.5.1 Biais d’un estimateur . . . . . . . . . . . . . . . . . . .
99
6.5.2 Variance et erreur quadratique moyenne d’un estimateur 100
6.5.3 Convergence d’un estimateur . . . . . . . . . . . . . . . 103

Table des matières

6.6

6.7

6.8
6.9

6.5.4 Exhaustivité d’un estimateur . . . . . . . . . . . . .
Recherche des meilleurs estimateurs sans biais . . . . . . .
6.6.1 Estimateurs UMVUE . . . . . . . . . . . . . . . . .
6.6.2 Estimation d’une fonction de θ et reparamétrisation
6.6.3 Borne de Cramer-Rao et estimateurs eﬃcaces . . .
6.6.4 Extension à un paramètre de dimension k > 1 . . .
L’estimation par la méthode du maximum de vraisemblance
6.7.1 Déﬁnitions . . . . . . . . . . . . . . . . . . . . . . .
6.7.2 Exemples et propriétés . . . . . . . . . . . . . . . . .
6.7.3 Reparamétrisation et fonctions du paramètre . . . .
6.7.4 Comportement asymptotique de l’EMV . . . . . . .
Les estimateurs bayésiens . . . . . . . . . . . . . . . . . . .
Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.

xi
.
.
.
.
.
.
.
.
.
.
.
.
.

105
110
110
114
114
118
121
122
123
126
127
128
131

7 Estimation paramétrique par intervalle de conﬁance
135
7.1 Déﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
7.2 Méthode de la fonction pivot . . . . . . . . . . . . . . . . . . . 138
7.3 Méthode asymptotique . . . . . . . . . . . . . . . . . . . . . . 140
7.4 Construction des IC classiques . . . . . . . . . . . . . . . . . . 144
7.4.1 IC pour la moyenne d’une loi N (μ, σ 2 ) . . . . . . . . . . 144
7.4.2 IC pour la variance σ 2 d’une loi de Gauss . . . . . . . . 146
7.4.3 IC sur la diﬀérence des moyennes de deux lois de Gauss 147
7.4.4 IC sur le rapport des variances de deux lois de Gauss . 149
7.4.5 IC sur le paramètre p d’une loi de Bernoulli . . . . . . 150
7.4.6 IC sur la diﬀérence des paramètres de deux lois de Bernoulli152
7.5 IC par la méthode des quantiles . . . . . . . . . . . . . . . . . 153
7.6 Approche bayésienne . . . . . . . . . . . . . . . . . . . . . . . . 157
7.7 Notions d’optimalité des IC . . . . . . . . . . . . . . . . . . . . 158
7.8 Région de conﬁance pour un paramètre de dimension k > 1 . . 159
7.9 Intervalles de conﬁance et tests . . . . . . . . . . . . . . . . . . 163
7.10 Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
8 Estimation non paramétrique et estimation fonctionnelle
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.2 Estimation de la moyenne et de la variance de la loi . . . .
8.2.1 Estimation de la moyenne μ . . . . . . . . . . . . . .
8.2.2 Estimation de la variance σ 2 . . . . . . . . . . . . .
8.3 Estimation d’un quantile . . . . . . . . . . . . . . . . . . .
8.4 Les méthodes de rééchantillonnage . . . . . . . . . . . . . .
8.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . .
8.4.2 La méthode du jackknife . . . . . . . . . . . . . . .
8.4.3 La méthode du bootstrap . . . . . . . . . . . . . . .
8.5 Estimation fonctionnelle . . . . . . . . . . . . . . . . . . . .
8.5.1 Introduction . . . . . . . . . . . . . . . . . . . . . .
8.5.2 L’estimation de la densité . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

167
167
168
168
169
170
172
172
173
177
181
181
182

xii

Statistique - La théorie et ses applications

8.6

8.5.3 L’estimation de la fonction de répartition . . . . . . . .
Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

192
198

9 Tests d’hypothèses paramétriques
201
9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
9.2 Test d’une hypothèse simple avec alternative simple . . . . . . 202
9.3 Test du rapport de vraisemblance simple . . . . . . . . . . . . 208
9.3.1 Propriété d’optimalité . . . . . . . . . . . . . . . . . . . 208
9.3.2 Cas d’un paramètre de dimension 1 . . . . . . . . . . . 212
9.4 Tests d’hypothèses multiples . . . . . . . . . . . . . . . . . . . 213
9.4.1 Risques, puissance et optimalité . . . . . . . . . . . . . 213
9.4.2 Tests d’hypothèses multiples unilatérales . . . . . . . . 214
9.4.3 Tests d’hypothèses bilatérales . . . . . . . . . . . . . . 219
9.5 Test du rapport de vraisemblance généralisé . . . . . . . . . . 220
9.6 Remarques diverses . . . . . . . . . . . . . . . . . . . . . . . . . 226
9.7 Les tests paramétriques usuels . . . . . . . . . . . . . . . . . . . 228
9.7.1 Tests sur la moyenne d’une loi N (μ, σ2 ) . . . . . . . . . 229
9.7.2 Test sur la variance σ 2 d’une loi N (μ, σ 2 ) . . . . . . . . 231
9.7.3 Tests de comparaison des moyennes de deux lois de Gauss 232
9.7.4 Tests de comparaison des variances de deux lois de Gauss 235
9.7.5 Tests sur le paramètre p d’une loi de Bernoulli (ou test
sur une proportion) . . . . . . . . . . . . . . . . . . . . 235
9.7.6 Tests de comparaison des paramètres de deux lois de
Bernoulli (comparaison de deux proportions) . . . . . . 237
9.7.7 Test sur la corrélation dans un couple gaussien . . . . . 240
9.8 Dualité entre tests et intervalles de conﬁance . . . . . . . . . . 242
9.9 Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
10 Tests pour variables catégorielles et tests non paramétriques
10.1 Test sur les paramètres d’une loi multinomiale . . . . . . . . .
10.1.1 Test du rapport de vraisemblance généralisé . . . . . .
10.1.2 Test du khi-deux de Pearson . . . . . . . . . . . . . . .
10.1.3 Équivalence asymptotique des deux tests . . . . . . . .
10.1.4 Cas particulier de la loi binomiale . . . . . . . . . . . .
10.2 Test de comparaison de plusieurs lois multinomiales . . . . . .
10.3 Test d’indépendance de deux variables catégorielles . . . . . . .
10.3.1 Test du RVG et test du khi-deux . . . . . . . . . . . .
10.3.2 Test exact de Fisher (tableau 2 × 2) . . . . . . . . . . .
10.4 Tests d’ajustement à un modèle de loi . . . . . . . . . . . . . .
10.4.1 Ajustement à une loi parfaitement spéciﬁée . . . . . . .
10.4.2 Ajustement dans une famille paramétrique donnée . . .
10.5 Tests non paramétriques sur des caractéristiques de lois . . . .
10.5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . .
10.5.2 Les statistiques de rang . . . . . . . . . . . . . . . . . .
10.5.3 Tests sur moyenne, médiane et quantiles . . . . . . . . .

251
252
252
254
255
256
257
259
259
262
264
265
267
272
272
272
273

Table des matières

xiii

10.5.4 Tests de localisation de deux lois . . . . . . . . . . . . .
10.5.5 Test pour la corrélation de Spearman . . . . . . . . . .
10.6 Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11 Régressions linéaire, logistique et non paramétrique
11.1 Introduction à la régression . . . . . . . . . . . . . . .
11.2 La régression linéaire . . . . . . . . . . . . . . . . . .
11.2.1 Le modèle . . . . . . . . . . . . . . . . . . . . .
11.2.2 Les estimateurs du maximum de vraisemblance
11.2.3 Intervalles de conﬁance . . . . . . . . . . . . .
11.2.4 Test H0 : β1 = 0 . . . . . . . . . . . . . . . . .
11.2.5 Cas non gaussien . . . . . . . . . . . . . . . . .
11.2.6 Régression et corrélation linéaires . . . . . . .
11.2.7 Extension à la régression multiple . . . . . . .
11.3 La régression logistique . . . . . . . . . . . . . . . . .
11.3.1 Le modèle . . . . . . . . . . . . . . . . . . . . .
11.3.2 Estimation de la fonction p(x) . . . . . . . . .
 . . . . .
11.3.3 Matrice des variances-covariances de β
11.3.4 Test H0 : β1 = 0 . . . . . . . . . . . . . . . . .
11.3.5 Intervalles de conﬁance . . . . . . . . . . . . .
11.3.6 Remarques diverses . . . . . . . . . . . . . . .
11.4 La régression non paramétrique . . . . . . . . . . . .
11.4.1 Introduction . . . . . . . . . . . . . . . . . . .
11.4.2 Déﬁnition des estimateurs à noyaux . . . . . .
11.4.3 Biais et variance . . . . . . . . . . . . . . . . .
11.4.4 La régression polynomiale locale . . . . . . . .
11.5 Exercices . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

274
281
283
289
289
292
292
293
296
297
299
300
303
305
305
306
308
309
310
312
314
314
314
315
318
320

Corrigés des exercices

323

Tables
Bibliographie
Index

415
421
425

Chapitre 1

Variables aléatoires
1.1

Notion de variable aléatoire

La théorie des probabilités a pour objet l’étude des phénomènes aléatoires
ou du moins considérés comme tels par l’observateur. Pour cela on introduit le concept d’expérience1 aléatoire dont l’ensemble des résultats possibles
constitue l’ensemble fondamental, noté habituellement Ω. On parle de variable
aléatoire (abréviation : v.a.) lorsque les résultats sont numériques, c’est-à-dire
que Ω est identique à tout ou partie de l’ensemble des nombres réels R.
On distingue habituellement :
- les variables aléatoires discrètes pour lesquelles l’ensemble Ω des résultats
possibles est un ensemble discret de valeurs numériques x1 , x2 , . . . , xn , . . .
ﬁni ou inﬁni (typiquement : l’ensemble des entiers naturels) ;
- les variables aléatoires continues pour lesquelles l’ensemble Ω est tout R (ou
un intervalle de R ou, plus rarement, une union d’intervalles).
On peut concevoir des variables mixtes, mais nous ne traiterons pas, sauf
exception, ces cas particuliers.
Dans toute expérience aléatoire on est amené à s’intéresser à des ensembles
de résultats, donc des parties de Ω, que l’on appelle événements, les résultats
formant eux-mêmes des événements élémentaires. Dans le cas d’une v.a. les
événements sont des parties de R, le plus souvent des intervalles. Par exemple
on s’intéressera au fait qu’un assuré occasionne un sinistre de coût supérieur à
1000 euros au cours d’une année.
Dès lors il reste à construire un modèle probabiliste pour l’ensemble fondamental considéré. Ceci ne pose pas de problème pour le cas d’une v.a. discrète.
En eﬀet il suﬃt de déﬁnir les probabilités de chaque résultat x1 , x2 , . . . , xn , . . . ,
1 Le terme est trop restrictif pour prendre en compte la variété des phénomènes étudiés.
On trouvera une brève discussion à ce propos au début du chapitre 5.

2

Statistique − La théorie et ses applications

à partir de quoi on peut, par les règles élémentaires des probabilités, calculer la
probabilité de tout événement (en sommant celles des résultats appartenant à
l’événement). De plus toute partie de Ω est un événement. C’est la présentation
que l’on trouve généralement dans les traités élémentaires.
Pour une v.a.continue les choses sont plus délicates. En eﬀet un point de R
est un intervalle de longueur nulle et la probabilité associée à tout point est ellemême nulle. On ne peut donc «probabiliser » R à partir de probabilités associées
à chacun de ses éléments. En fait les probabilités doivent être attribuées aux
événements. De plus, contrairement au cas discret, l’ensemble des parties de R
est trop vaste pour constituer un ensemble d’événements tous probabilisables
et l’on doit se restreindre à certaines parties (voir plus loin la note 1.2). Ceci n’a
toutefois aucune incidence sur le plan pratique tant il est vrai que les parties
de R qui sont exclues ne sont que des curiosités mathématiques. Par souci
d’homogénéité, dans le cas discret on considère la probabilisation de Ω à partir
de l’ensemble E des événements, comme dans le cas continu.
Soit, donc, l’ensemble E des événements construit à partir de Ω, on appelle
mesure de probabilité une fonction P qui à tout événement E fait correspondre
un nombre P (E) entre 0 et 1 que l’on appellera la probabilité de l’événement E
(cette fonction doit en outre vériﬁer certains axiomes, voir ci-après). Pour une
variable aléatoire on parlera plutôt de la loi de la variable aléatoire ou encore,
de sa distribution, par emprunt à la statistique descriptive.
Par commodité on désigne une variable aléatoire par une lettre majuscule
symbolique et on écrit simplement un événement sous la forme usuelle des
notations mathématiques. Ainsi, si X désigne la variable aléatoire «durée de vie
en années d’un aspirateur donné», (X < 3) dénotera l’événement «l’aspirateur
a une durée de vie inférieure à 3 ans». La probabilité associée à cet événement
pourra s’écrire P (X < 3). Cette commodité pourra parfois prêter à confusion
et il sera toujours utile de garder à l’esprit son caractère conventionnel. Ainsi
dans notre exemple P (X < 3) n’est rien d’autre que la mesure de probabilité
associée à l’intervalle ]−∞, 3[, soit P ( ]−∞, 3[) . Dans sa forme la plus générale
un événement pourra s’écrire (X ∈ A) où A est une partie de R.
Rappelons succinctement les principales propriétés d’une mesure de probabilité.
1. P (E) ∈ [0, 1] pour tout événement E et P () = 1
2. P (E) = 1 − P (E), E étant le complémentaire de E
3. P (E1 ∪ E2 ) = P (E1 ) + P (E2 ), pour tous événements E1 et E2 incompatibles (i.e. parties disjointes de Ω : E1 ∩ E2 = ∅ )
4. P (E1 ∪ E2 ) = P (E1 ) + P (E2 ) − P (E1 ∩ E2 ) dans le cas général
5. E1 ⊆ E2 (E1 inclus dans E2 ) =⇒ P (E1 ) ≤ P (E2 )

Chapitre 1. Variables aléatoires

3

6. La probabilité conditionnelle de E1 sachant E2 (pour autant que l’on ait
P (E2 ) = 0) est :
P (E1 |E2 ) =

P (E1 ∩ E2 )
P (E2 )

7. Les événements E1 et E2 de probabilités non nulles sont indépendants
si et seulement si :
P (E1 ∩ E2 ) = P (E1 )P (E2 )
ou, de façon équivalente quand P (E2 ) = 0, P (E1 |E2 ) = P (E1 ).
La propriété 1 et la propriété 3 généralisée à une suite E1 , E2 , . . . , En , . . .
d’événements deux à deux incompatibles constituent les axiomes de la théorie des probabilités. La propriété 7 s’étend à une suite d’événements de la
façon suivante : on dit que les événements E1 , E2 , . . . , En , . . . sont (mutuellement) indépendants si, pour tout sous-ensemble de ces événements, la probabilité de leur intersection est égale au produit de leurs probabilités (donc la
relation doit être vériﬁée pour les événements pris deux à deux, trois à trois,
etc.).
Note 1.1 Plus formellement et pour être plus général, on déﬁnit une v.a. en
partant d’une expérience aléatoire dont l’ensemble fondamental peut être de
nature quelconque. C’est pour cet ensemble fondamental qu’est déﬁnie la
mesure de probabilité pour former un espace probabilisé. Une v.a. devient alors
une fonction de Ω dans R, qui aﬀecte donc à chaque résultat possible une
valeur numérique. Par exemple, si l’expérience aléatoire consiste à tirer au
hasard un individu dans une population, l’ensemble des résultats possibles Ω est
l’ensemble des individus de la population. A partir de là on peut observer l’âge
de l’individu. Déﬁnissant ainsi la v.a. X «âge d’un individu tiré au hasard dans
la population» on obtient la probabilité, disons, de l’événement (18 ≤ X ≤ 20)
en calculant sur Ω la probabilité de tirer un individu dont l’âge est compris
dans cet intervalle. Plus généralement à tout événement E ⊆ R sur X on
attribue la probabilité de l’événement X −1 (E) = {ω ∈ Ω | X(ω) ∈ E} de
l’espace probabilisé initial (cet événement correspond à l’ensemble des résultats
possibles ω dans Ω qui conduisent par la fonction X à une valeur appartenant à
E). Pour des fonctions X extrêmement singulières il se pourrait que X −1 (E) ne
soit pas un événement probabilisable. On ne considèrera donc que des fonctions
mesurables, c’est-à-dire telles qu’une probabilité puisse être aﬀectée à tout E.
En pratique toutes les fonctions utilisées sont mesurables et nous ignorerons ce
problème dans cet ouvrage.
Ce formalisme n’a d’intérêt que s’il pré-existe, en amont du phénomène
numérique observé, des événements de probabilités connues ou facilement calculables. C’est ainsi dans notre exemple : tous les individus ont la même probabilité d’être tirés, égale à 1/N où N est le nombre total d’individus. Alors la

4

Statistique − La théorie et ses applications

probabilité d’un événement E pour X est 1/N fois le nombre d’individus dont
l’âge est dans E, donc la proportion d’individus dont l’âge est dans E .
Illustrons encore cela par un autre exemple fondé sur le jeu de cartes du
bridge. Un joueur donné reçoit 13 cartesparmi
 les 52 cartes au total. Les cartes
combinaisons de 13 cartes sont a
étant distribuées au hasard toutes les 52
13
priori équiprobables. Pour évaluer son «jeu» un joueur utilise le système de
points classique suivant : un as vaut 4 points, un roi 3, une dame 2 et un valet
1. Ainsi on déﬁnit une v.a. X «nombre de points dans son jeu». Pour calculer
P (X = 1), par exemple, il suﬃt (moyennant une bonne maı̂trise de l’analyse
combinatoire !) de dénombrer les combinaisons de
 13 cartes ayant un seul valet
). La probabilité est alors
et ni as ni roi ni dame (il y en a un nombre 41 36
12
égale au nombre de ces jeux divisé par le nombre total de combinaisons. On voit
comment, dans un tel cas, pour trouver la loi de X il est nécessaire de remonter
à l’expérience initiale du tirage au hasard de 13 cartes à laquelle s’applique de
façon réaliste le modèle d’équiprobabilité.

1.2

Fonction de répartition

La fonction de répartition est l’instrument de référence pour déﬁnir de façon
uniﬁée la loi de probabilité d’une variable aléatoire qu’elle soit discrète ou
continue. Si cette fonction est connue, il est possible de calculer la probabilité
de tout intervalle et donc, en pratique, de tout événement. C’est pourquoi c’est
elle qui est donnée dans les tables des lois de probabilité.

Déﬁnition 1.1 Soit X une variable aléatoire, on appelle fonction de répartition de X, que l’on note FX , la fonction déﬁnie sur R par :
FX (x) = P (X ≤ x).
La valeur prise par la fonction de répartition au point x est donc la probabilité de l’événement ] − ∞, x]. En anglais on l’appelle «cumulative distribution
function» par analogie avec la notion de fréquence cumulée en statistique descriptive.
Note 1.2 La fonction de répartition est déﬁnie pour tout x ∈ R. La question
se pose de savoir si la connaissance de FX , donc des probabilités de tous les
événements de la forme ] − ∞, x], suﬃt pour déterminer la probabilité d’un
événement quelconque.
Pour une v.a. discrète, il est clair que par soustraction on peut déterminer
la probabilité de chaque valeur possible et, à partir de là, de toutes les parties
de Ω par simple sommation. Toutefois dans les traités élémentaires où ne sont
abordées que les v.a. discrètes, l’utilisation de la fonction de répartition n’est
pas nécessaire, puisque l’on peut se contenter des probabilités individuelles.

Chapitre 1. Variables aléatoires

5

Pour les v.a. continues, comme il a été brièvement indiqué plus haut, on ne
peut déﬁnir une mesure de probabilité sur toutes les parties de R qui satisfasse
aux axiomes de la théorie. On est conduit à se restreindre aux événements
appartenant à la tribu borélienne de R. Cette tribu est l’ensemble des parties
de R engendrées par les unions, intersections et compléments d’événements
(éventuellement en suite inﬁnie) de la forme (X ≤ x). On comprend ainsi
que FX permette, en principe, de calculer la probabilité de tout événement. La
restriction à la tribu borélienne de R n’est pas contraignante car elle contient en
fait toutes parties concevables de R (points isolés, intervalles ouverts ou fermés,
unions de tels intervalles, etc.). A vrai dire il faut faire preuve de beaucoup
d’ingéniosité pour mettre en évidence une partie de R n’appartenant pas à la
tribu borélienne et nous n’aurons pas à nous préoccuper en pratique de cette
restriction (tout comme il a été dit dans la note 1.1 qu’on ne se préoccuperait
pas de vériﬁer si une fonction est mesurable).
Propriétés
1. FX est non décroissante puisque, pour h > 0, (X ≤ x) ⊂ (X ≤ x+h) et
donc P (X ≤ x) ≤ P (X ≤ x + h).
2. FX (x) varie de 0 à 1 quand x varie de −∞ à +∞, sachant que FX (x) est
une probabilité cumulée à partir de −∞. On écrira, en bref, FX (−∞) =
0 et FX (+∞) = 1 .
3. FX est continue à droite en tout x et FX (x) − FX (x− ) = P (X = x),
où FX (x− ) dénote la limite à gauche au point x.
Montrons succinctement cette dernière propriété qui, comme nous allons le
voir, résulte du fait que l’événement (X ≤ x) intervenant dans la déﬁnition
de FX (x) inclut la valeur x elle-même (pour des éléments de démonstration
plus rigoureux des propriétés énoncées ici, voir les exercices proposés en ﬁn de
chapitre). Par déﬁnition, on a :
FX (x− ) =

lim

ε→0,ε>0

FX (x − ε) =

lim

ε→0,ε>0

P (X ≤ x − ε) .

Comme tout événement (X ≤ x − ε) ne contient pas x on admettra qu’au
passage à la limite on obtient FX (x− ) = P (X < x). On a également :
FX (x+ ) =

lim

ε→0,ε>0

FX (x + ε) =

lim

ε→0,ε>0

P (X ≤ x + ε) ,

mais ici (X ≤ x + ε) contient toujours x et, donc, au passage à la limite on
obtient FX (x+ ) = P (X ≤ x) = FX (x). Comme les événements (X < x) et
(X = x) sont incompatibles, on en déduit :
FX (x) = FX (x− ) + P (X = x).

Statistique − La théorie et ses applications

6

En résumé, si la valeur x considérée reçoit une probabilité non nulle (cas
discret), alors il y a un saut de discontinuité à gauche d’amplitude égale à cette
probabilité, sinon FX est également continue à gauche et donc continue en x.
Nous revenons sur ces notions dans les cas particuliers des variables aléatoires
discrètes et des variables aléatoires continues.

1.3

Cas des variables aléatoires discrètes

Pour une variable aléatoire discrète X, l’ensemble des valeurs possibles est
un ensemble discret, ﬁni ou inﬁni, de points que nous noterons en ordre croissant : x1 < x2 < · · · < xi < · · · , sans préciser si l’on est dans le cas ﬁni ou
dans le cas inﬁni.
En vertu de ce qui vient d’être vu, la fonction de répartition reste constante
entre deux valeurs possibles et présente un saut de discontinuité dès qu’on
arrive sur une valeur xi . En xi le saut est égal à la probabilité associée à ce
point. Immédiatement à gauche de xi la fonction est égale à FX (xi−1 ), en xi et
à droite elle est égale à FX (xi ) (continuité à droite). Cette fonction en escalier
s’avère peu maniable et il est plus simple, pour déﬁnir la loi de X, de recourir à
sa fonction de probabilité pX (appelée aussi fonction de masse de probabilité)
qui pour tout xi (i = 1, 2, . . .) donne directement sa probabilité pX (xi ) .
Prenons l’exemple du nombre d’appels X arrivant à un standard téléphonique
au cours d’une minute, pour lequel un modèle de loi de Poisson de moyenne
10 serait approprié (voir cette loi en section 4.1.7). La variable aléatoire X est
déﬁnie par :
valeurs possibles
probabilités associées

0

1

e−10

10e−10

2
102 e−10
2

···
···

k
10k e−10
k!

···
···

ce qui donne le diagramme en bâtonnets et la fonction de répartition de la
ﬁgure 1.1.
Le passage de la fonction de répartition à la fonction de probabilité et
inversement est :
pX (xi ) = FX (xi ) − FX (x−
i ) = FX (xi ) − FX (xi−1 )

FX (x) =
pX (xi )
i | xi ≤x

1.4

Cas des variables aléatoires continues

Formellement on dira qu’une variable aléatoire X est continue s’il existe une
fonction fX non négative telle que, pour tout x ∈ R, la fonction de répartition

Chapitre 1. Variables aléatoires

0, 14

7

p(x)

0, 12
0, 1
0, 08
0, 06
0, 04
0, 02
x

0
0

1
0, 9
0, 8
0, 7
0, 6
0, 5
0, 4
0, 3
0, 2
0, 1
0

5

10

15

20

25

F( x)

x
0

5

10

15

20

25

Figure 1.1 - Fonction de probabilité et fonction de répartition de la loi de
Poisson de moyenne 10.

8

Statistique − La théorie et ses applications

puisse s’écrire


FX (x) =

x
−∞

fX (u)du .

La fonction fX est alors appelée fonction de densité de probabilité de X
ou simplement densité de X. Le fait que FX s’exprime comme une intégrale
implique qu’elle est continue partout et, par conséquent, pour tout x on a
P (X = x) = FX (x) − FX (x− ) = 0. Plus concrètement, chaque point de la
droite réelle est immatériel en tant qu’intervalle de longueur nulle et a une
probabilité nulle en tant qu’événement, mais peut être caractérisé par une densité de probabilité en ce point. Les événements d’intérêt seront généralement
des intervalles et pour ceux-ci il sera indiﬀérent d’y inclure ou non les
bornes.
La fonction de répartition devient alors particulièrement appropriée pour
calculer la probabilité de tout intervalle [a, b]. En eﬀet, comme on a :
(X ≤ b) = (X ≤ a) ∪ (a < X ≤ b) ,
les deux événements à droite étant incompatibles et les signes < pouvant être
remplacés par ≤, il s’ensuit que :
P (X ≤ b) = P (X ≤ a) + P (a ≤ X ≤ b)
P (a ≤ X ≤ b) = P (X ≤ b) − P (X ≤ a)
d’où, par la déﬁnition même de FX , les formules fondamentales :
P (a ≤ X ≤ b) = FX (b) − FX (a)
 b
fX (u)du .
=
a

On remarquera au passage que, dans le cas discret, les formules se compliquent
car selon qu’on inclut ou non les bornes de l’intervalle il faut introduire FX (b− )
et FX (a− ).
On admettra que, plus généralement, pour tout événement (X ∈ A) on a :

fX (x)dx.
P (X ∈ A) =
A

En général FX sera dérivable partout sauf peut-être en quelques points
qui seront des points de discontinuité pour fX (d’un point de vue purement
mathématique FX existerait même si fX était discontinue sur un ensemble
dénombrable de points). En un point x où elle est dérivable prenons un intervalle de longueur h centré sur x. La probabilité associée à cet intervalle est
alors P (x − h2 < X < x + h2 ) = FX (x + h2 ) − FX (x − h2 ), d’où :

Chapitre 1. Variables aléatoires

lim

h→0

P (x −

h
2

9

< X < x + h2 )

= FX (x) = fX (x) ,
h

ce qui justiﬁe l’appellation de densité de probabilité.
Bien que d’un point de vue pratique, pour les modèles de lois continues,
ce soit FX qui soit utile - et c’est bien elle qui est donnée dans les tables la représentation graphique de fX est plus parlante car elle met en évidence
les zones à plus forte probabilité. Chacun sait interpréter intuitivement, par
exemple, la fameuse courbe en cloche du modèle de la loi de Gauss.
A titre illustratif, considérons le jeu de loterie où l’on fait tourner une ﬂèche
sur un cadran avec une zone gagnante, et soit la variable X correspondant à
l’angle de la ﬂèche, par rapport à une origine déterminée, après expérience.
S’il n’y a pas de direction privilégiée la densité de probabilité est la même
partout, c’est-à-dire sur l’intervalle [0 , 360], et X suit une loi continue uniforme
(voir section 4.2.1) sur celui-ci. Les graphes de la densité et de la fonction
de répartition sont donnés en ﬁgure 1.2. La probabilité d’un intervalle [a, b],
correspondant à la surface sous la densité, y est mise en évidence. Notons que
FX est dérivable partout sauf aux bornes du support de fX (on appelle support
de fX l’ensemble des valeurs où elle n’est pas nulle).
Outre qu’elle est une fonction non négative, la densité a les propriétés suivantes :
 +∞
fX (x)dx = 1 ,
−∞

lim fX (x) = 0 ,

x→±∞

la première inégalité découlant du fait que l’intégrale vaut FX (+∞) − FX (−∞)
(voir la propriété n◦ 2 en section 1.2), la deuxième étant nécessaire (mais non
suﬃsante) pour que l’intégrale converge aux deux bornes.

Note 1.3 Lorsqu’on aborde la théorie des probabilités par la théorie de la mesure,
il n’y a pas lieu de faire de distinction entre variables discrètes et variables continues,
et donc entre pX et fX . Dans les deux cas il s’agit d’une densité par rapport à la
mesure générée par FX .

1.5

Notion essentielle de quantile

Déﬁnition 1.2 On appelle quantile d’ordre q de la variable X, où q ∈ [0, 1],
la valeur xq telle que P (X ≤ xq ) = q ou, de même, FX (xq ) = q.

Statistique − La théorie et ses applications

10

f(x)
0,004

P(a<X<b)

0,003

0,002

0,001
x

0
-100

0

a

100

200

b

300

400

100

200

b

300

x
400

F(x)
1

P(a<X<b)

0
-100

0

a

Figure 1.2 - Fonction de densité et fonction de répartition de la loi continue
uniforme sur [0, 1].

Chapitre 1. Variables aléatoires

11

La notion de quantile (appelée aussi fractile, ou percentile si exprimée en
pour cent) est directement liée à celle de fonction de répartition. Toute valeur
de R peut être vue comme un quantile d’un certain ordre. Cette notion de
quantile est essentielle du fait que l’ordre d’un quantile permet de positionner
la valeur correspondante sur la distribution considérée. Ainsi le quantile d’ordre
0,5 , alias la médiane, est une sorte de centre ou milieu de la distribution. En
statistique apparaı̂tra la nécessité de ﬁxer des limites de plausibilité pour les
valeurs d’une loi donnée et l’usage est de prendre pour cela les quantiles x0,025
et x0,975 , soit des valeurs à l’intérieur desquelles la v.a. a une probabilité 0,95
de se trouver.
Dans le cas continu, à tout ordre q ∈ [0, 1] correspond une valeur xq du
fait de la continuité de FX . Généralement FX est strictement croissante sur
l’ensemble des valeurs de x où 0 < FX (x) < 1 et xq est donc unique pour q
∈]0, 1[.
Dans le cas discret, nous avons vu que FX est une fonction en escalier et
il peut donc y avoir tout un intervalle de valeurs possibles si q correspond au
niveau d’une marche de FX , ou aucune valeur si q est entre deux marches. En
pratique on convient de prendre la valeur la plus faible dans le premier cas et
d’interpoler linéairement entre les deux valeurs possibles xi et xi+1 telles que
FX (xi ) < q et FX (xi+1 ) > q dans le deuxième cas.

1.6

Fonction d’une variable aléatoire

Le problème du passage de la loi d’une v.a. X à la loi d’une fonction
Z = g(X) de celle-ci est fréquent. Considérons, par exemple, la v.a. X exprimant la consommation d’une automobile en litres aux 100 kilomètres. A une
consommation de x litres/100 km correspond aux Etats-Unis une consommation z = 235/x «miles per gallon» (nombre de miles parcourus avec un gallon
d’essence). Ainsi la v.a. X devient une v.a. Z = 235/X.
Dans le cas continu la détermination de la loi de la nouvelle v.a. Z passe par
sa fonction de répartition FZ (z) = P (Z ≤ z) naturellement déﬁnie pour tout
z ∈ R par la probabilité, pour X, associée à l’ensemble des valeurs x telles que
g(x) ∈ ] − ∞, z]. En utilisant la symbolique des événements il suﬃt de résoudre
l’événement (Z ≤ z) en terme d’événement pour X.
Note 1.4 Rigoureusement, pour que les probabilités des événements sur Z soient
calculables il faut que la fonction g soit mesurable (voir note 1.1), c’est-à-dire que
pour tout événement E pour Z (donc borélien de R, voir note 1.2) g −1 (E) soit
un événement pour X (donc un borélien de R). Les fonctions non mesurables ne se
rencontrent pas en pratique.
Exemple 1.1 Montrons une fonction strictement croissante, une fonction non
monotone et une fonction strictement décroissante.

Statistique − La théorie et ses applications

12

a) Soit Z = 2X + 3 :

FZ (z) = P (Z ≤ z) = P (2X + 3 ≤ z) = P

z−3
X≤
2




= FX

z−3
2


.

b) Soit T = X 2 :
FT (t) = P (X 2 ≤ t)
√
√
√
√
P (− t ≤ X ≤ t) = FX ( t) − FX (− t) si t > 0
=
.
0 si t ≤ 0
c) Soit U = c/X où c > 0 et X est à valeurs dans ]0, +∞[ . Pour u > 0 on a :
FU (u) = P (U ≤ u) = P

c
c
c
≤u =P X≥
= 1 − FX
X
u
u

et FU (u) = 0 pour u ≤ 0.



Si g est strictement croissante comme dans le cas a) ci-dessus, le passage
de FX à FZ est simple puisque FZ (z) = FX (g −1 (z)). Si g est strictement
décroissante comme dans le cas c) on a FZ (z) = 1 − FX (g −1 (z)).
La densité de Z s’obtient simplement par dérivation de FZ .
Pour les v.a. discrètes, la fonction de répartition, nous l’avons vu, est peu
commode et l’on passera par la déﬁnition de la fonction de probabilité. Dans
les notations du type de celles introduites en début de section 3, l’ensemble
des valeurs possibles zk pour k = 1, 2, . . . est l’ensemble des valeurs engendrées
par g(xi ) pour i = 1, 2, . . . . La probabilité pZ (zk ) est obtenue en sommant les
probabilités pX (xi ) des valeurs xi telles que g(xi ) = zk .

1.7

Exercices

Les exercices 1.1 à 1.4 sont d’un niveau avancé et sont uniquement donnés
pour indiquer les éléments de démonstration des propriétés énoncées en section
1.2.
Exercice 1.1 * Soit une suite croissante d’événements {An }, c’est-à-dire telle
∞

que A1 ⊆ A2 ⊆ · · · ⊆ An ⊆ · · · . Montrer que P ( ∪ An ) = lim P (An ). On
n=1

n→∞

rappelle que l’additivité des probabilités pour des événements incompatibles
vaut pour une suite inﬁnie d’événements.
Aide : considérer la suite d’événements {An ∩ An−1 }.

Chapitre 1. Variables aléatoires

13

Exercice 1.2 * Soit une suite décroissante d’événements {Bn }, c’est-à-dire
∞

telle que B1 ⊇ B2 ⊇ · · · ⊇ Bn ⊇ · · · . Montrer que P ( ∩ Bn ) = lim P (Bn ).
n=1

n→∞

∞

Aide : considérer la suite {B n } et admettre que le complémentaire de ∩ Bn
n=1

∞

est ∪ B n .
n=1

Exercice 1.3 * Montrer que l’on peut écrire FX (+∞) = 1 (i.e. lim FX (x) =
x→+∞

1) et de même FX (−∞) = 0.
Aide : on utilisera le résultat de l’exercice 1.1 en considérant des événements
du type ] − ∞, n] et ] − n, +∞[.
Exercice 1.4 * Montrer que P (X = x) = FX (x) − FX (x− ).
Aide : envisager l’événement {x} comme intersection des termes d’une suite
décroissante et utiliser le résultat de l’exercice 1.2.
Exercice 1.5 Soit l’expérience aléatoire consistant à jeter un dé jusqu’à ce
qu’un six apparaisse pour la première fois et soit X la v.a. «nombre de jets
nécessaires». Déterminer la fonction de probabilité de X. Vériﬁer que la somme
des probabilités sur l’ensemble des valeurs possibles est bien égale à 1. Calculer
P (1 < X ≤ 3). Ecrire et dessiner la fonction de répartition de X.
Aide : calculer d’abord P (X > k).
Exercice 1.6 Soit la fonction f (x) = cx(1 − x) pour x ∈ [0, 1] et 0 sinon.
Pour quelle valeur de c est-ce une densité de probabilité ? Déterminer alors la
fonction de répartition de cette loi et sa médiane.
Exercice 1.7 Justiﬁer que la fonction F (x) = 1 − e− 2 pour x > 0 et 0 sinon, est une fonction de répartition. Déterminer les quantiles d’ordres 0,25 et
0,75 (appelés premier et troisième quartiles). Soit X une v.a. suivant cette loi,
calculer P (1 < X ≤ 2).
x

Exercice 1.8 Soit X de densité fX (x) = 2x pour x ∈ [0, 1] et 0 sinon.
Déterminer la fonction de répartition et la densité de 1/X . Même question
pour ln(1/X).
Exercice 1.9 Soit X de loi continue uniforme sur [0, 1] et Y = −θ ln(1 − X)
avec θ > 0. Déterminer la fonction de répartition et la densité de Y.

Chapitre 2

Espérance mathématique et
moments
2.1

Introduction et déﬁnition

Dans cette section nous considérons toujours une v.a. X, soit de fonction
de probabilité pX dans le cas discret, soit de densité fX dans le cas continu. La
notion d’espérance mathématique d’une variable aléatoire correspond à la notion descriptive de moyenne pour une distribution empirique de valeurs. Nous
ferons plus loin (section 2.4) une analogie entre une distribution «théorique»
(une loi de probabilité) et une distribution empirique (une série d’observations
numériques). Prenons pour exemple le temps de fabrication d’un produit qui
connaı̂t des variations aléatoires selon une loi supposée connue. L’espérance
mathématique va indiquer quel est «en moyenne» le temps de fabrication du
produit. Pour cela on eﬀectue la somme des valeurs possibles en les aﬀectant
de poids égaux à leurs probabilités dans le cas discret, l’analogue dans le cas
continu s’exprimant par une intégrale avec pondération par la densité de probabilité.
Déﬁnition 2.1 On appelle espérance mathématique de X, si elle existe,
la valeur notée E(X) telle que :
E(X) =

···


xi pX (xi )
i=1
 +∞

E(X) =
−∞

dans le cas discret,

x fX (x)dx

dans le cas continu.

Du point de vue du graphe de fX (respectivement pX ) cette valeur correspond au centre de gravité de la surface sous la courbe (respectivement des
bâtonnets représentant les probabilités des points). En particulier, s’il existe

Statistique − La théorie et ses applications

16

un axe de symétrie, elle se situe au niveau de cet axe (par exemple l’espérance
mathématique de la loi uniforme sur [0, 360] est 180).
En bref, E(X) sera aussi appelée la moyenne de X.
L’existence de E(X) n’est pas garantie si fX (respectivement pX ) converge
trop lentement vers zéro à l’inﬁni, comme dans l’exemple suivant.
Exemple 2.1 La loi de Cauchy est déﬁnie par fX (x) =
L’espérance mathématique se calcule par


+∞

−∞

1
π(x2 +1)

pour x ∈ R.

x
dx ,
π(x2 + 1)

mais cette intégrale ne converge pas quand x → +∞ ni quand x → −∞ car la
fonction à intégrer s’y comporte comme 1/x . Plus précisément :

a

b

x
1
dx =
ln(1 + x2 )
+ 1)
2π

π(x2

b
a

=

1
ln(1 + b2 ) − ln(1 + a2 )
2π

qui tend vers +∞ quand b → +∞ et vers −∞ quand a → −∞. La loi de
Cauchy n’admet donc pas de moyenne (et ceci bien qu’elle soit symétrique par
rapport à x = 0 ).


2.2

Espérance d’une fonction d’une variable
aléatoire

Soit Z = g(X) une v.a. fonction de la v.a. X. Pour calculer E(Z) on peut
d’abord déterminer sa loi (donc fZ ou pZ ) à partir de celle de X, comme nous
l’avons fait en section 1.6 . Toutefois il est possible de montrer que l’on peut
directement calculer E(Z) sur la loi de X, à savoir à partir de fX ou pX (voir les
exercices pour la démonstration dans le cas continu avec une fonction dérivable
monotone).
Proposition 2.1 Soit g(X) une fonction de la v.a. X. Alors :


+∞

E(g(X)) =
E(g(X)) =

−∞
···


g(x) fX (x)dx

g(xi ) pX (xi )

dans le cas continu (si l’intégrale existe),

dans le cas discret (si la somme existe).

i=1

On voit donc que pour le calcul de E(g(X)) il suﬃt de remplacer la valeur
de x par sa valeur g(x) (ou xi par g(xi )).

Chapitre 2. Espérance mathématique et moments

17

Exemple 2.2 Considérons X de loi uniforme sur l’intervalle [0, 1]. Sa fonction
de répartition est FX (x) = x et sa densité fX (x) = 1 pour x ∈ [0, 1]. Soit la
fonction Z = X 2 .
Calculons d’abord E(X) en établissant la loi de Z. Celle-ci est donnée par :
√
√
√
FZ (z) = P (Z ≤ z) = P (X 2 ≤ z) = P (− z ≤ X ≤ z) = P (X ≤ z)
√
√
puisque P (X ≤ 0) = 0. Donc FZ (z) = FX ( z) = z pour z ∈ [0, 1]. Ainsi
1
fZ (z) = Fz (z) = 2√
pour z ∈ [0, 1] et 0 sinon, d’où :
z


1

z

E(Z) =
0

1
√ dz =
2 z



1
0

√

z 3/2 1
1
z
dz = [
] = .
2
3 0
3

Calculons maintenant directement :
 3 1
 1
x
1
2
2
x .1.dx =
= .
E(Z) = E(X ) =
3
3
0
0

Note 2.1 En étendant l’intégrale de Riemann classique à l’intégrale de RiemannStieltjes, on peut traiter de la même façon cas discret et cas continu. Exposons succinctement cela dans le cas où l’ensemble des valeurs possibles est borné par l’intervalle
[a, b]. Dans la mesure où une fonction g est continue sur [a, b] sauf pour un ensemble

b

dénombrable de points, on peut déﬁnir l’intégrale de Riemann a g(x)dx de façon
simple en subdivisant [a, b] en n intervalles réguliers délimités par :

a = x0 < x1 < x2 < · · · < xn−1 < xn = b.
Cette intégrale est alors la limite, quand n → ∞, des sommes
n


g(xk ) (xk − xk−1 ).

k=1

Dans le contexte qui nous intéresse, l’intégrale de Riemann-Stieltjes relative à FX est
la limite des sommes
n


g(xk ) [FX (xk ) − FX (xk−1 )] ,

k=1

b

notée a g(x) dFX (x).
Ainsi, dans le cas discret ne subsistent dans la somme que les sous-intervalles où
FX varie, c’est-à-dire contenant au moins une valeur possible xi et, au passage à la
limite, ne subsistent que ces valeurs. La limite vaut alors
···

i=1

g(xi ) FX (xi ) − FX (x−
i ) =

···

i=1

g(xi ) pX (xi ).

Statistique − La théorie et ses applications

18

Dans le cas continu, par le théorème de la valeur moyenne on peut écrire

FX (xk ) − FX (xk−1 ) = fX (ξk ) (xk − xk−1 ) où ξk ∈ ]xk−1 , xk [, et la limite donne
b
l’intégrale de Riemann usuelle a g(x) fX (x) dx.
d
Notons qu’en prenant g(x) = 1 sur [a, b], l’intégrale c dFX (x), où [c, d] est
inclus dans [a, b], est égale à FX (d) − FX (c), la probabilité associée à l’intervalle
]c, d], que ce soit dans le cas discret ou dans le cas continu.

2.3

Linéarité de l’opérateur E(.), moments,
variance

Pout toute combinaison linéaire ag(X) + bh(X) de fonctions g et h de X
on a :
E(ag(X) + bh(X)) = aE(g(X)) + bE(h(X)) .
Ceci découle immédiatement de la linéarité de la sommation ou de l’intégration. Voyons cela sur le cas particulier aX + b pour le cas continu :
 +∞
(ax + b)fX (x)dx
E(aX + b) =
−∞
 +∞

=a
−∞


xfX (x)dx + b

+∞
−∞

fX (x)dx = aE(X) + b .

Remarquons au passage que l’on peut voir b comme une variable aléatoire
certaine (discrète), c’est-à-dire prenant cette seule valeur avec probabilité 1
et, en cohérence avec la déﬁnition de l’espérance mathématique, écrire par
convention E(b) = b.
On notera bien que dans le cas général E(g(X)) n’est pas égal à g(E(X)),
par exemple E(X 2 ) = (E(X))2 .
Nous en venons maintenant à la notion de moments, lesquels sont des
espérances mathématiques des puissances de X. Leur intérêt vient du fait qu’ils
permettent de caractériser les distributions. Ainsi nous avons déjà vu que la
moyenne (puissance 1) fournit une valeur centrale. Les puissances supérieures
fournissent diverses caractéristiques de la forme de la distribution.
Déﬁnition 2.2 On appelle moment simple d’ordre r de la v.a. X, où r est
un entier positif, la valeur (si elle existe) μr = E(X r ).
Ainsi μ1 est la moyenne de X que l’on note plus simplement μ (ou μX s’il
y a plusieurs v.a. à distinguer). En fait les caractéristiques de forme reposent
plutôt sur les moments centrés, c’est-à-dire sur les espérances mathématiques
des puissances de X − E(X), ou X − μ, transformation de X appelée centrage
de X.

Chapitre 2. Espérance mathématique et moments

19

Déﬁnition 2.3 On appelle moment centré d’ordre r de la v.a. X, où r est
un entier positif, la valeur (si elle existe) μr = E((X − μ)r ).
Pour r = 1 on a E(X − μ) = E(X) − μ = μ − μ = 0 ce qui caractérise le
centrage de X. Pour r = 2 on a la variance de X, qui est une caractéristique
de dispersion de la distribution comme en statistique descriptive et, à ce titre,
mérite une attention particulière.
Déﬁnition 2.4 On appelle variance de X, la valeur (si elle existe) notée
V (X), déﬁnie par :
V (X) = E((X − μ)2 ).
2
). La
On la note également plus simplement par σ 2 (éventuellement σX
racine carrée de V (X), notée naturellement σ (éventuellement σX ), est appelée
écart-type de X.

Les moments d’ordres supérieurs sont moins utiles et nous les mentionnons
pour mémoire.
Le moment centré d’ordre 3, moyennant une standardisation pour éliminer
l’eﬀet d’échelle, fournit le coeﬃcient d’asymétrie :
E((X − μ)3 )
σ3
dont on voit qu’il est nul en cas de symétrie (nécessairement par rapport à μ ).
Du moment centré d’ordre 4 on déduit le coeﬃcient d’aplatissement ou
curtose :
E((X − μ)4 )
−3
σ4
qui indique, en comparaison avec la loi de Gauss, le degré de concentration
autour de la moyenne (pour la loi de Gauss μ4 est égal à 3σ 4 et ce coeﬃcient
est donc nul).
En développant (X − μ)r et en utilisant la linéarité de l’opérateur E(.), on
voit que μr s’exprime en fonction de μ1 , μ2 , · · · , μr . En particulier, on trouve :
μ2 = E((X − μ)2 ) = E(X 2 − 2μX + μ2 )
= E(X 2 ) − 2μE(X) + μ2 = E(X 2 ) − 2μ2 + μ2
= E(X 2 ) − μ2 .
Ceci constitue une formule très utile pour le calcul de la variance que nous
conviendrons d’appeler formule de décentrage de la variance :
V (X) = E(X 2 ) − μ2 .

20

Statistique − La théorie et ses applications

Cette formule s’apparente à celle de la statistique descriptive pour le calcul
de la variance d’une série numérique (on verra dans la section suivante une
analogie directe entre variance probabiliste et variance descriptive). Comme en
statistique descriptive la variance ne peut être négative. De même, on a :
V (aX + b) = a2 V (X).
Pour le voir il suﬃt d’appliquer la déﬁnition de la variance à la v.a. aX + b :
V (aX + b) = E([aX + b − E(aX + b)]2 )
= E([a(X − E(X))]2 )

puisque E(b) = b et E(aX) = aE(X)

= E(a [X − E(X)] )
2

2

et, encore par linéarité,

V (aX + b) = a E([X − E(X)] ) = a V (X).
2

2

2

Notons au passage que si X a une variance nulle c’est nécessairement une
variable aléatoire certaine. En eﬀet, pour le cas continu :


+∞

V (X) =
−∞

(x − μ)2 fX (x)dx

ne peut s’annuler puisque fX est non négative et ne peut être nulle partout.
Pour le cas discret :
V (X) =

···


(xi − μ)2 pX (xi )

i=1

ne peut s’annuler dès lors qu’il y a deux valeurs possibles. Inversement, si X est
certaine sa variance est évidemment nulle, de sorte qu’une variable aléatoire
est certaine si et seulement si sa variance est nulle.

Existence des moments
Si μr existe alors les moments d’ordres inférieurs μr−1 , μr−2 , · · · , μ1 existent,
et donc μr existe. En eﬀet la fonction xr−1 étant dominée par la fonction
xr au voisinage de +∞ ou de −∞, la convergence de l’intégrale (ou de la
somme) contenant xr entraı̂ne celle de l’intégrale contenant xr−1 . Notons,
pour mémoire, que la variance existe si et seulement si μ2existe. Par ailleurs,
+∞
pour l’existence du moment d’ordre r, la convergence de −∞ |xr | fX (x)dx =
···
 r
E(|X r |) est une condition suﬃsante (ou la convergence de
|xi | pX (xi ) dans
le cas discret).

i=1

Chapitre 2. Espérance mathématique et moments

2.4

21

Tirage aléatoire dans une population ﬁnie :
distribution empirique et distribution
probabiliste

La relation entre distribution empirique et distribution probabiliste suite à
un tirage aléatoire permet, en particulier, de mieux appréhender les notions de
moyenne et de variance d’une v.a. en les reliant aux notions correspondantes
de la statistique descriptive. Considérons une population de N individus sur
lesquels s’observe un certain caractère quantitatif X (par exemple l’âge arrondi
en années). Supposons qu’il y ait, dans cette population, r valeurs distinctes
(avec 2 ≤ r ≤ N ) notées x1 , x2 , · · · , xr et s’observant avec des fréquences
relatives (fréquences1 divisées par N ) p1 , p2 , · · · , pr . La moyenne observée
r

dans la population est donc
xi pi .
i=1

Considérons maintenant la v.a. X «valeur d’un individu tiré au hasard dans
cette population». Par tirage au hasard, on entend que chaque individu a la
même probabilité 1/N d’être sélectionné. De cette équiprobabilité il découle que
la probabilité d’observer la valeur xi est la fréquence relative pi de cette valeur
dans la population (voir note 1.1, deuxième paragraphe). Il y a donc identité
entre la distribution empirique de X dans cette population et la distribution
(plus exactement la loi) de la v.a.discrète X. En particulier E(X) et V (X) sont
identiques à la moyenne et à la variance du caractère X dans la population
(en prenant bien le diviseur naturel N pour le calcul de la variance de cette
dernière). Pour la moyenne, la formule indiquée ci-dessus est la même que celle
d’une v.a. discrète vue en section 2.1 et il en serait naturellement de même
pour la variance.

2.5

Fonction génératrice des moments

La fonction génératrice des moments nous intéresse dans la mesure où elle
peut faciliter le calcul des moments d’une loi. Cependant son existence - et donc
son usage - sera limitée aux lois dont la densité (éventuellement la fonction de
probabilité) décroı̂t plus vite qu’une exponentielle à l’inﬁni (voir plus loin en
note 2.4 la fonction caractéristique des moments qui, elle, est toujours déﬁnie).
Nous supposons ci-après qu’elle existe au moins au voisinage de 0 et que la loi
admet des moments de tous ordres.
Déﬁnition 2.5 On appelle fonction génératrice des moments de la v.a.
X, si elle existe, la fonction :
ΨX (t) = E(etX ).
1 Nous suivons l’usage anglo-saxon commode selon lequel une fréquence est un eﬀectif et
une fréquence relative est une proportion.

Statistique − La théorie et ses applications

22

C’est une fonction de t par la variable t introduite dans la fonction aléatoire
etX .
Proposition 2.2 Le moment d’ordre r de la v.a. X est donné par :
(r)

μr = ΨX (0)
(r)

où ΨX est la dérivée d’ordre r de ΨX . En particulier l’espérance mathématique

(μ1 ) de X est la valeur de la dérivée première ΨX pour t = 0.
Note 2.2 En supposant remplies les conditions requises pour les écritures suivantes
on a :


tX

E(e

+∞

)=
−∞

∞

 (tx)k
k=0

k!

fX (x)dx =

∞ k 

t
k=0

k!

+∞
−∞

xk fX (x)dx =

∞


μk

k=0

tk
k!

et, par identiﬁcation avec le développement de ΨX (t) en série de Taylor-Mac-Laurin,
on a bien la propriété ci-dessus.

Exemple 2.3 loi exponentielle.
Cette loi continue, qui dépend d’un paramètre λ > 0, a pour densité (voir
section 4.2.2) :
λe−λx si x ≥ 0
f (x) =
0
si x < 0 .
Calculons la fonction génératrice des moments d’une v.a. X qui suit cette loi :
 +∞
 +∞
tX
tx
−λx
e λe
dx = λ
e(t−λ)x dx ,
ΨX (t) = E(e ) =
0

0

puis en posant u = (t − λ)x et en supposant t < λ pour la convergence
 0
λ
λ
.
ΨX (t) = −
eu du =
t − λ −∞
λ−t
Les deux premières dérivées sont :


ΨX (t) =


donc μ1 = ΨX (0) =

1
λ

λ
(λ − t)2


,

et μ2 = ΨX (0) =



ΨX (t) =

2λ
(λ − t)3

,

2
.
λ2

On obtient ainsi rapidement E(X) = 1/λ et, par la formule de décentrage,
V (X) = E(X 2 ) − (E(X))2 = λ22 − λ12 = λ12 .
On pourrait obtenir aussi aisément les moments d’ordres supérieurs.



Chapitre 2. Espérance mathématique et moments

23

∞
Note 2.3 Si l’on sait développer ΨX (t)en série entière, ΨX (t) = k=0 ak tk , on
accède directement aux moments. Le moment d’ordre k est en eﬀet le coeﬃcient
du terme en tk , multiplié par k! (voir note 2.2 ci-dessus). Par exemple pour la loi
exponentielle on a :

ΨX (t) =

t2
t
1
tk
+
=
1
+
+
·
·
·
+
+ ···
λ λ2
λk
(1 − λt )

et μk est donc égal à k!/λk , comme on l’a vu pour k = 1 et k = 2.

Exemple 2.4 loi géométrique.
Cette loi discrète, qui dépend d’un paramètre p ∈ [0, 1], a pour fonction de
probabilité (voir section 4.1.4) :
pX (x) = p(1 − p)x
On a alors :
ΨX (t) =

∞


pour x = 0, 1, 2, · · · .

e p(1 − p) = p
tx

x

x=0

∞


(1 − p)et

x

x=0

1
qui est déﬁnie pour (1 − p)et < 1 ou t < log( 1−p
) . Ainsi :

ΨX (t) =

p
1 − (1 − p)et
p(1 − p)et



ΨX (t) =
p(1 − p)et



ΨX (t) =
=
=

2

−2

2

p(1 − p)et [−(1 − p)et ]

[1 − (1 − p)et ]
[1 − (1 − p)et ]
p(1 − p)et [1 − (1 − p)et + 2(1 − p)et ]

3

3

[1 − (1 − p)et ]
t
p(1 − p)e [1 + (1 − p)et ]
3

[1 − (1 − p)et ]

d’où :
μ1 =
μ2 =
et

[1 − (1 − p)et ]

p(1 − p)
1−p
=
p2
p

p(1 − p)(2 − p)
(1 − p)(2 − p)
=
3
p
p2

(1 − p)(2 − p)
−
V (X) =
p2



1−p
p

2
=

1−p
.
p2


24

Statistique − La théorie et ses applications

Note 2.4 Fonction caractéristique des moments ΦX
Elle est déﬁnie via une extension à des variables aléatoires à valeurs dans l’ensemble des nombres complexes C par :

ΦX (t) = E(eitX ) = E(cos(tX)) + iE(sin(tX)).


Puisque fX est intégrable sur tout R et que eitx  ≤ 1 pour tout x, ΦX (t) est
déﬁnie pour tout t, quelle que soit la loi (le même type d’argument valant pour
une loi discrète avec pX ). En fait, la fonction caractéristique des moments permet de déﬁnir parfaitement une loi de probabilité, et ceci de façon duale avec la
fonction de répartition. Moyennant les conditions nécessaires de dérivabilité on a
(k)
μk = i−k ΦX (0). On recourra éventuellement à la fonction caractéristique lorsque
la fonction génératrice n’existera pas au voisinage de 0. Quand cette dernière existe
on en déduit immédiatement ΦX (t) = ΨX (it).

2.6

Formules d’approximation de l’espérance et
de la variance d’une fonction d’une v.a.

Ces formules sont utiles car on se heurte souvent à un problème d’intégration
(ou de sommation) pour le calcul de E(g(X)) ou de V (g(X)). Nous adoptons
les mêmes notations qu’en section 2.2 et supposons que g est dérivable deux fois
au voisinage de μ = E(X). En développant g(x) en série de Taylor au voisinage
de μ :
g(x) = g(μ) + (x − μ)g  (μ) +

(x − μ)2 
g (μ) + o((x − μ)2 )
2

et en négligeant le terme2 o((x − μ)2 ) on obtient :
E(g(X))  g(μ) + g  (μ)E(X − μ) + g  (μ)

E((X − μ)2 )
2

1
 g(μ) + g  (μ) V (X) ,
2
puis :
1
g(x) − E(g(X))  (x − μ) g  (μ) + [(x − μ)2 − V (X)]g  (μ)
2
[g(x) − E(g(X))]2  (x − μ)2 [g  (μ)]2 ,
d’où :

V (g(X))  V (X)[g  (μ)]2 .

Le même type d’approximation peut être obtenu pour les fonctions de plusieurs
variables déﬁnies au chapitre suivant.
o(u)

2 La notation o(u) est utilisée pour désigner une fonction telle que
→ 0 quand u → 0,
u
c’est-à-dire qu’elle devient négligeable par rapport à u quand u est petit.

Chapitre 2. Espérance mathématique et moments

2.7

25

Exercices

Exercice 2.1 Soit la v.a. discrète X prenant pour valeurs 0, 1, 2 avec les
probabilités respectives 0,7 ; 0,2 ; 0,1. Soit Y = X 2 − 1. Calculer E(Y ).
Exercice 2.2 Soit la v.a. X continue, de fonction de répartition FX et de
densité fX , et soit la fonction Z = g(X) où g est une fonction strictement
croissante (continûment) dérivable. En appliquant la méthode décrite en section
1.6, déterminer la fonction de répartition de Z et en déduire que sa densité
est fZ (z) = fX (g −1 (z)) |(g −1 (z)) |. On montrera que ceci reste vrai pour une
fonction strictement décroissante.
 +∞
Établir la propriété énoncée dans la proposition 2.1, i.e. −∞ g(x) fX (x)dx =
E(Z) (aide : utiliser le changement de variable x = g −1 (z) dans l’intégrale ).
Exercice 2.3 Soit la loi (dite exponentielle double ou loi de Laplace) de densité :
1
f (x) = e−|x| , x ∈ R .
2
Montrer que sa fonction génératrice des moments est Ψ(t) = (1 − t2 )−1 . En
déduire sa variance et son moment d’ordre 4. Calculer son coeﬃcient d’aplatissement (déﬁni en section 2.3).
Exercice 2.4 Soit la loi de Pareto (voir section 4.2.6) de paramètres strictement positifs a et θ, dont la fonction de densité est :
⎧
θ+1
⎨ θ a
si x ≥ a
a x
.
f (x) =
⎩
0
si x < a
1. Calculer la moyenne et la variance de cette loi. Quand ces moments existentils ? Généraliser à l’existence d’un moment d’ordre quelconque.
2. Montrer que sa fonction génératrice des moments n’existe pas.
Exercice 2.5 Soit la v.a. X de densité f (x) = 3x2 si x ∈ [0, 1] et 0 sinon.
1. Calculer E(1/X).
2. Déterminer la fonction de répartition de Y = 1/X et en déduire sa densité.
Calculer E(Y ) et vériﬁer ainsi le résultat obtenu au point précédent.
Les notions de ce chapitre seront largement illustrées au cours du chapitre
4 sur les lois usuelles.

Chapitre 3

Couples et n-uplets de
variables aléatoires
3.1

Introduction

Dans ce chapitre nous ne développerons l’étude simultanée de plusieurs v.a.
que de façon restreinte en ne présentant que ce qui est nécessaire pour préparer
l’approche statistique ultérieure.
Dans un premier volet (sections 3.2 à 3.5) nous étudierons les couples de
v.a. en mettant en évidence la façon de formaliser la relation entre deux quantités aléatoires. Nous introduirons notamment les notions de covariance et de
corrélation qui répondent, dans un cadre probabiliste, aux notions du même
nom de la statistique descriptive. L’étude des relations deux à deux entre plusieurs variables aléatoires nous conduira, en section 3.8, à introduire la notation matricielle, en particulier avec la matrice des variances-covariances sur
laquelle repose essentiellement la statistique «multivariée». En ce sens l’étude
des couples de variables aléatoires est un point de départ suﬃsant pour aborder
la théorie multivariée.
Dans un deuxième temps (sections 3.6 et 3.7) nous porterons notre attention sur une suite de n v.a., non pas pour étudier le jeu de leurs relations, mais comme prélude aux propriétés des échantillons aléatoires qui sont
les objets principaux de la statistique mathématique. En eﬀet nous verrons
qu’un échantillon aléatoire de taille n se déﬁnit comme une suite de n v.a.
indépendantes et de même loi, ce qui correspond à l’observation répétée du
même phénomène quantitatif. La relation entre ces observations n’est pas pertinente vu leur caractère d’indépendance.

28

3.2

Statistique − La théorie et ses applications

Couples de v.a.

Nous nous intéressons donc à l’étude de deux entités numériques a priori
aléatoires, par exemple le poids et la taille d’un individu (cas continu), le
nombre d’enfants et le nombre de pièces du logement d’un ménage (cas discret). Nous nous contenterons, comme nous l’avons fait pour une seule v.a.,
d’une déﬁnition informelle.
Un couple de v.a. peut être vu comme un ensemble Ω de valeurs de R2
auquel on associe une mesure de probabilité. Comme pour le cas d’une v.a.
simple (voir section 1.1) la mesure de probabilité est une fonction portant sur
l’ensemble des événements, lesquels sont des parties de R2 . La fonction de
répartition conjointe sera l’instrument fondamental pour donner la probabilité
d’une région quelconque du plan (quoique dans le cas discret on préférera, en
pratique, recourir à la fonction de probabilité conjointe).
Dans ce qui suit nous désignerons de façon générale par (X, Y ) le couple de
variables aléatoires. Par simplicité, nous ne considérons que des couples où les
deux variables sont de même nature, discrètes ou continues, et exclurons le cas
mixte.
Déﬁnition 3.1 Soit (X, Y ) un couple de v.a., on appelle fonction de répartition conjointe de (X, Y ), que l’on note FX,Y , la fonction déﬁnie sur R2
par :
FX,Y (x, y) = P (X ≤ x, Y ≤ y).
Dans ces notations, précisons que l’événement (X ≤ x, Y ≤ y) peut se lire
(X ≤ x) ∩ (Y ≤ y), c’est-à-dire qu’à la fois X soit inférieur ou égal à x et Y
soit inférieur ou égal à y.
Note 3.1 En principe la fonction de répartition conjointe suﬃt à calculer la probabilité de tout événement car les seules parties de R2 probabilisées sont celles générées
par les unions, intersections et compléments de parties du type (X ≤ x, Y ≤ y),
formant la tribu borélienne de R2 (voir l’analogie avec une v.a. simple dans la note
1.2).
Déﬁnition 3.2 (cas discret) Soit (X, Y ) un couple de v.a. discrètes pouvant
prendre les couples de valeurs {(xi , yj ); i = 1, 2, . . . ; j = 1, 2, . . .} . On appelle
fonction de probabilité conjointe la fonction, notée pX,Y , qui donne les
probabilités associées à ces couples de valeurs, soit, pour tout i et tout j :
pX,Y (xi , yj ) = P (X = xi , Y = yj ) .
Déﬁnition 3.3 Soit (X, Y ) un couple de v.a. continues, on appelle fonction
de densité de probabilité conjointe la fonction non négative sur R2 notée

Chapitre 3. Couples et n-uplets de v.a.

29

fX,Y telle que :

FX,Y (x, y) =

y



−∞

x

−∞

fX,Y (u, v)dudv .

Par convention, lorsque l’on parlera d’un couple de v.a. continues, on supposera l’existence de cette fonction.
Si l’on s’intéresse à un événement sur X quelle que soit la valeur prise par Y,
on retombe sur la loi de la v.a. X qui, dans le contexte du couple, est appelée loi
marginale de X. On peut faire le lien avec la fonction de répartition conjointe
en écrivant :
FX (x) = P (X ≤ x) = P (X ≤ x, Y ∈ R)
= lim P (X ≤ x, Y ≤ y)
y→+∞

= FX,Y (x, +∞)
De même FY (y) = FX,Y (+∞, y).
Dans le cas discret il est clair que la fonction de probabilité marginale de
X, par exemple, peut s’obtenir en sommant la fonction de probabilité conjointe
sur toutes les valeurs possibles de Y, i.e. :
pX (xi ) =

...


pX,Y (xi , yj ) .

j=1

Pour le cas continu on admettra la relation du même type portant sur les
densités :
 +∞
fX (x) =
fX,Y (x, y)dy.
−∞

On peut déﬁnir encore des lois conditionnelles pour l’une des variables,
l’autre étant ﬁxée à telle ou telle valeur. Nous illustrons ceci d’abord dans le
cas discret qui est plus simple. Ainsi, reprenant l’exemple introductif où X
est le nombre de pièces du logement d’un ménage pris au hasard et Y est le
nombre d’enfants de ce ménage, nous pouvons considérer par exemple la loi
de X sachant (Y=2). Dans l’analogie entre probabilités et fréquences relatives
(voir section 2.4), cela équivaut à déﬁnir la distribution du nombre de pièces
parmi les ménages ayant deux enfants. Plus généralement, on déﬁnira la fonction de probabilité conditionnelle de X sachant (Y = yj ) en appliquant la
règle des probabilités conditionnelles P (A|B) = P (A ∩ B)/P (B), soit, avec des
notations parlant d’elles-mêmes :
pX|Y =yj (xi ) =

pX,Y (xi , yj )
pY (yj )

,

i = 1, 2, . . . .

On peut évidemment déﬁnir de façon similaire pY |X=xi (yj ).

30

Statistique − La théorie et ses applications

Pour le cas continu, les choses sont plus compliquées car, comme nous
l’avons vu, la probabilité que Y , par exemple, prenne une valeur donnée est
nulle. Il n’empêche, même si cela peut paraı̂tre paradoxal au premier abord,
que l’on peut déﬁnir une loi de X sachant (Y = y), dès lors toutefois qu’en
y on ait fY (y) > 0. On voit l’intérêt de ceci dans le cas de l’exemple introductif où X serait le poids d’un individu et Y sa taille (en cm). La loi de X
sachant (Y = 170) serait en quelque sorte, par analogie avec les fréquences relatives, la distribution des poids parmi les personnes mesurant 170 cm. On peut
établir la fonction de répartition conditionnelle par un raisonnement limite. La
probabilité de (X ≤ x) sachant que (y − h2 ≤ Y ≤ y + h2 ) se calcule par :
P (X ≤ x , y −
P (y −
=
=

h
2

h
2

≤ Y ≤ y + h2 )

≤ Y ≤ y + h2 )

P (X ≤ x , Y ≤ y + h2 ) − P (X ≤ x , Y ≤ y − h2 )
P (y − h2 ≤ Y ≤ y + h2 )
FX,Y (x , y + h2 ) − FX,Y (x , y − h2 )
FY (y + h2 ) − FY (y − h2 )

.

Pour le numérateur, on a utilisé le fait que :
h
h
h
h
) = (X ≤ x, y − ≤ Y ≤ y + ) ∪ (X ≤ x, Y ≤ y − )
2
2
2
2
et que ces deux derniers événements sont incompatibles. En faisant tendre
h vers 0, on obtient la fonction de répartition conditionnelle de X sachant
(Y = y), soit, moyennant une division du numérateur comme du dénominateur
par h :
(X ≤ x, Y ≤ y +

FX|Y =y (x) = lim

h→0

FX,Y (x , y + h2 ) − FX,Y (x , y − h2 ) /h
FY (y + h2 ) − FY (y − h2 ) /h

où le dénominateur tend vers la densité marginale de Y en y (voir section 1.4)
1
de FX,Y par rapport à y, au
et le numérateur tend vers la dérivée partielle
x
point (x, y). Cette dernière étant égale à −∞ fX,Y (u, y)du, on a ﬁnalement :
x
fX,Y (u, y)du
.
FX|Y =y (x) = −∞
fY (y)
Par dérivation par rapport à x, on obtient la densité conditionnelle :
fX|Y =y (x) =

fX,Y (x, y)
fY (y)

dont l’expression rappelle celle de la fonction de probabilité conditionnelle du
cas discret.
1 Tout comme en section 1.4 il a été dit que F
X est dérivable partout sauf peut-être sur
un ensemble dénombrable de points, FX,Y sera dérivable par rapport à x et à y partout sauf
éventuellement sur une partie de R2 de probabilité nulle.

Chapitre 3. Couples et n-uplets de v.a.

3.3

31

Indépendance de deux variables aléatoires

L’indépendance d’une v.a. X d’une part et d’une v.a. Y d’autre part se
rapporte aux occurrences simultanées d’événements sur X et d’événements sur
Y. Nous devons donc partir du couple (X, Y ).
Déﬁnition 3.4 Deux v.a. X et Y sont dites indépendantes si, étant donné
deux événements quelconques (X ∈ A) et (Y ∈ B), on a :
P (X ∈ A, Y ∈ B) = P (X ∈ A) P (Y ∈ B).
Proposition 3.1 X et Y sont indépendantes si et seulement si :
pour tout (x, y) ∈ R2 ,

FX,Y (x, y) = FX (x) FY (y).

Le fait que l’indépendance entraı̂ne que la fonction de répartition conjointe
soit le produit des deux fonctions de répartition marginales est évident en
considérant les événements particuliers de la forme (X ≤ x) et (Y ≤ y). Le
fait que cette condition soit également suﬃsante pour assurer l’indépendance
tient au caractère générateur des événements du type (X ≤ x, Y ≤ y) pour l’ensemble des événements envisagés dans R2 . Les deux propositions suivantes, distinguant cas discret et cas continu, seront particulièrement utiles (on conserve
les mêmes notations que précédemment).
Proposition 3.2 Deux v.a. discrètes X et Y sont indépendantes si et seulement si , pour tout i = 1, 2, . . . et tout j = 1, 2, . . . ,
pX,Y (xi , yj ) = pX (xi ) pY (yj ) .
Proposition 3.3 Deux v.a. continues X et Y sont indépendantes si et seulement si, pour tout (x, y) ∈ R2 ,
fX,Y (x, y) = fX (x) fY (y) .

Proposition 3.4 Si X et Y sont indépendantes, alors pour toutes fonctions g
et h, les v.a. g(X) et h(Y ) sont également indépendantes.
Ce résultat est immédiat dans la mesure où tout événement sur g(X) peut
s’exprimer comme un événement sur X et de même pour h(Y ) sur Y.
Note 3.2 Il va de soi que ces fonctions doivent être mesurables (voir note 1.1).

Statistique − La théorie et ses applications

32

3.4

Espérance mathématique, covariance,
corrélation

Pour le couple de v.a. (X, Y ) nous connaissons déjà E(X) et E(Y ), moyennes
respectives des lois marginales de X et de Y. De façon semblable à ce qui
a été fait en section 2.2, nous pouvons aussi déﬁnir la notion d’espérance
mathématique d’une fonction g(X, Y ) du couple. En particulier, dans l’approche statistique on utilisera abondamment la fonction somme X + Y , que
nous étudierons plus spécialement dans la section suivante comme une application de la section présente.
Étant donné g(X, Y ) une v.a. à valeurs dans R, selon le même principe qu’en
section 2.2, nous pouvons directement déterminer son espérance mathématique
en considérant les images par g de toutes les valeurs possibles du couple (X, Y )
et en les pondérant par les probabilités (ou densités de probabilité pour le cas
continu) correspondantes. D’où, moyennant l’existence des doubles sommes ou
des intégrales doubles :
E(g(X, Y )) =

··· 
···


g(xi , yj )pX,Y (xi , yj ) dans le cas discret,

i=1 j=1



+∞



+∞

E(g(X, Y )) =
−∞

−∞

g(x, y)fX,Y (x, y)dxdy

dans le cas continu.

On pourrait également établir la loi de cette nouvelle v.a. Z = g(X, Y ) et
calculer sa moyenne E(Z). Mais nous ne traiterons pas ici la façon d’obtenir la
loi d’une fonction de deux variables aléatoires.
Proposition 3.5 (linéarité de l’espérance mathématique) Pour la fonction aX + bY on a :
E(aX + bY ) = aE(X) + bE(Y ).
Ceci est une extension du résultat donné en début de section 2.3 et découle
également de la linéarité des sommations et intégrations doubles. Cette propriété reste évidemment valable si l’on substitue à X une v.a. g(X) et à Y une
v.a. h(Y ), par exemple :
E(2X 2 + 3Y 2 ) = 2E(X 2 ) + 3E(Y 2 ).
Les notions de moments simples ou centrés vues en section 2.3 s’étendent
au cas du couple. En bref, on déﬁnit le moment simple croisé d’ordres (p, q) par
E(X p Y q ) et le moment centré correspondant par E([X − E(X)]p [Y − E(Y )]q ).
Seul le cas p = 1 et q = 1 mérite notre intérêt, conduisant notamment aux
notions clés de covariance et corrélation entre deux variables aléatoires.

Chapitre 3. Couples et n-uplets de v.a.

33

Déﬁnition 3.5 On appelle covariance de X et de Y, que l’on note cov(X, Y ),
le nombre (s’il existe) :
cov(X, Y ) = E([X − E(X)][Y − E(Y )]).
On remarquera d’emblée que c’est une notion symétrique en X et Y, i.e.
cov(X, Y ) = cov(Y, X).
Proposition 3.6 (formule de décentrage de la covariance)
cov(X, Y ) = E(XY ) − E(X)E(Y ).
En eﬀet :
cov(X, Y ) = E([X − E(X)][Y − E(Y )])
= E(XY − E(X) Y − X E(Y ) + E(X)E(Y ))
= E(XY ) − E(X)E(Y ) − E(X)E(Y ) + E(X)E(Y )
= E(XY ) − E(X)E(Y ) .
Cette formule est à rapprocher de la formule de décentrage de la variance vue
en section 2.3. D’ailleurs, on notera que cov(X, X) = E([X − E(X)]2 ) = V (X).
Proposition 3.7 Si X et Y sont indépendantes alors cov(X, Y ) = 0.
En eﬀet il suﬃt de vériﬁer qu’en cas d’indépendance E(XY ) = E(X)E(Y ).
Faisons-le dans le cas continu, par exemple, en rappelant que l’indépendance
implique fX,Y (x, y) = fX (x)fY (y).


+∞



+∞

E(XY ) =
−∞
 +∞

=
−∞

−∞

xyfX (x)fY (y)dxdy


yfY (y)




+∞
−∞

xfX (x)dx dy

+∞

= E(X)
−∞

yfY (y)dy

= E(X)E(Y ).
Notons bien que deux v.a. peuvent avoir une covariance nulle sans
pour autant être indépendantes. Montrons-le sur un exemple artiﬁciel.
Exemple 3.1 Soient X et Y deux variables aléatoires discrètes, chacune pouvant prendre les valeurs 0 ,1 ou 2. Les probabilités conjointes sont données à
l’intérieur du tableau croisé ci-dessous, les marges représentant les probabilités
marginales.

Statistique − La théorie et ses applications

34

HH Y
0
HH
X
H
0
0
1
2/9
2
0
2/9

1

2

4/9
0
1/9
5/9

0
2/9
0
2/9

4/9
4/9
1/9
1

On a :
2
5
2
4 2
+ = , E(Y ) = + 2 × = 1,
9 9
3
9
9
2
1
2
E(XY ) = 1 × 2 × + 2 × 1 × =
9
9
3
E(X) =

d’où cov(X, Y ) = E(XY ) − E(X)E(Y ) = 0. Or X et Y ne sont pas indépendantes puisque, par exemple, P (X = 0, Y = 0) est nul alors que

P (X = 0)P (Y = 0) = 29 × 49 = 0.
Propriétés de la covariance
1.

cov(aX + b, cY + d) = ac cov(X, Y )

2.

cov(X + Y, Z) = cov(X, Z) + cov(Y, Z)

Pour montrer le point 1, appliquons la déﬁnition de la covariance aux v.a.
aX + b et cY + d :
cov(aX + b, cY + d) = E([aX + b − E(aX + b)][cY + d − E(cY + d])
= E([aX + b − aE(X) − b][cY + d − cE(Y ) − d])
= E(a[X − E(X)] c[Y − E(Y )]) = ac cov(X, Y ).
On voit que les constantes disparaissent en raison des centrages eﬀectués par
la covariance. Le point 2 se démontre en développant de façon analogue.
Déﬁnition 3.6 On appelle (coeﬃcient de) corrélation linéaire de X et de
Y , que l’on note corr(X, Y ), le nombre (s’il existe) :
corr(X, Y ) =

cov(X, Y )
σX σY

où σX est l’écart-type de X et σY celui de Y.
Cette formule s’apparente à celle de la statistique descriptive pour le calcul
de la corrélation linéaire sur une série d’observations couplées. Dans le cas
particulier du tirage aléatoire d’un individu dans une population vu en section
2.5, la corrélation descriptive devient la corrélation probabiliste. Supposons
que l’on étudie la population des ménages résidant dans une ville donnée, les
variables considérées étant X le nombre de pièces du logement et Y le nombre

Chapitre 3. Couples et n-uplets de v.a.

35

d’enfants. Soit fij la fréquence relative, dans la population, du couple de valeurs
(xi , yj ). Le coeﬃcient de corrélation linéaire descriptif (ou empirique) du lien
entre nombre de pièces et nombre d’enfants dans cette population est :
···
··· 




i=1 j=1
···


(xi −

(xi − x)(yj − y)fij

x)2 f

i=1

i

···


j=1

(yj − y)2 fj

···
où x = i=1 xi fi est le nombre moyen de pièces pour l’ensemble des ménages
avec fi égal à la fréquence du nombre de pièces xi et, de la même façon avec fj ,
y est le nombre moyen d’enfants. En tirant un ménage au hasard, on induit un
couple de v.a. (X, Y ) pour lequel la probabilité associée au couple de valeurs
(xi , yj ) devient fij . Le numérateur de la formule ci-dessus est alors cov(X, Y )
et le dénominateur σX σY .
Nous énonçons ci-après quelques propriétés de la corrélation linéaire, identiques à celles de la statistique descriptive.
Propriétés de la corrélation linéaire
1.

corr(X, Y ) = corr(Y, X)

2.

corr(aX + b, cY + d) = corr(X, Y )

La propriété 1 est évidente. La propriété 2 résulte du fait que σaX+b = a σX
puisque V (aX +b) = a2 V (X) (voir section 2.3) et σcY +d = c σY . Le produit ac
obtenu dans la covariance disparaı̂t donc en divisant par le produit des écartstypes. Cette propriété indique que la corrélation linéaire entre deux v.a. est
invariante dans un changement d’échelle (et même un changement d’origine
comme pour le passage d’une température en Celsius à une température en
Fahrenheit), ce qui semble raisonnable pour une mesure de lien entre deux
entités numériques.
Proposition 3.8 Quelle que soit la loi conjointe du couple (X, Y ) on a :
−1 ≤ corr(X, Y ) ≤ 1 .
Démonstration : considérons la v.a. X +λY où λ est un nombre réel quelconque.
Par déﬁnition de la variance :
V (X + λY ) = E([X + λY − E(X + λY )]2 )
= E([X − E(X) + λ(Y − E(Y ))]2 )
= E([X − E(X)]2 + 2λ[X − E(X)][Y − E(Y )] + λ2 [Y − E(Y )]2 )
= E([X − E(X)]2 ) + 2λE([X − E(X)][Y − E(Y )]) + λ2 E([Y − E(Y )]2 )
= V (X) + 2λcov(X, Y ) + λ2 V (Y ) .

Statistique − La théorie et ses applications

36

Or V (X + λY ) ≥ 0 quel que soit λ. Cela implique, pour le polynôme du
deuxième degré en λ ci-dessus, que le déterminant [cov(X, Y )]2 − V (X)V (Y )]
est négatif ou nul, et donc :
[cov(X, Y )]2 ≤ V (X)V (Y )
[cov(X, Y )]2
[corr(X, Y )]2 =
≤1.
V (X)V (Y )

et


Notons que si [corr(X, Y )]2 = 1 alors le déterminant est nul et, pour la
racine double λ0 du polynôme, on a V (X + λ0 Y ) = 0, c’est-à-dire (voir section 2.3) que X + λ0 Y est une v.a. certaine. En d’autres termes, il existe une
dépendance linéaire parfaite entre X et Y. Ceci est à rapprocher du fait qu’en
statistique descriptive une corrélation égale à ±1 équivaut à un alignement
parfait des points représentant les couples de valeurs. Ce résultat justiﬁe aussi
l’appellation de corrélation linéaire.
La corrélation s’annule si et seulement si la covariance s’annule et, donc,
une corrélation nulle n’implique pas l’indépendance.

3.5

Somme de deux v.a.

Etudions la fonction particulière X + Y issue du couple (X, Y ) en vue d’une
généralisation dans la section suivante à une somme de n v.a. qui, comme il
a été dit plus haut, sera un objet essentiel de la statistique. Nous savons déjà
que, par la linéarité,
E(X + Y ) = E(X) + E(Y ) .
Proposition 3.9 On a :
V (X + Y ) = V (X) + V (Y ) + 2cov(X, Y ) .
Si X et Y sont indépendantes, alors :
V (X + Y ) = V (X) + V (Y ).
La première équation est le cas particulier λ = 1 du développement de
V (X + λY ) dans la démonstration de la proposition 3.8. La deuxième est
évidente puisque l’indépendance implique une covariance nulle.
Proposition 3.10 (fonctions génératrices des moments) Si X et Y sont
indépendantes, alors :
ΨX+Y (t) = ΨX (t)ΨY (t).

Chapitre 3. Couples et n-uplets de v.a.

37

En eﬀet ΨX+Y (t) = E(e(X+Y )t ) = E(eXt eY t ) = E(eXt )E(eY t ) puisque
si X et Y sont indépendantes, alors les fonctions eXt et eY t sont également
indépendantes (voir proposition 3.4).
Note 3.3 Il en va évidemment de même pour la fonction caractéristique des moments.

3.6

Les n-uplets de v.a. ; somme de n v.a.

Nous considérons maintenant la généralisation du couple (ou 2-uplet) à
un n-uplet, c’est-à-dire à un vecteur aléatoire, prenant ses valeurs dans Rn
et que nous noterons (X1 , X2 , · · · , Xn ). Comme il a été dit en introduction
notre intérêt va se porter essentiellement sur le cas particulier d’un échantillon
aléatoire et c’est pourquoi nous ne nous attardons pas sur la loi conjointe du
n-uplet. Disons brièvement que la notion de fonction de répartition conjointe
se généralise naturellement selon :
FX1 ,X2 ,··· ,Xn (x1 , x2 , · · · , xn ) = P (X1 ≤ x1 , X2 ≤ x2 , · · · , Xn ≤ xn ) .
Les notions de lois marginales et lois conditionnelles se généralisent de la
même façon. On peut ainsi déﬁnir des lois marginales pour tout sous-ensemble
de composantes du vecteur aléatoire (X1 , X2 , · · · , Xn ). On peut déﬁnir des
lois conditionnelles d’un sous-ensemble sachant les valeurs d’un autre sousensemble. La notion de covariance (respectivement corrélation) se généralise en
matrice des variances-covariances (respectivement des corrélations) des composantes prises deux à deux (voir section 3.8). La notion d’indépendance dans un
couple se généralise à la notion d’indépendance mutuelle des n composantes selon laquelle les événements portant sur tous sous-ensembles sont indépendants.
Par simpliﬁcation nous omettrons l’adjectif «mutuelle» qui sera implicite.
Nous limitons désormais notre étude des n-uplets au cas où les n composantes ont la même loi de probabilité marginale et sont indépendantes.
On peut considérer alors le n-uplet comme n observations successives d’un
même phénomène aléatoire, ces observations étant indépendantes les unes des
autres (au sens où le fait que, par exemple, la première observation donne
telle valeur n’inﬂue en rien sur le résultat de la deuxième observation). On voit
poindre ici de façon évidente l’approche statistique, de tels n-uplets constituant
précisément ce que nous appellerons plus loin des échantillons aléatoires.
Pour l’heure nous donnons des résultats sur les fonctions somme et moyenne
des n composantes (conduisant plus loin à l’étude du comportement de la
somme ou de la moyenne d’un échantillon aléatoire).
Proposition 3.11 (proposition fondamentale) Soit X1 , X2 , · · · , Xn une
suite de v.a. indépendantes et suivant une même loi de probabilité de moyenne
μ et de variance σ 2 . On a, pour la somme Sn = X1 + X2 + · · · + Xn :

Statistique − La théorie et ses applications

38

E(Sn ) = nμ

V (Sn ) = nσ2

et pour la moyenne X n = Sn /n :
E(X n ) = μ

V (X n ) =

σ2
.
n

Pour Sn ces résultats sont la généralisation de proche en proche, d’une part
de la propriété de linéarité de l’espérance mathématique, à savoir E(Sn ) =
E(X1 ) + E(X2 ) + · · · + E(Xn ), d’autre part de l’additivité des variances pour
des variables indépendantes, à savoir V (Sn ) = V (X1 ) + V (X2 ) + · · · + V (Xn ).
Pour la moyenne on a :



Sn
E(X n ) = E
=
n
 
Sn
V (X n ) = V
=
n

nμ
1
E(Sn ) =
=μ
n
n
nσ 2
σ2
1
V
(S
)
=
=
.
n
n2
n2
n

σ
Notons, pour mémoire, que l’écart-type de X n est √ .
n
Des v.a. indépendantes et de même loi sont couramment notées en bref
variables aléatoires i.i.d., abréviation de «indépendantes et identiquement
distribuées». Nous adopterons dorénavant cette notation.
Proposition 3.12 (fonction génératrice d’une somme de v.a. i.i.d.)
Soit Ψ(t) la fonction génératrice des moments de la loi commune aux n v.a.
i.i.d., alors ΨSn (t) = [Ψ(t)]n .
Ceci est une extension évidente de proche en proche de la proposition 3.10
sur la somme de deux v.a. .

3.7

Sondage aléatoire dans une population et
v.a. i.i.d.

Ayant franchi un pas vers l’idée d’observations répétées, on peut se poser
la question de savoir comment se traduit en termes probabilistes l’expérience
consistant à eﬀectuer non plus un seul tirage aléatoire comme vu en section
2.4, mais n tirages successifs d’individus dans une population. S’agissant d’une
population bien réelle de N individus, ce contexte expérimental est celui du
sondage.

Chapitre 3. Couples et n-uplets de v.a.

39

Un sondage aléatoire simple (par distinction vis-à-vis de plans de sondage
plus complexes) consiste à sélectionner un premier individu avec équiprobabilité
de tous les N individus, puis un deuxième individu avec équiprobabilité des
N − 1 individus restants et ainsi de suite pour les N − 2 individus restants,
etc. jusqu’à sélectionner n individus. Pour une variable quantitative d’intérêt
sur les individus, notons X1 l’observation aléatoire du premier tirage, X2 celle
du deuxième tirage, ..., Xn celle du n-ième tirage. Constatons que, dans ce
schéma, il n’y a pas indépendance des v.a. X1 , X2 , · · · , Xn . Par exemple, au
deuxième tirage, les valeurs possibles (recevant la probabilité N1−1 ) sont sujettes
au résultat du premier tirage. Le sondage n’est donc pas une situation de v.a.
i.i.d., ce qui complique tant soit peu les choses et explique que la théorie des
sondages occupe une place à part dans la statistique mathématique. Une façon
de contourner le problème de la dépendance consisterait à eﬀectuer un sondage
avec remise ( par opposition au sondage usuel précédent que l’on qualiﬁe de sans
remise), c’est-à-dire à réintégrer dans la population, à chaque tirage, l’individu
tiré. Mais ceci n’est jamais appliqué en pratique car il y a perte d’eﬃcacité
due à la possibilité de tirer le même individu plusieurs fois. A supposer que les
tirages avec remise se fassent bien indépendamment les uns des autres il est
clair que, dans ce cas, les v.a. X1 , X2 , · · · , Xn sont i.i.d..
Remarquons intuitivement que, si le taux de sondage n/N est faible (par
exemple un échantillon de taille 1000 dans la population française des individus
âgés de 15 ans et plus), le sondage sans remise rejoint le sondage avec remise.
Ceci justiﬁe qu’en pratique on utilise les résultats de la théorie statistique
classique développés dans les chapitres à venir, dans les situations de sondage.
Disons que si n/N reste inférieur à 0,1 on a des approximations correctes,
d’autant plus que d’autres approximations du même ordre de grandeur sont
souvent inévitables dans la théorie des sondages elle-même.
Note 3.4 Nous avons considéré des tirages sans remises successifs. Il est équivalent,
en pratique, de tirer simultanément
n individus parmi les N individus de la po 
échantillons
possibles de taille n devant avoir la même
pulation, chacun des N
n
N 
probabilité 1/ n d’être sélectionné.

3.8

Notation matricielle des vecteurs aléatoires

Pour établir les propriétés d’un p-uplet (X1 , X2 , · · · , Xp ) de v.a. il est commode d’adopter la notation matricielle. Ainsi, on note :
⎛
⎞
X1
⎜ X2 ⎟
⎜
⎟
X=⎜ . ⎟
⎝ .. ⎠
Xp

Statistique − La théorie et ses applications

40

le vecteur aléatoire de dimension p, à valeurs dans Rp . On déﬁnit alors l’espérance mathématique de X, notée E(X), par le vecteur des espérances mathématiques (si elles existent) :
⎞
⎛
E(X1 )
⎜ E(X2 ) ⎟
⎟
⎜
E(X) = ⎜
⎟.
..
⎠
⎝
.
E(Xp )
Si les covariances des composantes prises 2 à 2 existent, la matrice d’élément
(i, j) égal à cov(Xi , Xj ) est appelée matrice des variances-covariances de
X et nous la noterons V(X). Notons que cette matrice est symétrique et que
ses éléments diagonaux sont les variances des composantes.
Soient maintenant A une matrice (q × p) et c un vecteur (q × 1). Alors la
relation Y = AX + c déﬁnit un vecteur aléatoire Y à valeurs dans Rq .
Proposition 3.13 Soit X un vecteur aléatoire d’espérance E(X) et de matrice
de variance-covariance V(X) et soit le vecteur aléatoire Y tel que Y = AX + c.
On a alors :
E(Y) = A E(X) + c ,
V(Y) = A V(X) At .
Le symbole At désigne la matrice transposée de A. Nous omettrons la
démonstration qui ne présente pas de diﬃculté et oﬀre peu d’intérêt.
Indiquons le cas particulier où A est le vecteur ligne (1 × p),
A = (1

1

···

1) ,

pour lequel Y est la somme des composantes de X. On obtient alors la généralisation de la proposition 3.9 à p v.a. quelconques :
 p
p



Xi =
V (Xi ) + 2
cov(Xi , Xj ) ,
V
i=1

où



3.9

i<j

i=1

i<j

est une sommation sur tous les couples (Xi , Xj ) avec i < j.

Loi de Gauss multivariée

La loi de Gauss, ou loi normale, pour une v.a. à valeurs dans R est la
célèbre courbe en cloche (graphe de sa densité). Elle est décrite en détail en
section 4.2.4. Indiquons simplement ici qu’il s’agit en fait d’une famille de lois
dépendant de deux paramètres qui sont la moyenne μ et la variance σ 2 de
chaque loi, d’où la notation N (μ, σ2 ). Toute fonction linéaire aX + b d’une

Chapitre 3. Couples et n-uplets de v.a.

41

v.a. gaussienne X est une v.a. gaussienne dont la moyenne et la variance se
calculent par les règles générales vues en section 2.3 : E(aX + b) = aE(X) + b
et V (aX + b) = a2 V (X). Par une transformation linéaire ad hoc on peut se
ramener à la loi2 N (0 ; 1) appelée loi de Gauss centrée-réduite (celle fournie dans
les tables). Nous montrons des propriétés analogues pour un vecteur aléatoire
gaussien.
Un vecteur aléatoire gaussien X de dimension p est parfaitement déﬁni par
son vecteur des espérances noté μ et sa matrice des variances-covariances notée
μ, Σ). La densité conjointe de ses composantes au point
Σ. Sa loi est notée Np (μ
(x1 , x2 , · · · , xp ) ∈ Rp est :
!
1
1
t
−1
exp
−
Σ
(x
−
μ
)
(x
−
μ
)
fX (x1 , x2 , · · · , xp ) =
2
(2π)p/2 (det Σ)1/2
où det Σ dénote le déterminant de la matrice Σ et x dénote le vecteur colonne
de composantes x1 , x2 , · · · , xp .
Notons que la matrice Σ doit être inversible. Sinon le vecteur aléatoire ne
serait pas réellement de dimension p au sens où il y aurait au moins une liaison
linéaire exacte entre ses composantes et, par conséquent, ce vecteur prendrait
ses valeurs dans un sous-espace de dimension inférieure à p.
Un cas particulier important est celui de la loi dont le vecteur des espérances
mathématiques est le vecteur nul 0 et la matrice des variances-covariances est
la matrice identité Ip , soit la loi Np (0, Ip ) que nous appellerons loi de Gauss
p-variée centrée-réduite par analogie avec la loi usuelle centrée-réduite N (0 ; 1)
dans le cas où p = 1. Pour cette loi, l’expression de la densité devient :
"
#
!
p
n
$
1 2
1
1 2
1
√
exp
−
x
exp
−
x
=
i
2
2 i
(2π)p/2
2π
i=1

i=1

qui se sépare en un produit de densités respectives à chacune des composantes,
qui sont les densités marginales de celles-ci. En se reportant à la section 4.2.4
on peut voir que toutes les composantes ont une loi marginale N (0; 1). Ainsi
ces composantes sont indépendantes. Il est clair que ceci est également vrai si la
matrice Σ est diagonale, à ceci près que chaque composante a une loi marginale
gaussienne dont la moyenne est la composante correspondante du vecteur μ et
la variance est la valeur sur la position correspondante de la diagonale de Σ.
On a donc la proposition suivante.
Proposition 3.14 Un vecteur aléatoire gaussien a ses composantes indépendantes si et seulement si la matrice des variances-covariances est diagonale,
i.e. si et seulement si les covariances des composantes prises deux à deux sont
toutes nulles.
2 Bien que la notation générique soit N (μ, σ 2 ), nous aurons l’habitude de remplacer la
virgule par un point-virgule lorsque l’on aura des nombres explicites aﬁn d’éviter la confusion
avec la virgule décimale.

42

Statistique − La théorie et ses applications

Alors que nous avions vu en section 3.4 qu’une covariance nulle n’impliquait pas l’indépendance pour un couple de v.a. de façon générale, dans le
cas gaussien il y a équivalence entre indépendance et covariance (ou
corrélation) nulle.
Nous admettrons le théorème suivant très utile pour caractériser les vecteurs
gaussiens.
Théorème 3.1 (théorème de caractérisation) Un vecteur aléatoire est
gaussien si et seulement si toute combinaison linéaire de ses composantes est
une variable aléatoire gaussienne.
On déduit immédiatement de cette caractérisation essentielle que si X est de
μ, Σ) alors Y = AX est également un vecteur aléatoire gaussien puisque
loi Np (μ
toute combinaison linéaire des composantes de Y est une combinaison linéaire
des composantes de X et est donc gaussienne. De plus, le fait d’ajouter un
vecteur de constantes c à un vecteur gaussien ne fait que déplacer la moyenne,
la densité restant gaussienne avec μ + c se substituant à μ dans l’expression
générale donnée plus haut. En reprenant les résultats de la proposition 3.13,
on en déduit la proposition suivante.
μ, Σ) et soit le vecteur
Proposition 3.15 Soit X un vecteur aléatoire de loi Np (μ
aléatoire Y = AX + c où A est une matrice (q × p) de rang q et c un vecteur
μ + c, A Σ At ).
(q × 1). Alors Y est de loi Nq (Aμ
La condition que la matrice A soit de rang maximal (avec q nécessairement
inférieur ou égal à p) s’impose pour que la matrice des variances-covariances
A Σ At de Y soit inversible, i.e. que Y soit réellement de dimension q et non
pas à valeurs dans un sous-espace de dimension inférieure.
On montre que l’on peut toujours par un choix judicieux de la matrice A et
du vecteur c se ramener à un vecteur Y de loi p-variée centrée-réduite. En eﬀet
on peut d’abord ramener la moyenne à un vecteur nul en passant au vecteur
aléatoire X − μ. Puis, du fait que la matrice Σ est inversible et symétrique,
il existe C, une matrice (p × p) de rang p, telle que CCt = Σ. Considérons
alors le vecteur Y = C−1 (X − μ). Sa moyenne est évidemment nulle puisque
E(Y) = C−1 E(X − μ)= C−1 0 = 0, et sa variance est :
V(Y) = C−1 V(X) (C−1 )t = C−1 Σ (C−1 )t
= C−1 CCt (C−1 )t = Ip .
Remarques
1. En plus d’être symétrique et de rang maximal, la matrice Σ doit être déﬁnie
strictement positive. C’est-à-dire que pour tout vecteur v ∈ Rp non nul on a
vt Σ v >0. En eﬀet une combinaison linéaire des composantes de X est de la
forme vt X, sa variance est vt Σ v, laquelle doit rester positive.

Chapitre 3. Couples et n-uplets de v.a.

43

2. Pour qu’un couple de v.a. forme un couple gaussien, il ne suﬃt pas que
chaque v.a. soit gaussienne. En d’autres termes les lois marginales peuvent être
gaussiennes sans que la loi conjointe soit gaussienne sur R2 . En revanche, toutes
les lois marginales des composantes d’un vecteur gaussien sont des gaussiennes
en tant que combinaison linéaire particulière donnant un coeﬃcient 1 à cette
composante et 0 à toutes les autres.
Par ailleurs si les composantes sont indépendantes et gaussiennes alors la
loi conjointe est gaussienne multivariée.

3.10

Exercices

Exercice 3.1 Le tableau suivant représente la loi du couple (X, Y ) : X nombre
d’enfants dans un ménage, Y nombre de téléviseurs du ménage (pour un ménage
pris au hasard dans une population de ménages ayant 1 à 3 enfants et 1 à 3
téléviseurs).
HH Y
1
2
3
HH
X
H
1
0,22 0,11 0,02
2
0,20 0,15 0,10
3
0,06 0,07 0,07
Calculer le coeﬃcient de corrélation entre X et Y .
Exercice 3.2 Montrer que la covariance entre la somme et la diﬀérence de
deux v.a. indépendantes et de même loi est toujours nulle.
Exercice 3.3 Soient X et Y deux v.a. indépendantes suivant une même loi de
Bernoulli de paramètre p (voir section 4.1.2). Donner la loi de X + Y. Calculer
P (X +Y = 0), P (X −Y = 0) et P (X +Y = 0, X −Y = 0). Les deux v.a. X +Y
et X − Y sont-elles indépendantes ? Que vaut leur covariance en application de
l’exercice 3.2 ? Quelle conclusion générale en tirez-vous ?
Exercice 3.4 Soient X et Y deux v.a. indépendantes et soit Z = X + Y.
Calculer P (Z ≤ z|X = x) et en déduire que fZ|X=x (z) = fY (z − x).
Déterminer la densité conjointe de Z et de X, et en déduire que fZ (z) =
 +∞
f (z − x)fX (x)dx.
−∞ Y
Donner la loi de T = X − Y.
Exercice 3.5 Déterminer la loi de la somme de deux v.a. indépendantes, continues uniformes sur [0, 1].
Aide : on déterminera la zone du plan en coordonnées (x, y) déﬁnie par
{(x, y) | x + y < z, 0 < x < 1, 0 < y < 1} et, à partir de la loi conjointe, on
calculera géométriquement, selon les diﬀérents cas pour z, P (X + Y < z).

44

Statistique − La théorie et ses applications

Exercice 3.6 On mesure la longueur et la largeur d’un terrain rectangulaire.
2
.
La mesure de la longueur est une v.a. X de moyenne μX et de variance σX
La mesure de la largeur est une v.a. Y de moyenne μY et de variance σY2 .
On suppose que ces deux mesures sont indépendantes. Quelle est l’espérance
mathématique et la variance pour la mesure de la surface du terrain ?
Exercice 3.7 (marche aléatoire) On se situe sur un axe en une position initiale.
On se déplace alors par étapes successives et indépendantes les unes des autres
de la façon suivante. A chaque étape on fait un pas d’un mètre à droite (+1)
avec probabilité p ou à gauche (-1) avec probabilité 1−p. Soit X le déplacement
à une étape quelconque. Calculer E(X) et V (X).
Soit Y l’éloignement de la position initiale après n étapes. Calculer E(Y )
et V (Y ).
Exercice 3.8 Soit un couple (X, Y ) gaussien bivarié. On suppose que X et Y
2
la variance de X, σY2 la variance de Y et ρ
sont de moyenne nulle et on note σX
le coeﬃcient de corrélation linéaire du couple. Établir que la densité conjointe
du couple est :
!
 2
x
1
1
xy
y2
%
fX,Y (x, y) =
exp −
−
2ρ
+
2
2(1 − ρ2 ) σX
σX σY
σY2
2πσX σY 1 − ρ2
(on notera que les courbes de niveau du graphe de fX,Y sont des ellipses de
centre (0,0) ).

Chapitre 4

Les lois de probabilités
usuelles
Nous abordons ici une sorte de catalogue des lois les plus utilisées dans la
modélisation statistique. Nous nous eﬀorcerons de justiﬁer l’utilité de ces lois
en précisant le type de situations où elles sont appropriées. De façon générique
et sauf mention expresse on notera X une v.a. qui suit la loi décrite. Chaque loi
fera l’objet d’un symbole spéciﬁque. Par exemple, la loi binomiale de paramètres
n et p sera notée B(n, p) et on écrira X ; B(n, p) pour signiﬁer que X suit
cette loi.

4.1
4.1.1

Les lois discrètes
La loi uniforme discrète

L’ensemble des valeurs possibles est {1, 2, 3, · · · , r}, r étant un paramètre
de la loi. Uniforme signiﬁe que chaque valeur reçoit la même probabilité 1/r.
En fait cette loi est peu utilisée en tant que modèle statistique, mais mérite
d’être présentée en raison de sa simplicité. On la rencontre dans les jeux de
hasard, par exemple dans le lancement d’un dé : X est le nombre de points
obtenus et r est égal à 6. Si le dé est parfaitement symétrique chaque face, et
donc chaque nombre de points, a la probabilité 1/6 d’apparaı̂tre.
Pour une v.a. X qui suit cette loi, on a :
r+1
E(X) =
2
1 2
V (X) =
(r − 1) .
12
En eﬀet :
1 r(r + 1)
r+1
1
=
E(X) = (1 + 2 + · · · + r) =
r
r
2
2

Statistique − La théorie et ses applications

46

et, sachant que 12 +22 +· · ·+r 2 = 16 r(r +1)(2r +1), on peut calculer la variance
par la formule de décentrage :
V (X) = E(X 2 ) − (E(X))2

2

1 2
r+1
2
2
= (1 + 2 + · · · + r ) −
r
2

2
1
1 2
r+1
= (r + 1)(2r + 1) −
=
(r − 1) .
6
2
12

est

Ainsi, pour le jet d’un dé, l’espérance mathématique du nombre de points
7
et sa variance 35
.
2
12

4.1.2

Loi de Bernoulli B(p)

C’est la loi la plus simple que l’on puisse envisager puisqu’il n’y a que
deux valeurs possibles, codées 1 et 0. On note p la probabilité associée à la
valeur 1, p étant le paramètre de la loi (la probabilité 1 − p associée à la valeur
0 est souvent notée q dans les ouvrages, mais nous jugeons cela superﬂu). On
écrit X ; B(p) et donc :
X ; B(p) ⇐⇒ X

valeurs possibles
probabilités

1
p

0
1−p .

On peut écrire la fonction de probabilité de la façon suivante :
P (X = x) = px (1 − p)1−x

,

x ∈ {0, 1}.

On a E(X) = p et V (X) = p(1 − p) puisque :
E(X) = 0 × (1 − p) + 1 × p = p
V (X) = E(X 2 ) − (E(X))2 = 02 × (1 − p) + 12 × p − p2 = p(1 − p) .

La fonction génératrice des moments est :
ΨX (t) = E(etX ) = et.1 p + et.0 (1 − p)
= pet + (1 − p) .

En pratique la v.a. X sera utilisée comme fonction indicatrice d’un
événement donné au cours d’une expérience aléatoire (par exemple avoir un

Chapitre 4. Les lois de probabilités usuelles

47

appareil tombant en panne avant l’expiration de la garantie, être infecté au
cours d’une épidémie, être bénéﬁciaire pour une entreprise). X prend la valeur
1 si l’événement se produit et 0 s’il ne se produit pas à l’issue de l’expérience.
Dans ce contexte, p représente la probabilité de l’événement considéré.
La v.a. X sera une variable de comptage lors de répétitions de l’expérience
constituant le processus de Bernoulli décrit ci-après et conduisant notamment
à la loi binomiale.
Par convention la réalisation de l’événement sera appelée «succès» et sera
codée 1, sa non-réalisation sera appelée «échec» et sera codée 0.

4.1.3

Le processus de Bernoulli et la loi binomiale B(n, p)

Le processus consiste en une suite de répétitions de l’expérience aléatoire de
Bernoulli, toutes ces répétitions successives étant indépendantes les unes des
autres. La probabilité de succès à chaque répétition est p.
Un processus de Bernoulli est donc modélisé par une suite X1 , X2 , X3, . . .
de v.a. i.i.d., chacune de loi B(p). Dans ce processus on peut s’intéresser à
diﬀérents types de comptages, menant à diﬀérentes lois. Nous verrons les plus
courants : comptage des succès en s’arrêtant à un nombre de répétitions ﬁxé
à l’avance (loi binomiale), comptage des échecs avant d’atteindre le premier
succès (loi géométrique) ou le r-ième succès (loi binomiale négative).
La loi binomiale est la loi de la v.a. X correspondant au nombre de succès
au cours de n répétitions du processus. Elle est omniprésente en statistique.
L’application la plus fréquente se situe dans le domaine des sondages. Ayant
sélectionné au hasard n individus dans une grande population (voir le sondage
aléatoire simple en section 3.7) on peut «estimer» la proportion p d’individus ayant un caractère 1 donné (succès). Si le taux de sondage est faible,
on a vu que l’on pouvait admettre que le tirage sans remise est très proche du
tirage avec remise. Pour ce dernier la probabilité de succès à chaque tirage est p
et il y a indépendance des tirages. La v.a. X correspond au nombre d’individus
ayant le caractère d’intérêt parmi n individus sélectionnés.
La loi binomiale a deux paramètres n et p, et l’ensemble des valeurs possibles
est {0, 1, 2, · · · , n} . Calculons directement la probabilité p(x) d’obtenir x succès
parmi n répétitions.
Toute suite contenant x succès et n−x échecs a une probabilité px (1−p)n−x
en raison de l’indépendance des répétitions successives, et ceci quel que soit
l’ordre d’apparition des succès et des échecs. Imaginons que nous écrivions la
succession des résultats avec une séquence de lettres S et E (succès, échec).
Combien y-a-t-il d’écritures possibles ? Une suite particulière étant parfaitement déﬁnie par les positions occupées par les x lettres S, il suﬃt de dénombrer
1 Le mot «caractère» est à prendre dans un sens élargi. Ce peut être, par exemple, l’acquiescement à une opinion proposée dans un questionnaire.

48

Statistique − La théorie et ses applications

combien il y a de choix de x positions parmi n positions. C’est le nombre de
combinaisons à x éléments que l’on peut former à partir de n éléments :
 
n!
n
.
=
p!(n − p)!
x
D’où p(x) =
patibles.

n
x

px (1 − p)n−x , toutes ces suites étant distinctes et donc incom-

Déﬁnition 4.1 On dit que la v.a. discrète X suit une loi binomiale B(n, p) si
sa fonction de probabilité est :
 
n x
p (1 − p)n−x , x = 0, 1, 2, · · · , n .
p(x) =
x

Proposition
4.1 Soit X1 , X2 , · · · , Xn une suite de v.a. i.i.d. de loi B(p), alors

Sn = ni=1 Xi suit une loi B(n, p).
Ceci est la traduction du nombre de comptage des succès à travers la variable
indicatrice de Bernoulli.
De cette proposition on déduit que la somme de deux v.a. indépendantes de
lois respectives B(n1 , p) et B(n2 , p) est une v.a. de loi B(n1 +n2 , p). En eﬀet cette
somme peut être considérée comme celle de n1 + n2 répétitions indépendantes
du processus de Bernoulli avec probabilité p de succès.
Proposition 4.2 Soit X ; B(n, p), alors :
E(X) = np
V (X) = np(1 − p)
ΨX (t) = pet + (1 − p)

n

.

Démonstration : comme X peut être vue comme la somme de n v.a. indépendantes X1 , X2 , · · · , Xn de même loi B(p), il suﬃt d’appliquer la proposition
fondamentale 3.11 sur la somme de n v.a. i.i.d., avec μ = p et σ 2 = p(1 − p)
qui sont respectivement la moyenne et la variance de la loi B(p), pour obtenir
la moyenne et la variance de X . En appliquant le résultat de la proposition
3.12 on obtient sa fonction génératrice des moments. On peut vériﬁer, à titre


d’exercice, que ΨX (0) = np (voir section 2.5).

Chapitre 4. Les lois de probabilités usuelles

4.1.4

49

Les lois géométrique G(p) et binomiale négative
BN (r, p)

Soit un processus de Bernoulli de paramètre p. La loi géométrique G(p),
ou loi de Pascal, est la loi de la v.a. X «nombre d’échecs avant de parvenir
au premier succès». L’ensemble des valeurs possibles est N et la fonction de
probabilité est :
p(x) = p(1 − p)x , x ∈ N ,
car il n’y a qu’une séquence possible : x échecs suivis d’un succès.
On a alors :
1−p
p
1−p
V (X) =
p2
E(X) =

E(etX ) =

p
.
1 − (1 − p)et

Démonstration : la fonction génératrice s’écrit :

E(etX ) =

∞

k=0

etk p(1 − p)k = p

∞


[(1 − p)et ]k =

k=0

p
.
1 − (1 − p)et

Elle est déﬁnie si (1 − p)et < 1 ou t < − ln(1 − p), donc au voisinage de 0. La
dérivée première de cette expression est :
p(1 − p)et
[1 − (1 − p)et ]2
et sa valeur pour t = 0 est (1 − p)/p qui correspond à la moyenne. En prenant
la dérivée seconde au point 0, on obtient E(X 2 ) = (1 − p)(2 − p)/p2 , puis V (X)
par la formule de décentrage.

La loi binomiale négative est une généralisation de la loi géométrique où
l’on considère X «nombre d’échecs avant de parvenir au r-ième succès». Sa
fonction de probabilité est :


r+x−1 r
p (1 − p)x , x ∈ N.
p(x) =
x
En eﬀet pour toute séquence de x échecs et r succès la probabilité est pr (1−p)x .
Sachant que le dernier résultat de la séquence doit être un succès, il reste à
dénombrer les séquences avec x échecs et r−1 succès ce qui revient à dénombrer


.
les possibilités de choix de x positions parmi x + r − 1 positions, soit r+x−1
x

Statistique − La théorie et ses applications

50

On a alors :
r(1 − p)
p
r(1 − p)
V (X) =
p2
p
ΨX (t) = [
]r (au voisinage de 0) .
1 − (1 − p)et
E(X) =

Ceci peut être établi grâce aux propositions 3.11 et 3.12, en remarquant qu’une
v.a. BN (r, p) peut être considérée comme une somme de r v.a. indépendantes
de loi G(p). En eﬀet toute séquence à r succès, dont un succès ﬁnal, peut être
vue comme une suite de r séquences du type de celles de la loi géométrique.
Certains auteurs préfèrent à X la v.a. Y «nombre total de répétitions pour
atteindre le r-ième succès». On a donc Y = X + r, avec :


x−1 r
pY (x) =
p (1 − p)x−r , x = r, r + 1, ...
x−r
et :

4.1.5

E(Y ) =

r(1 − p)
r
.
, V (Y ) =
p
p2

La loi hypergéométrique H(N, M, n)

Soit un ensemble de N individus dont M possèdent un certain caractère,
que nous appellerons «succès» par analogie avec la loi binomiale, et N − M ne
la possèdent pas. On eﬀectue un tirage aléatoire sans
de n individus

 remise
échantillons
de taille
dans cet ensemble. On entend par là que chacun des N
n
 
n possibles a la même probabilité 1/ N
d’être
sélectionné
(voir
note
3.4).
On
n
considère la v.a. X «nombre de succès observés parmi les n individus». On a
alors la fonction de probabilité :
M N −M 
p(x) =

x

, x = 0, 1, ..., n.
Nn−x

n

Le numérateur correspond au nombre de choix de x individus parmi M et n−x
parmi N − M . Avec comme valeurs possibles {0, 1, ..., n} nous supposons que
M et N −M sont supérieurs à n. Toutefois si n > M alors la plus grande valeur
possible est M et si n > N − M la plus petite valeur possible est n − (N − M ).
Nous pouvons garder la formule générale ci-dessus en convenant que ab = 0 si
a < b.
On démontre (le résultat étant intuitif pour la moyenne) que :
M
N
M N −n
M
)
.
V (X) = n (1 −
N
N N −1
E(X) = n

Chapitre 4. Les lois de probabilités usuelles

51

On peut faire le rapprochement avec le processus de Bernoulli et la loi binomiale qui correspondent au tirage aléatoire avec remise. Le nombre de succès
suit une loi B(n, p) avec p = M/N . Dans les deux situations la moyenne est
identique alors que, pour la loi hypergéométrique, la variance reçoit un «facteur
correctif de sans remise» égal à (N − n)/(N − 1). Clairement les deux situations se rapprochent si le «taux de sondage» n/N diminue. Plus formellement
on
montrer que, pour tout x, p(x) tend vers l’expression correspondante
npeut
x
(1
−
p)n−x de la loi binomiale quand N → ∞ et (M/N ) → p.
p
x

4.1.6

La loi multinomiale

Il s’agit d’une extension de la situation binaire (succès, échec) de la loi binomiale, à une situation
«multinaire» à c catégories, de probabilités respectives
c
p1 , p2 , ..., pc avec k=1 pk = 1.
On s’intéresse aux fréquences observées N1 , N2 , ..., Nc des diﬀérentes catégories au cours de n observations répétées indépendantes. La fonction de probabilité conjointe des v.a. N1 , N2 , ..., Nc est :
P (N1 = n1 , N2 = n2 , ..., Nc = nc ) =

n!
pn1 pn2 ...pnc c
n1 !n2 !...nc ! 1 2

si tous les nk (k de 1 à c) appartiennent à {0, 1, 2, ..., n} et vériﬁent la contrainte

c
k=1 nk = n, la probabilité étant nulle sinon. Ceci s’établit par le même type
de raisonnement que pour la loi binomiale. Le terme pn1 1 pn2 2 · · · pnc c correspond
à la probabilité de toute série de n répétitions avec n1 d’entre elles donnant la
catégorie 1, n2 donnant la catégorie 2,. . . et nc donnant la catégorie c. Le terme
avec les factoriels correspond au nombre de séries de ce type possibles (nombre
de façons d’occuper n1 positions parmi les n successions pour la catégorie 1,
n2 positions pour la catégorie 2, · · · , nc pour la catégorie c).
La loi marginale de Nk est clairement la loi B(n, pk ). Par ailleurs on démontre
que cov(Nk , Nl ) = −npk pl . On peut donc écrire l’espérance et la matrice des
variances-covariances du vecteur 
aléatoire N = (N1 , N2 , ..., Nc ) à valeurs dans
{0, 1, 2, ..., n}c avec la contrainte ck=1 Nk = n :
⎞
⎛
⎞
⎛
−np1 p2
···
−np1 pc
np1
np1 (1 − p1 )
⎜ np2 ⎟
⎜ −np1 p2
np2 (1 − p2 ) · · ·
−np2 pc ⎟
⎟
⎜
⎟
⎜
E(N) = ⎜ . ⎟ , V(Y) = ⎜
⎟.
..
..
..
⎠
⎝ .. ⎠
⎝
.
.
···
.
npc

−np1 pc

−np2 pc

···

npc (1 − pc )

Cette loi est à la base de l’étude des variables catégorielles du chapitre 10.

4.1.7

Le processus et la loi de Poisson P(λ)

On considère un processus d’occurrences d’un événement donné sur l’échelle
du temps, par exemple l’arrivée des appels à un standard téléphonique. Pour un

Statistique − La théorie et ses applications

52

temps t > 0 ﬁxé (à partir d’une certaine origine des temps) on déﬁnit la variable
aléatoire X(t) «nombre d’occurrences dans l’intervalle ]0, t]». Par commodité
on pose :
pk (t) = P (X(t) = k), où k ∈ N.
En bref, on dit qu’on a un processus de Poisson si :
– il y a une invariance temporelle, à savoir que pk (t) ne dépend pas de l’origine des temps, mais dépend uniquement de la longueur t de l’intervalle,
quels que soient k et t ;
– il y a indépendance des nombres d’occurrences pour deux intervalles disjoints ;
– pour un très petit intervalle la probabilité d’avoir deux occurrences ou
plus est négligeable devant la probabilité d’avoir une occurrence exactement et cette dernière est proportionnelle à la longueur de cet intervalle.
Plus formellement :
p1 (h) = λh + o(h)
∞


pk (h) = o(h)

k=2

o(h)
où, rappelons-le, o(h) est une fonction telle que
→ 0 quand h → 0.
h
Le paramètre λ > 0 caractérise l’intensité de fréquence des occurrences.
Sous ces hypothèses on démontre que :
pk (t) =

e−λt (λt)k
, k ∈ N.
k!

La loi de Poisson est la loi du nombre d’occurrences dans une unité
de temps, donc pour t = 1 dans les formulations ci-dessus. Par conséquent on
dit que la v.a. X suit une loi de Poisson P(λ) si sa fonction de probabilité est :
p(k) =

Sachant que
alors :

∞ k

λ
k=0

k!

e−λ λk
, k ∈ N.
k!

= eλ , la somme des probabilités est bien égale à 1. On a

E(X) = λ
V (X) = λ
t
ΨX (t) = eλ(e −1) .

Chapitre 4. Les lois de probabilités usuelles

53

Le paramètre λ est donc le nombre moyen d’occurrences par unité de
temps pour le processus. Les démonstrations sont simples :
∞
∞
∞



e−λ λk
e−λ λk−1
e−λ λj
k
λ
=
=λ
=λ
E(X) =
k!
(k − 1)!
j!
j=0
k=0

E(X 2 ) =

k=1

∞


∞

k2

k=0

= λ2

∞

e−λ λk−2
k=2

∞


e−λ λk
e−λ λk  e−λ λk
=
+
k(k − 1)
k
k!
k!
k!
(k − 2)!

k=2

+λ

∞

e−λ λk−1
k=1

(k − 1)!

k=1

= λ2 + λ

d’ où V (X) = λ ,
ΨX (t) =

∞

etk e−λ λk
k=0

k!

=

∞

e−λ (λet )k
k=0

k!

λ(et −1)

=e

t
∞

e−λe (λet )k

k=0

k!

t

= eλ(e

−1)

.

Remarques
– On verra plus loin la loi exponentielle qui est celle du temps s’écoulant
entre deux occurrences successives.
– On a la propriété additive suivante : soient X1 ; P(λ1 ) et X2 ; P(λ2 )
indépendante de X1 , alors X1 +X2 ; P(λ1 +λ2 ). Ceci se voit directement
en appliquant la proposition 3.10 sur la fonction génératrice d’une somme.
Approximation de la loi binomiale par la loi de Poisson
Si l’on choisit une unité de temps suﬃsamment petite pour que la probabilité
d’avoir plus d’une occurrence devienne négligeable on voit que le processus de
Poisson peut être rapproché d’un processus de Bernoulli par discrétisation de
l’écoulement continu du temps en unités successives.
Montrons que la fonction de probabilité de la loi P(λ) est équivalente à
celle de la loi B(n, p) quand n → ∞ et p → 0 de façon que np → λ. On a
vu (section 4.1.3) que la fonction génératrice des moments de la loi B(n, p) est
n
Ψ(t) = [pet + (1 − p)] . D’où :
ln Ψ(t) = n ln pet + (1 − p)
= n ln 1 + p(et − 1)
= n[p(et − 1) + o(p)] .
Comme np tend vers λ, ln Ψ(t) tend vers λ(et − 1). Par suite Ψ(t) tend vers
exp{λ(et − 1)} qui est la fonction génératrice de la loi P(λ).
Ceci a un intérêt pratique pour approcher la loi binomiale lorsque l’événement «succès» est rare (p est petit), avec un grand nombre de répétitions.

54

Statistique − La théorie et ses applications

On considère que, si n ≥ 50 et p ≤ 0, 1, la loi binomiale B(n, p) est approchée
de façon tout à fait satisfaisante par la loi de Poisson de paramètre λ = np.
On peut aussi utiliser une telle approximation si l’événement «succès» est très
fréquent (p ≥ 0, 9) en intervertissant succès et échec.

Exemple 4.1 La probabilité pour qu’un réacteur d’avion d’un certain type
connaisse une panne avant sa première révision est 1/1000. Sachant qu’une
compagnie d’aviation possède sur ses avions 100 réacteurs de ce type calculons
la probabilité qu’elle ne rencontre pas plus de deux problèmes avec ces réacteurs
avant la première révision. Le nombre de réacteurs à problème est une v.a. X
de loi B(100 ; 0,001) qui peut être approchée par la loi P(0,10). Donc :
P (X ≤ 2)  e−0,1 +

e−0,1 .0,1 e−0,1 .(0,1)2
+
= 0,99985.
1!
2!


Le modèle de Poisson s’applique dans de nombreuses situations de comptages par unité de temps ou par unité de surface : nombre de sinistres par an
pour un assuré, problèmes de ﬁles d’attente (arrivées à un guichet, nombre de
personnes servies), particules émises par une source radioactive. Pour un comptage par unité de surface (par exemple le nombre de couples d’une espèce d’oiseaux nichant par quadrat d’une forêt), le modèle correspond à une répartition
spatiale au hasard.

4.2
4.2.1

Les lois continues
La loi continue uniforme U[a, b]

On dit que X suit une loi uniforme sur l’intervalle ﬁni [a, b] si sa densité
est constante sur [a, b] et nulle à l’extérieur de cet intervalle, soit :
⎧
⎨
f (x) =

⎩

1
b−a

si a ≤ x ≤ b

0

sinon

.

Nous en avons déjà vu une illustration en section 1.4.
Sa fonction de répartition est :
⎧
0
⎪
⎪
⎪
⎨ x−a
F (x) =
⎪
b−a
⎪
⎪
⎩
1

si x < a
si a ≤ x ≤ b
si x > b

.

Chapitre 4. Les lois de probabilités usuelles

55

On peut aisément vériﬁer que :
a+b
,
2
(b − a)2
V (X) =
.
12
E(X) =

La loi uniforme de référence est la loi U[0 , 1] correspondant aux générateurs de
nombres au hasard des logiciels (fonction «RANDOM» ou «ALEA»). A partir
d’un tel générateur on peut produire des nombres au hasard sur [a, b] par la
transformation y = (b − a)x + a. Nous verrons en section 4.3 comment simuler
une loi quelconque à partir de ces «nombres au hasard».

4.2.2

La loi exponentielle E(λ)

Comme mentionné dans les remarques sur le processus de Poisson, la loi
exponentielle correspond à la variable aléatoire X du temps s’écoulant entre
deux occurrences successives lors d’un tel processus. Avec les notations de la
section 4.1.7 la probabilité qu’il n’y ait aucune occurrence dans un intervalle
de temps de longueur t est égale à p0 (t) = e−λt , d’où P (X > t) = e−λt et
l’expression de la fonction de répartition P (X ≤ t) :
"
1 − e−λt si t ≥ 0
F (t) =
,
0
si t < 0
puis de la densité, par dérivation :
"
f (t) =

λe−λt

si t ≥ 0

0

si t < 0

.

On a déjà montré (voir section 2.5) que :
1
λ
1
V (X) = 2
λ
λ
ΨX (t) =
.
λ−t
E(X) =

Logiquement, puisque λ est le nombre moyen d’occurrences par unité de
temps, λ1 est la durée moyenne entre deux occurrences successives. On reparamétrise souvent la loi en posant θ = 1/λ, d’où :
x
1 −
f (x) = e θ , x ≥ 0,
θ
qui met en évidence sa moyenne θ, la variance étant alors θ2 .

56

Statistique − La théorie et ses applications

La loi exponentielle est également le modèle de durée de vie pour un
système idéal sans usure, λ1 étant l’espérance de vie du système. En eﬀet on
peut voir que l’âge du système ne joue aucun rôle quant aux chances de survie
à un horizon donné puisque :
P (X > t + h|X > t)

=

P ((X > t + h) ∩ (X > t))
P (X > t)

=

P (X > t + h)
e−λ(t+h)
=
= e−λh ,
P (X > t)
e−λt

qui ne dépend pas de t.

4.2.3

La loi gamma Γ(r, λ)

Soit X
1 , X2 , ..., Xr une suite de r variables aléatoires i.i.d. de loi E(λ) et

soit T = ri=1 Xi . On démontre (voir exercices) que T suit une loi de densité :
⎧
λr
⎪
⎨
xr−1 e−λx si x ≥ 0
(r − 1)!
,
f (x) =
⎪
⎩ 0
si x < 0
laquelle déﬁnit la loi Γ(r, λ). Des propriétés des sommes de v.a. i.i.d. on déduit
immédiatement :
r
λ
r
V (T ) = 2
λ
E(T ) =

ΨT (t) = (

λ r
) ,
λ−t

si t < λ .

Vériﬁons que la densité ci-dessus est bien celle qui conduit à cette fonction
génératrice des moments :
 +∞
λr
xr−1 e−λx dx
etx
ΨT (t) = E(etT ) =
(r − 1)!
0
 +∞
λr
xr−1 e−(λ−t)x dx
=
(r − 1)! 0
 +∞
λr
λr
1
ur−1 e−u du =
=
r
(r − 1)! (λ − t) 0
(λ − t)r
en vertu de la relation classique

 +∞
0

ur−1 e−u du = (r − 1)! .

La loi Γ(r, λ) modélise en particulier le temps séparant une occurrence de
la r-ième suivante dans un processus de Poisson. Elle joue un rôle similaire à
celui de la loi binomiale négative dans le processus de Bernoulli.

Chapitre 4. Les lois de probabilités usuelles

57

On peut généraliser la loi Γ(r, λ) à une valeur de r non entière (mais strictement positive) en remplaçant, dans la densité, l’expression (r −1)! par la fonc∞
tion gamma d’Euler déﬁnie, pour tout réel positif, par Γ(r) = 0 xr−1 e−x dx,
dont la loi a hérité son nom.
La fonction de répartition de cette loi n’est pas explicite et nécessite le
recours à un logiciel (ou des tables, mais celles-ci ne sont pas très courantes).

4.2.4

La loi de Gauss ou loi normale N (μ, σ2 )

Il s’agit, comme on sait, de la loi de probabilité fondamentale de la statistique en raison du théorème central limite que nous verrons en section 5.8.3.
On dit que la variable aléatoire X suit une loi de Gauss, ou loi normale,
notée N (μ, σ 2 ), si elle a pour densité :
!
1
1 (x − μ)2
f (x) = √
exp −
, x∈R.
2
σ2
2πσ 2
Les paramètres sont notés μ et σ 2 du fait qu’ils correspondent respectivement
à la moyenne et à la variance de la loi (voir la démonstration ci-après), σ étant
donc son écart-type. Le graphe de la densité est la fameuse courbe en cloche
symétrique autour de la valeur μ.
Pour μ = 0 et σ 2 = 1 on a la loi de Gauss centrée-réduite N (0 ; 1) dont la
fonction de répartition, notée Φ, est donnée dans les tables statistiques usuelles :
 x
z2
1
e− 2 dz.
Φ(x) = √
2π −∞
Montrons que si X ; N (μ, σ 2 ) alors sa transformée centrée-réduite
suit la loi N (0 ; 1). Soient FX et FZ les fonctions de répartition
Z = X−μ
σ
respectives de X et de Z, alors :
FZ (Z ≤ z) = P (Z ≤ z) = P (

X −μ
≤ z) = P (X ≤ μ + zσ) = FX (μ + σz),
σ

et en dérivant FZ et FX par rapport à z, on a :
1 (μ + zσ − μ)2
1
fZ (z) = σfX (μ + zσ) = √ exp −
2
σ2
2π

!

1 2
1
= √ e− 2 z .
2π

En inversant le raisonnement on montre que si Z ; N (0 ; 1) alors
X = μ + σZ ; N (μ, σ 2 ) et, plus généralement, ceci implique que toute fonction linéaire d’une v.a. gaussienne est une v.a. gaussienne.
En vertu de cette propriété le calcul de P (X ≤ x) se ramène à un calcul de
probabilité sur la variable gaussienne centrée-réduite. Mettons en évidence la
règle de calcul par la proposition suivante.

Statistique − La théorie et ses applications

58

Proposition 4.3 Soit X ; N (μ, σ 2 ), alors :


x−μ
où Z ; N (0; 1),
P (X ≤ x) = P Z ≤
σ


x−μ
soit encore :
P (X ≤ x) = Φ
.
σ

Exemple 4.2 Soit X ; N (10 ; 4). Calculons, par exemple, P (X ≤ 13) :
P (X ≤ 13) = P (Z ≤

13 − 10
) = P (Z ≤ 1,5) = Φ(1,5) = 0,9332
2

par lecture de la table de la loi normale centrée-réduite.
En lecture inverse déterminons le quantile d’ordre 0,95 de la loi de X. Pour Z
on lit dans la table que le quantile d’ordre 0,95 est 1,645 , i.e. P (Z ≤1,645) =0,95.
≤1,645) =0,95 et P (X ≤ 10+1,645× 2) =0,95. Pour la loi de X
D’où P ( X−10
2
le quantile est donc 10+1,645× 2 =13,29.

D’une façon générale P (X ≤ x) est obtenu en lisant la probabilité d’être
x−μ
dans la table et le quantile d’ordre α de la loi de X est égal
inférieur à
σ
à μ + zα σ où zα est le quantile d’ordre α de la table, i.e. tel que Φ(zα ) = α.
Calculons la fonction génératrice des moments de Z ; N (0; 1) :

ΨZ (t) = E(etZ ) =
1
=√
2π



+∞

−∞

+∞

e

1 2
1
etz √ e− 2 z dz
2π

− 12 (z−t)2

−∞

t2

e2
e dz = √
2π
t2
2



+∞

1

2

e− 2 u du .

−∞

D’où :
t2

ΨZ (t) = e 2 .
Par cette fonction, vériﬁons que la moyenne et la variance de Z sont bien,
respectivement, 0 et 1 :
t2

ΨZ (t) = te 2 ,


E(Z) = ΨZ (0) = 0 ,
t2

ΨZ (t) = (1 + t2 )e 2 ,



V (Z) = E(Z 2 ) = ΨZ (0) = 1.

Pour X = μ + σZ on a donc E(X) = μ et V (X) = σ 2 V (Z) = σ 2 , ce qui
justiﬁe la notation N (μ, σ2 ).

Chapitre 4. Les lois de probabilités usuelles

59

On peut directement accéder aux moments de tous ordres par le dévelopt2
pement en série entière de e 2 (voir la note 2.3) :
e

t2
2

=

2
∞

( t )s

2

s=0

s!

=

∞

1 2s
t
s s!
2
s=0

d’où :
(2s)!
.
2s s!
Du fait que la densité est une fonction paire les moments d’ordres impairs sont
nuls. On notera, pour mémoire, que μ4 = 3.
μ2s =

Pour X ; N (μ, σ 2 ), comme (X − μ)r = σ r Z r , on obtient immédiatement les
moments centrés :
(2s)!
μ2s = σ 2s s
2 s!
et, en particulier, μ4 = 3σ 4 .
De plus :
ΨX (t) = exp(tμ + σ 2

t2
),
2

car :
E(etX ) = E(et(μ+σZ) ) = etμ E(e(tσ)Z ) = etμ e

(tσ)2
2

.

Nous donnons maintenant une proposition essentielle pour les développements
statistiques.
Proposition 4.4 Toute combinaison linéaire de v.a. gaussiennes indépendantes
est une variable aléatoire gaussienne.
Démonstration : il suﬃt de démontrer cela avec deux v.a., l’extension à plusieurs
v.a. se faisant de proche en proche. De plus, on a vu que si X est gaussienne alors
Y = aX est gaussienne. Il suﬃt donc de démontrer la proposition pour Y1 + Y2 ,
où Y1 et Y2 sont indépendantes. Soient Y1 ; N (μ1 , σ12 ) et Y2 ; N (μ2 , σ22 ).
Selon la proposition 3.10 on a :
ΨY1 +Y2 (t) = ΨY1 (t)ΨY2 (t :) = etμ1 +
1

2

2

2
σ1
2
2 t

etμ2 +

2
σ2
2
2 t

2

= et(μ1 +μ2 )+ 2 (σ1 +σ2 )t

qui est la fonction génératrice des moments de la loi N (μ1 + μ2 , σ12 + σ22 ) . 
Notons que la proposition n’est pas vraie pour des v.a. dépendantes. Ainsi
deux v.a. peuvent être marginalement gaussiennes sans pour autant que toute

60

Statistique − La théorie et ses applications

combinaison linéaire de celles-ci soit gaussienne, car cela dépend de la nature
de leur loi conjointe.
Quelques valeurs clés de la loi de Gauss
A partir de la lecture dans la table de la loi de Gauss centrée-réduite des
quantiles d’ordres 0,975 ; 0,995 et 0,9995, soit :
Φ(0,975) = 1,96
Φ(0,995) = 2,57
Φ(0,9995) = 3,30 ,
on déduit ces intervalles de dispersion autour de la moyenne pour X ; N (μ, σ2 ) :
P (μ − 1,96 σ < X < μ + 1,96 σ) = 0,95
P (μ − 2,57 σ < X < μ + 2,57 σ) = 0,99
P (μ − 3,30 σ < X < μ + 3,30 σ) = 0,999 .
La première égalité est souvent résumée en disant que la probabilité d’obtenir
une valeur dans l’intervalle «moyenne plus ou moins 2 écarts-types» est de
95% (plus exactement 0,9544). En termes de fréquence des observations on a
coutume de dire que, grosso modo, 95% des observations doivent se situer dans
cet intervalle. Cette propriété est d’ailleurs souvent étendue de façon tout à fait
abusive à tout type de loi.
La troisième relation montre qu’il n’y a pratiquement aucune chance de trouver
une observation au-delà de 3 écarts-types de la moyenne.

4.2.5

La loi lognormale LN (μ, σ2 )

Cette loi fournit souvent un bon modèle pour les variables strictement positives ayant une distribution asymétrique avec allongement vers les valeurs
élevées, en particulier dans les domaines biologique (poids des personnes, par
exemple), économique (distribution des revenus) et physique.
Soit X une v.a. à valeurs strictement positives, on dit qu’elle suit une loi
lognormale de paramètres μ et σ 2 , notée LN (μ, σ 2 ), si ln X ; N (μ, σ 2 ).
Sa densité, peu utilisée car on préfère généralement se ramener à l’échelle
logarithmique, peut être déduite de celle de la loi de Gauss par la transformation
exponentielle selon la méthode du changement de variable exposée en section
1.6. Posons Y = ln X, on a :


ln x − μ
Y
.
F (x) = P (X ≤ x) = P (e ≤ x) = P (Y ≤ ln x) = Φ
σ
En dérivant, on obtient aisément :
⎧
⎪
⎨ √1 exp{− 1 (ln x − μ)2 }
2σ 2
x 2πσ
f (x) =
⎪
⎩ 0

si x > 0
.
si x ≤ 0

Chapitre 4. Les lois de probabilités usuelles

61

Comme P (X ≤ x) = P (log X ≤ log x) les quantiles restent en correspondance
par la transformation exponentielle et, par exemple, la médiane est eμ . Il n’en
va pas ainsi des moments qui peuvent toutefois se déduire directement de la
fonction génératrice de la loi de Gauss N (μ, σ 2 ). En eﬀet, en posant encore
Y = ln X, on a :
E(X k ) = E(ekY ) = ΨY (k) = ekμ+

k2 σ 2
2

,

soit, en prenant k = 1 puis k = 2 :
1

E(X) = eμ+ 2 σ

2

E(X 2 ) = e2μ+2σ
2

2
2

d’où : V (X) = e2μ+σ (eσ − 1) .
.

4.2.6

La loi de Pareto

Cette loi a été introduite pour modéliser la distribution de revenus
supérieurs à un seuil donné, puis s’est avérée utile pour d’autres phénomènes
(par exemple la distribution de la taille de grains de sable passés au travers d’un
tamis). Elle a deux paramètres strictement positifs : le paramètre de seuil a et
un paramètre de forme θ. La fonction de répartition et la fonction de densité
sont :
⎧
⎧
θ
θ+1
⎨ θ a
⎨ 1− a
si x ≥ a
si x ≥ a
x
a x
et f (x) =
.
F (x) =
⎩
⎩
0
si x < a
0
si x < a
La densité étant une puissance de x, on calcule aisément (voir exercices du
chapitre 2) :
θa
(n’existe que si θ > 1) ,
θ−1
θa2
V (X) =
(n’existe que si θ > 2).
(θ − 1)2 (θ − 2)
E(X) =

Sa fonction génératrice des moments n’existe pas (sa fonction caractéristique voir note 2.4 - ne s’exprime pas par des fonctions usuelles).

4.2.7

La loi de Weibull W (λ, α)

Cette loi généralise la loi exponentielle pour modéliser des durées de vie.
Elle intervient également dans les problèmes dits de valeurs extrêmes (par
exemple l’occurrence de crues exceptionnelles d’une rivière). La fonction de

62

Statistique − La théorie et ses applications

répartition et la fonction de densité de cette loi, notée W (λ, α) où λ et α sont
deux paramètres strictement positifs, sont :
"
"
α
α
si x ≥ 0
αλxα−1 e−λx
si x > 0
1 − e−λx
et f (x) =
.
F (x) =
0
si x < 0
0
si x ≤ 0
Quand α = 1 on a la loi E(λ), quand α < 1 la densité décroı̂t depuis +∞,
1/α
.
quand α > 1 elle admet un maximum (mode de la loi) au point [ λ1 ( α−1
α )]
On montre que :
E(X) =

Γ(1 + α1 )
λ1/α

,

V (X) =

Γ(1 + α2 ) − Γ2 (1 + α1 )
λ2/α

où Γ est la fonction gamma d’Euler (voir section 4.2.3).
Montrons quelques particularités utiles pour la modélisation de durées de
vie.
Proposition 4.5 Si X ; E(λ) alors X 1/α suit une loi W (λ, α) .
Cette proposition est évidente par le principe du changement de variable exposé en section 1.6. Ainsi pour α > 1 cela revient à une contraction de l’échelle
du temps et donc à introduire un eﬀet d’usure. Considérons en eﬀet, comme
nous l’avons fait pour la loi exponentielle, la probabilité qu’un système «survive» un temps h ﬁxé (h > 0) au-delà du temps t et étudions cette probabilité
comme une fonction ρ de t. On a :
ρ(t) = P (X > t + h|X > t) =

α
α
P (X > t + h)
= e−λ[(t+h) −t ] .
P (X > t)

La fonction (t + h)α − tα étant croissante pour α > 1 et décroissante pour
α < 1, la probabilité diminue avec le temps pour α > 1 ce qui correspond bien
à un phénomène d’usure. Au contraire pour α < 1 on a une probabilité qui
augmente (on peut penser ici à la durée de chômage où plus le temps s’écoule
plus il est diﬃcile d’en sortir).

4.2.8

La loi de Gumbel

C’est une autre loi de modélisation de valeurs extrêmes dont la fonction de
répartition est :
!
x−α
−
, x ∈ R (β > 0) .
F (x) = exp −e β
On montre que sa moyenne est α + γβ, où γ =0,577... est la constante d’Euler,
et que sa variance est π 2 β 2 /6.
La valeur α correspond à son mode. Sa fonction génératrice des moments
est Ψ(t) = eαt Γ(1 − βt). Elle est liée à la loi limite du maximum d’une série de
n observations quand n tend vers l’inﬁni, pour une grande variété de lois.

Chapitre 4. Les lois de probabilités usuelles

4.2.9

63

La loi bêta Beta(α, β)

Cette loi fournit un modèle pour les mesures comprises entre 0 et 1, en
particulier pour des taux ou des proportions. Sa densité est :
⎧
Γ(α + β + 2)
⎪
⎨
xα (1 − x)β si x ∈ ]0; 1[
Γ(α + 1)Γ(β + 1)
f (x) =
⎪
⎩
0
si x ∈]0;
/ 1[
avec α > −1 et β > −1.
Pour α = β = 0 on a la loi uniforme U[0, 1]. Pour α et β strictement positifs
elle admet un mode en x = α/(α + β).
Sachant que, pour tout α > −1 et tout β > −1, on a :


1

xα (1 − x)β dx =

0

Γ(α + 1)Γ(β + 1)
,
Γ(α + β + 2)

on calcule aisément, pour X ; Beta(α, β) :
α+1
α+β+2
(α + 1)(β + 1)
.
V (X) =
(α + β + 2)2 (α + β + 3)
E(X) =

4.3

Génération de nombres issus d’une loi
donnée

Il n’est pas toujours possible d’étudier de façon analytique le comportement de modèles, d’estimateurs ou de statistiques de tests en raison de leur
complexité. Dans ce cas on recourt à des simulations d’échantillons pour
suppléer à l’absence d’éléments théoriques et nous nous réfèrerons parfois, dans
les chapitres ultérieurs, à des résultats obtenus de cette façon. Cette approche
constitue l’essence de la méthode de Monte-Carlo.
Tous les logiciels oﬀrant des possibilités de calcul disposent d’un générateur
de «nombres au hasard» (fonction RANDOM, ALEA, etc., voir section 4.2.1)
qui correspondent à des observations issues d’une loi U[0 , 1] ou que l’on peut
considérer comme telles. Car, en réalité, ces nombres que l’on qualiﬁe plutôt de
pseudo-aléatoires, sont engendrés par un mécanisme purement déterministe.
Supposons que l’on veuille générer des réalisations d’une loi continue de
fonction de répartition F strictement croissante et que l’on dispose de la fonction inverse F −1 , soit de façon analytique, soit de façon numérique (de nombreux logiciels statistiques ou autres proposent, par exemple, les fonctions

64

Statistique − La théorie et ses applications

«Gauss-inverse», «exponentielle-inverse», etc.). Étant donné une variable aléatoire U de loi U[0 , 1], considérons la fonction X = F −1 (U ) et déterminons sa
fonction de répartition en appliquant la méthode de la section 1.6. On a :
P (X ≤ x)

= P (F −1 (U ) ≤ x)
= P (U ≤ F (x)) = F (x)

puisque, pour la loi uniforme, P (U ≤ u) = u. Donc X suit la loi F.
Ainsi, à partir d’une suite de nombres au hasard u1 , u2 , · · · , un on peut obtenir une suite de nombres x1 , x2 , · · · , xn issus de la loi F , par la transformation
xi = F −1 (ui ).
Pour la loi E(λ), par exemple, F (x) = 1 − e−λx et la fonction inverse est
explicite : F −1 (x) = − λ1 ln(1 − x). On utilisera alors la transformation xi =
− λ1 ln(1 − ui ).
Pour une loi discrète la méthode ci-dessus n’est pas applicable du fait que F,
étant une fonction en escalier, n’est pas inversible. Dans le cas où le nombre de
valeurs possibles a1 < a2 < · · · < ak < · · · < ar est restreint on peut l’adapter
de la façon suivante :
si
si
si

ui ∈ [0, F (a1 )[ alors générer xi = a1
···
ui ∈ [F (ak−1 ), F (ak )[ alors générer xi = ak
···
ui ∈ [F (ar−1 ), 1] alors générer xi = ar ,

le choix d’ouvrir ou de fermer chaque intervalle d’un côté ou de l’autre n’ayant
pas d’importance si les ui sont générés avec suﬃsamment de décimales.
En particulier on peut produire un processus de Bernoulli de paramètre p
en donnant la valeur 1 si ui < p et 0 si ui ≥ p.
Il existe toutefois des méthodes de génération adaptées et plus eﬃcaces
pour chaque loi que l’on trouvera dans les ouvrages consacrés spéciﬁquement à
la simulation.

4.4

Exercices

Exercice 4.1 En transformant linéairement la v.a. de la marche aléatoire (voir
exercices du chapitre 3) en une v.a. de Bernoulli, établir la loi de cette marche
aléatoire après n pas.
Exercice 4.2 Montrer directement que la fonction génératrice des moments
Ψ(t) de la loi binomiale négative est pr /[1 − (1 − p)et ]r et qu’elle est déﬁnie
pour t < − ln(1 − p).

Chapitre 4. Les lois de probabilités usuelles

65

Aide : substituer à (1 − p)x l’expression [(1 − p)et ]x .
En écrivant que la somme des termes de la fonction de probabilité vaut 1 et en
dérivant terme par terme par rapport à p, déduire l’expression de la moyenne
de la loi.
Exercice 4.3 * (Approche historique de Moivre mettant en évidence la loi
de Gauss) Soit X ; B(n, p), montrer que pour n → ∞, p restant ﬁxe, la
probabilité P (X) = x est équivalente à la fonction de densité en x de la loi
de U ; N (np, np(1 − p)) ou encore à P (x − 12 < U < x + 12 ). On admettra
intuitivement que les valeurs d’intérêt pour x (à probabilités non négligeables)
tendent vers l’inﬁni quand n → ∞ et que, pour ces valeurs, nx → p.
2
:
On utilisera pour
démonstration la formule de
√ la−n
√Stirling
n+ 12
−n n+ 12
n! = 2πe n
(1 + o(1)) soit n! ∼ 2πe n
Exercice 4.4 Soit X ; B(n, p). Montrer que si n → ∞ et p → 0 de façon que
np reste constant, alors P (X = x) tend vers la probabilité correspondante de
la loi de Poisson de paramètre np.
n!
Aide : on admettra que (n−x)!n
x → 1 quand n → ∞.
Exercice 4.5 Soit X ; H(N, M, n). Montrer que, quand N → ∞ et M
N → p
(non nul), P (X = x) tend vers la probabilité correspondante de la loi B(n, p).
Aide : comme pour l’exercice précédent.
Exercice 4.6 Soient X1 et X2 deux v.a. indépendantes de Poisson de paramètres respectifs λ1 et λ2 . Montrer que la loi conditionnelle de X1 sachant
X1 + X2 = n est une loi binomiale.
Exercice 4.7 Soit X ; G(p). Déterminer P (X > n) et montrer que la probabilité P (X > n+k | X > n) est indépendante de n. [Note : Ceci est à rapprocher
de la propriét́é analogue de la loi E(λ). La loi G(p) peut modéliser la durée de
vie d’un système sans usure, en temps discret d’intervalles réguliers].
Exercice 4.8 Soit X ; U[0, 1], montrer que Y = (b − a)X + a suit une loi
U[a, b].
Exercice 4.9 Montrer que la fonction génératrice des moments de la loi Γ(r, λ),
pour r > 0 non nécessairement entier, est Ψ(t) = [λ/(λ − t)]r . Pour quelles valeurs de t est-elle déﬁnie ? En déduire sa moyenne et sa variance. [Rappel sur
 +∞
la fonction gamma d’Euler : Γ(r) = 0 xr−1 e−x dx avec r > 0].
Exercice 4.10 En s’appuyant sur un processus de Poisson sous-jacent, déterminer pour r entier la fonction de répartition de la loi Γ(r, λ). En déduire sa
densité.
2 Dans l’expression de cette formule le terme o(1) indique une fonction qui devient
négligeable devant 1 (donc qui tend vers zéro) quand n tend vers l’inﬁni.

66

Statistique − La théorie et ses applications

Exercice 4.11 Soit X ; Γ(r, λ), montrer que rX suit une loi Γ(r, λr ). Montrer
que λX suit une loi Γ(r, 1).
Exercice 4.12 Montrer que si X suit une loi de Pareto dont le paramètre de
seuil a est égal à 1, alors lnX suit une loi exponentielle.
Exercice 4.13 Le temps moyen de service à un distributeur de billets est de 30
secondes. Vous arrivez et trouvez cinq personnes en attente (la première venant
juste d’accéder au guichet). Quelle est la probabilité que vous attendiez moins
de 30 secondes (on supposera être en présence d’un processus de Poisson) ?
Aide : on utilisera le deuxième résultat de l’exercice 4.11 et on établira une
1
relation de récurrence pour In = 0 xn e−x dx en intégrant par parties.
Exercice 4.14 Pour un projet de construction d’un immeuble de 20 logements, on étudie la capacité nécessaire du parking. On note X la variable
«nombre de voitures d’un ménage». Pour tout ménage on admet que la probabilité d’avoir une voiture est 0,70 et celle d’avoir 2 voitures est de 0,30 (on
néglige toute autre possibilité). On supposera l’indépendance du nombre de
voitures entre les ménages.
On pose Y = X − 1. Quelle est la loi de Y ?
Quelle est la loi de la somme de 20 variables i.i.d. de même loi que Y ? En
déduire la probabilité qu’un parking de 29 places soit suﬃsant pour les 20
ménages.
Exercice 4.15 Grâce à une importante étude épidémiologique on constate que
la distribution des poids des individus dans une population adulte donnée peut
être convenablement modélisée par une loi lognormale. Considérant que le poids
moyen est de 70 kg et que l’écart-type des poids est de 12 kg résoudre les deux
équations permettant de déterminer les valeurs des paramètres μ et σ 2 de la
loi lognormale.

Chapitre 5

Lois fondamentales de
l’échantillonnage

5.1

Phénomènes et échantillons aléatoires

Nous entrons maintenant véritablement dans le domaine de la statistique
en nous penchant sur l’étude d’observations répétées issues d’un certain
phénomène de nature aléatoire.
Schématiquement, on peut distinguer deux classes de phénomènes aléatoires. D’une part l’aléatoire peut être provoqué expérimentalement comme,
par exemple, dans les jeux de hasard ou dans les mécanismes de tirage au
sort «d’individus» dans des «populations1 » ﬁnies pour les sondages, pour le
contrôle de qualité, etc.(voir section 3.7). Dans ce contexte expérimental la notion d’expérience aléatoire, point de départ de la modélisation probabiliste, a
un sens tout à fait réel.
D’autre part, on peut aussi recourir à une modélisation aléatoire lorsqu’on
est incapable de prévoir avec exactitude les réalisations d’un phénomène. Le
caractère aléatoire est simplement attribué au phénomène pour reﬂéter l’incertitude de l’observateur par rapport à un ensemble de résultats possibles, par
exemple le nombre d’appels parvenant à un standard téléphonique dans une
unité de temps, la durée de vie d’un appareil, etc. Il n’y a pas ici d’expérience
aléatoire à proprement parler. Toutefois il est nécessaire, pour l’approche statistique, de pouvoir observer le phénomène de façon répétée aﬁn de constituer
des échantillons.
1 Nous mettons ces termes entre guillemets car ils sont à prendre dans un sens large et non
uniquement par référence à des populations humaines. A proprement parler les «individus»
sont des unités statistiques qui peuvent être les entreprises d’un secteur d’activité, les arbres
d’une forêt, les pièces d’un lot de production, etc. La population est aussi appelée univers.

68

Statistique − La théorie et ses applications

Déﬁnition 5.1 On appelle échantillon aléatoire de taille n (en bref néchantillon) une suite de n variables aléatoires indépendantes et de même loi
(ou v.a. i.i.d). Cette loi est appelée la loi mère de l’échantillon.
Cette déﬁnition appelle quelques remarques.
– Mathématiquement la notion d’échantillon aléatoire est identique à celle
de v.a. i.i.d., et l’usage de ce terme ne se justiﬁe qu’en raison du contexte
de l’échantillonnage. Sauf mention contraire, quand on parlera d’échantillon dans cet ouvrage, il s’agira implicitement d’une suite de v.a. i.i.d.
– Il sera commode d’associer à la loi mère un symbole de v.a., par exemple
X, le n-échantillon étant alors désigné par X1 , X2 , ..., Xn . Ainsi on peut
écrire que pour tout i = 1, ..., n, E(Xi ) = E(X) qui représente la moyenne
de la loi mère.
– On parle souvent, en lieu et place de la loi mère, de la distribution de
la population - voire même simplement de la population - en référence
à un sondage. Certes ce terme est abusif car, d’une part on y confond
les individus et les valeurs numériques observables sur ces individus et,
d’autre part, il n’existe pas nécessairement une population réelle (quelle
est la population des appels à un standard, des produits d’un certain type
manufacturés par une entreprise ?). Toutefois il nous arrivera de recourir
à ce terme comme s’il existait une sorte de population virtuelle dont les
observations seraient issues comme par un tirage au hasard.
– Le statut de v.a. i.i.d. exige que le phénomène soit invariant au cours
des observations successives et que ces observations n’exercent aucune
inﬂuence entre elles. Il s’agit bien souvent d’une profession de foi, ces
conditions n’étant généralement pas rigoureusement vériﬁables, ni rigoureusement vériﬁées.
– Pour ce qui est des notations on distinguera la notion d’échantillon
aléatoire X1 , X2 , ..., Xn dont on peut dire qu’elle se réfère à des résultats
potentiels avant expérience ou a priori, de celle d’échantillon réalisé
x1 , x2 , ..., xn correspondant aux valeurs observées après expérience ou a
posteriori.
L’objectif de ce chapitre est d’étudier certaines caractéristiques de l’échantillon aléatoire, essentiellement sa moyenne et sa variance, en relation avec
celles de la loi mère. A priori (au sens de la remarque précédente) une telle
caractéristique est une v.a. qui prend le nom de «statistique» dans le contexte
de l’échantillonnage, selon la déﬁnition suivante.
Déﬁnition 5.2 Soit X1 , X2 , ..., Xn un n-échantillon, on appelle statistique
toute v.a. Tn = h(X1 , X2 , ..., Xn ), fonction de X1 , X2 , ..., Xn .
On peut concrétiser la loi d’une statistique (donc d’une caractéristique, telle
la moyenne de l’échantillon) en imaginant une simulation en très grand nombre

Chapitre 5. Lois fondamentales de l’échantillonnage

69

d’échantillons de taille n, en calculant pour chacun d’eux la valeur prise par
la statistique et en étudiant la distribution de ces valeurs. De façon imagée on
peut dire qu’il s’agit de la distribution d’échantillonnage de la statistique sur
«l’univers» de tous les échantillons possibles. Notons qu’une statistique peut
être une fonction à valeurs dans R, R2 ou Rp . En particulier les moments empiriques ci-après sont à valeurs dans R. Les déﬁnitions qui suivent se rapportent
toutes à un échantillon aléatoire noté X1 , X2 , ..., Xn .

5.2

Moyenne, variance, moments empiriques

Déﬁnition 5.3 On appelle moyenne de l’échantillon ou moyenne empirique la statistique, notée X̄, déﬁnie par :
1
Xi .
n i=1
n

X̄ =

Déﬁnition 5.4 On appelle variance empirique la statistique, notée S'2 ,
déﬁnie par :
n
1
(Xi − X̄)2 .
S'2 =
n i=1
Nous commençons maintenant à établir certaines relations entre les lois de
ces statistiques et la loi mère.
Proposition 5.1 Soit μ et σ 2 , respectivement la moyenne et la variance de la
loi mère. On a :
σ2
.
E(X̄) = μ ,
V (X̄) =
n
En eﬀet :

1
1
1
Xi ) =
E(Xi ) =
μ = μ.
n i=1
n i=1
n i=1
n

E(

n

n

Puis, en raison de l’indépendance des Xi :
V(

n
n
n
1 
1  2
nσ 2
σ2
1
.
Xi ) = 2
V (Xi ) = 2
σ = 2 =
n
n
n
n
n
i=1

i=1

i=1

Proposition 5.2 La moyenne de la loi de la variance empirique est :
E(S'2 ) =

n−1 2
σ .
n

Statistique − La théorie et ses applications

70

En eﬀet :
1
1
(Xi − μ) − (X̄ − μ)
(Xi − X̄)2 =
n
n
n

i=1

n

2

i=1

n
n
1
1
=
(Xi − μ)2 − 2(X̄ − μ)
(Xi − μ) + (X̄ − μ)2
n
n
i=1

i=1

n
1
=
(Xi − μ)2 − 2(X̄ − μ)2 + (X̄ − μ)2
n i=1

1
(Xi − μ)2 − (X̄ − μ)2 .
n i=1
n

=
D’où :
E(S'2 ) =

1
σ2
n−1 2
V (Xi ) − V (X̄) = σ 2 −
=
σ .
n i=1
n
n
n

Quand on abordera l’estimation ponctuelle (chapitre 6) on dira que S'2
est un estimateur biaisé de σ 2 . Par anticipation déﬁnissons l’estimateur S 2 =
n '2
2
2
2
n−1 S qui est sans biais pour σ , c’est-à-dire tel que E(S ) = σ .
Déﬁnition 5.5 On appelle variance de l’échantillon la statistique
1 
(Xi − X̄)2 .
n − 1 i=1
n

S2 =

Dorénavant on étudiera S 2 plutôt que S'2 à laquelle on pourra éventuellement
se référer en conservant le terme de variance empirique.
En prenant la racine carrée de S 2 (respectivement de S'2 ) on déﬁnit l’écart'
type S de l’échantillon (respectivement, l’écart-type empirique S).

Cas particulier : loi mère gaussienne
Si la loi mère est N (μ, σ 2 ) alors X̄ est gaussienne, en tant que combinaison
linéaire de gaussiennes indépendantes (voir proposition 4.4). Par conséquent :
X̄ ; N (μ,

σ2
).
n

La loi de S 2 sera vue dans la section 5.3. Par ailleurs, nous admettrons la
proposition suivante.
Proposition 5.3 Si la loi mère est gaussienne, X̄ et S 2 sont des v.a. indépendantes.

Chapitre 5. Lois fondamentales de l’échantillonnage

71

Note 5.1 : D’une façon générale, la loi de X̄ (et a fortiori celle de S 2 ), pour
une loi mère quelconque, n’est pas facile à identiﬁer. Des cas particuliers seront
vus en exercice (loi mère exponentielle, loi mère de Cauchy). Si la loi mère est
de Bernoulli B(p) ou
nde Poisson P(λ) on peut déduire la loi de X̄ à partir de
celle de la somme i=1 Xi = nX̄ qui est, respectivement, une loi binomiale
B(n, p) ou une loi de Poisson P(nλ) selon les propriétés de ces lois vues au
chapitre 4.
Déﬁnition 5.6 On appelle moment empirique d’ordre r, noté Mr , la statistique
n
1 r
X .
Mr =
n i=1 i
Déﬁnition 5.7 On appelle moment centré empirique d’ordre r, noté Mr ,
la statistique
n
1
(Xi − X̄)r .
Mr =
n
i=1

Proposition 5.4 Si la loi mère admet un moment μr d’ordre r (voir déﬁnition
2.2) alors :
E(Mr ) = μr .
Ceci découle directement du fait que, pour tout i = 1, ..., n, E(Xir ) =
E(X r ) = μr (où X est le symbole de v.a. associé à la loi mère). On verra en
fait, plus loin, qu’un moment empirique est un «estimateur» naturel du moment
de même ordre de la loi (appelé parfois, par contraste, moment théorique). En
revanche, comme on l’a vu pour la variance empirique S'2 qui correspond au
moment empirique centré d’ordre 2, E(Mr ) n’est pas nécessairement égal à μr ,
le moment centré théorique de même ordre. Ceci résulte du fait que le centrage
est eﬀectué avec la moyenne de l’échantillon X̄ et non pas avec la vraie moyenne
μ de sa loi. Les moments centrés empiriques s’expriment, par développement
des (Xi − X̄)r , en fonction des moments empiriques simples de la même façon
que le font les moments théoriques entre eux, puisqu’il s’agit alors de développer
(X − μ)r .
En particulier, on a la formule de décentrage de la variance empirique :
n
n
1 2
1
(Xi − X̄)2 =
X − (X̄)2
n i=1
n i=1 i
qui fait le pendant de celle de la section 2.3 : V (X) = E(X 2 ) − μ2 .

72

Statistique − La théorie et ses applications

Note 5.2 : En considérant un n-échantillon de couples d’observations
(X1 , Y1 ), ..., (Xn , Yn ) on peut déﬁnir des moments croisés empiriques d’ordres
p et q quelconques et leurs correspondants centrés :
1 p q
X Y
n i=1 i i
n

1
(Xi − X̄)p (Yi − Ȳ )q .
n i=1
n

et

Pour p = q = 1 le moment centré est la covariance empirique utilisée en
statistique descriptive :
1
(Xi − X̄) (Yi − Ȳ ) .
n i=1
n

1
Comme pour la variance on introduit le facteur n−1
au lieu de n1 pour éliminer
le biais vis-à-vis de la covariance théorique (voir déﬁnition 3.5).

Par analogie avec la déﬁnition 3.6 de la corrélation linéaire on déﬁnit la
corrélation linéaire empirique en divisant la covariance empirique par le produit
des écarts-types empiriques des v.a. X et Y , soit après simpliﬁcation :
n
(Xi − X̄) (Yi − Ȳ )
( i=1
,
n
n
2
2
(X
−
X̄)
(Y
−
Y
)
i
i=1
i=1 i
formule bien connue en statistique descriptive.
Nous abordons maintenant trois lois omniprésentes en statistique car liées
aux distributions d’échantillonnage de moyennes et de variances dans le cas
gaussien.

5.3

Loi du Khi-deux

Déﬁnition 5.8 Soit Z1
, Z2 , ..., Zν une suite de variables aléatoires i.i.d. de loi
ν
2
N (0 ; 1). Alors la v.a.
i=1 Zi suit une loi appelée loi du Khi-deux à ν
degrés de liberté, notée χ2 (ν).

Proposition 5.5 La densité de la loi du Khi-deux à ν degrés de liberté est :
fν (x) =

ν
x
1
2 −1 e− 2
pour x > 0 (0 sinon).
ν x
2 Γ( 2 )
ν
2

Chapitre 5. Lois fondamentales de l’échantillonnage

73

Démonstration : calculons la fonction génératrice de Z 2 où Z ; N (0 ; 1). On
a:
ΨZ 2 (t)

2

= E(etZ ) =

+∞


1
2
z2
etz √ e− 2 dz
2π
−∞

 − 1 (1−2t)z2
1 +∞
= √
e 2
dz
2π −∞
=√
=√

 − 1 u2
√
1 +∞
1
√
e 2 du en posant u = 1 − 2t z
1 − 2t 2π −∞

1
1 − 2t

qui est déﬁnie pour t < 12 .
Pour la somme des Zi2 indépendantes on a donc (voir proposition 3.12) :
Ψ νi=1 Zi2 (t) = (

ν
1
)2
1 − 2t

qui n’est autre que la fonction génératrice d’une loi Γ( ν2 , 12 ) vue en section
4.2.3, dont la densité est bien celle de la proposition ci-dessus. On voit donc,
au passage, qu’une loi du Khi-deux est un cas particulier de loi gamma.

Proposition 5.6 La moyenne de la loi χ2 (ν) est égale au nombre de degrés
de liberté ν, sa variance est 2ν.
Repartons de la déﬁnition de la loi χ2 (ν). Comme Zi ; N (0 ; 1) on a :
E(Zi2 ) = V (Zi ) = 1

d’où

ν

E(
Zi2 ) = ν
i=1

V (Zi2 ) = E(Zi4 ) − (E(Zi2 ))2 = μ4 − 1.

n

Or d’après un résultat de la section 4.2.4, μ4 = 3 d’où V (Zi2 ) = 2 et V (
= 2ν.

i=1

Zi2 )

Proposition 5.7 Si T1 ; χ2 (ν1 ), T2 ; χ2 (ν2 ), T1 et T2 indépendantes, alors
T1 + T2 ; χ2 (ν1 + ν2 ).
Cette proposition est évidente de par la déﬁnition de la loi du Khi-deux.
Nous revenons maintenant sur la loi de S 2 dans le cas d’un échantillon de loi
mère gaussienne.
Théorème 5.1 Soit un n-échantillon X1 , X2 , ..., Xn de loi N (μ, σ 2 ) on a :
(n − 1)S 2
; χ2 (n − 1).
σ2

74

Statistique − La théorie et ses applications

Démonstration : en reprenant les développements qui suivent l’énoncé de la
proposition 5.2, on établit que :
n


(Xi − μ)2

=

i=1
n


soit

i=1



Xi − μ
σ

n


(Xi − X̄)2 + n(X̄ − μ)2 ,

i=1

n

2

i=1 (Xi
σ2

=

− X̄)2


+

X̄ − μ
√
σ/ n

2
.

2

X̄−μ
√ qui
et le carré de σ/
Les deux termes de droite sont, respectivement, (n−1)S
σ2
n
est une gaussienne centrée-réduite. Ces termes aléatoires étant indépendants
(comme fonctions de S 2 et de X̄, voir proposition 5.3) et les v.a. Xiσ−μ étant
indépendantes de loi N (0 ; 1) on a, en termes de fonctions génératrices :



1
1 − 2t

n/2


= Ψ (n−1)S2 (t).
σ

Finalement :


Ψ (n−1)S2 (t) =
σ

1
1 − 2t

1
1 − 2t

1/2
(si t <

1
).
2

 n−1
2
,


ce qui prouve le théorème.

Sachant que l’espérance d’une loi χ2 (n − 1) est n − 1 et sa variance 2(n − 1),
on voit que :
2σ 4
E(S 2 ) = σ 2 et V (S 2 ) =
.
n−1
En fait la loi du Khi-deux a un usage beaucoup plus vaste en statistique notamment dans la théorie des tests comme nous le verrons au chapitre 9. Toutes
ses applications reposent sur des sommes de carrés de termes gaussiens ou approximativement gaussiens. Notons, ﬁnalement, que la fonction de répartition
de la loi (ou plutôt de la famille de lois) du Khi-deux ne s’explicite pas et
que l’on doit recourir à des tables ou à une fonction ad hoc dans les logiciels
statistiques pour le calcul de probabilités.

5.4

Loi de Student

Déﬁnition 5.9 Soit Z et Q deux v.a. indépendantes telles que Z ; N (0 ; 1)
et Q ; χ2 (ν). Alors la v.a.
Z
T =(
Q
ν

suit une loi appelée loi de Student à ν degrés de liberté, notée t(ν).

Chapitre 5. Lois fondamentales de l’échantillonnage

75

Proposition 5.8 La densité de la loi de Student à ν degrés de liberté est :
Γ( ν+1 )
f (x) = √ 2 ν
πν Γ( 2 )



x2
1+
ν

− ν+1
2

, x ∈ R.

Ce résultat, que nous ne démontrons pas, est dû à W.S. Gosset en 1908,
qui prit le pseudonyme de Student. Ni la fonction de répartition, ni la fonction
génératrice ne s’explicitent. Il existe donc des tables de la fonction de répartition
ou une fonction ad hoc dans les logiciels statistiques. On admettra encore la
proposition suivante.
Proposition 5.9 Soit T ; t(ν) alors E(T ) = 0 si ν ≥ 2 et V (T ) =
ν ≥ 3.

ν
ν−2

si

Le fait que la moyenne est nulle est évident puisque la densité est une
fonction paire. On notera que la variance vaut 3 dès qu’elle est déﬁnie (ν = 3)
et tend vers 1 quand ν → +∞. Pour être plus précis, l’allure de la loi de Student
est similaire à celle d’une loi de Gauss centrée-réduite avec un étalement un
peu plus fort, cette diﬀérence s’estompant rapidement lorsque ν s’accroı̂t et
devenant négligeable pour ν > 200. Ceci s’explique, en fait, par sa déﬁnition
même mettant en jeu une v.a. N (0 ; 1) au numérateur et une v.a. qui converge
en probabilité (voir cette notion en section 5.8.1) vers 1, au dénominateur.
Pour ν = 1 la loi est la loi de Cauchy. Selon la déﬁnition 5.9, c’est la loi du
rapport de deux gaussiennes centrées et réduites indépendantes (cette déﬁnition
impose en fait de prendre la valeur absolue de la variable du dénominateur,
mais cette restriction peut être levée). Sa moyenne n’existe pas en raison de ses
“queues de distribution” pesantes (voir exemple 2.1), ce qui lui confère certaines
particularités, par exemple que la loi des grands nombres (voir section 5.8.2)
ne s’applique pas (voir exercices).
Théorème 5.2 (Application fondamentale) Soit X1 , X2 , ..., Xn un n-échantillon de loi mère N (μ, σ 2 ). Alors :
X̄ − μ
√ ; t(n − 1).
S/ n
La démonstration est immédiate en prenant, avec les notations utilisées
pour la déﬁnition 5.9,
Z=

(n − 1)S 2
X̄ − μ
√
.
et Q =
σ/ n
σ2

Ce résultat, qui a motivé en réalité les travaux de Gosset, met en évidence la
modiﬁcation apportée à la loi N (0 ; 1) de la v.a. Z ci-dessus, lorsqu’on substitue
à l’écart-type théorique σ de la loi mère, l’écart-type de l’échantillon S. On

Statistique − La théorie et ses applications

76

comprend au passage, qu’introduisant un terme aléatoire supplémentaire, on
provoque un étalement plus grand. En fait les applications de la loi de Student
se rencontrent souvent en statistique dès lors qu’on est appelé à remplacer σ,
en général inconnu, par son «estimateur» naturel S.

5.5

Loi de Fisher-Snedecor

Déﬁnition 5.10 Soit U et V deux v.a. indépendantes telles que U ; χ2 (ν1 )
et V ; χ2 (ν2 ). Alors la v.a.
F =

U/ν1
V /ν2

suit une loi de Fisher-Snedecor à ν1 degrés de liberté au numérateur
et ν2 degrés de liberté au dénominateur, notée F (ν1 , ν2 ). En bref on
l’appellera loi de Fisher.
Proposition 5.10 La densité de la loi F (ν1 , ν2 ) est :
fν1 ,ν2 (x) =

2
Γ( ν1 +ν
2 )
ν1
Γ( 2 )Γ( ν22 )



ν1
ν2

 ν21

x
(1 +

Si ν2 ≥ 3 sa moyenne existe et est égale à
et est égale à

2ν22 (ν1 +ν2 −2)
ν1 (ν2 −2)2 (ν2 −4)

ν1 −2
2

ν1 +ν2
ν1
x) 2
ν2

ν2
ν2 −2 .

si x > 0 (0 sinon) .

Si ν2 ≥ 5 sa variance existe

.

Nous admettrons ces résultats sans démonstration, notant avec curiosité
que la moyenne, quand elle existe, ne dépend que des degrés de liberté du
dénominateur. La fonction de répartition (tout comme la fonction génératrice)
n’étant pas explicite il existe des tables ou des fonctions ad hoc dans les logiciels.
La proposition suivante permet une économie de tables.
Proposition 5.11 Soit H ; F (ν1 , ν2 ), alors

1
H

; F (ν2 , ν1 ).

Cette proposition est évidente de par la déﬁnition même de la loi de Fisher.
Montrons qu’il suﬃt, grâce à cette propriété, de disposer des quantiles
d’ordre supérieur à 0,50. Soit à calculer, par exemple, pour H ; F (ν1 , ν2 ),
le quantile d’ordre 0,05. On a :
P (H < h0,05 ) = 0,05


1
1
= 0,05
>
P
H
h0,05


1
1
P
<
= 0,95 .
H
h0,05

Chapitre 5. Lois fondamentales de l’échantillonnage

77

Il est donc égal à l’inverse du quantile 0,95 lu sur la loi F (ν2 , ν1 ). Plus généralement le quantile d’ordre α de la loi F (ν1 , ν2 ) est l’inverse du quantile d’ordre
1 − α de la loi F (ν2 , ν1 ).
Les applications de la loi de Fisher sont nombreuses en statistique dès lors
que l’on veut étudier le rapport de deux sommes de carrés de termes gaussiens
indépendants. L’application la plus directe concerne la loi du rapport des variances S12 /S22 de deux échantillons indépendants de tailles respectives n1 et n2 ,
issus de deux lois mères gaussiennes ayant une même variance σ 2 . En eﬀet :
(n1 − 1)S12
; χ2 (n1 − 1) ,
σ2
d’où immédiatement :

(n2 − 1)S22
; χ2 (n2 − 1),
σ2

S12
; F (n1 − 1, n2 − 1).
S22

On remarquera encore que si T ; t(ν) alors T 2 ; F (1 , ν).

5.6

Statistiques d’ordre

Cette notion est très utile dans une série de problèmes, notamment ceux
de minima et de maxima (voir les exercices) que nous abordons tout d’abord.
Comme précédemment nous considérons un échantillon aléatoire X1 , X2 , ..., Xn
dont la loi mère a pour fonction de répartition F .
Pour une série de nombres réels (x1 , x2 , ..., xn ) notons max{x1 , x2 , ..., xn }
la fonction de Rn dans R qui lui associe le nombre maximal de cette série. On
peut donc déﬁnir une v.a., notée X(n) , fonction de (X1 , X2 , ..., Xn ) par :
X(n) = max{X1 , X2 , ..., Xn }.
La fonction de répartition de cette statistique se déduit aisément de F . En
eﬀet l’événement (X(n) ≤ x) est équivalent à (X1 ≤ x, X2 ≤ x, ..., Xn ≤ x).
Par conséquent :
FX(n) (x) = P (X1 ≤ x , X2 ≤ x , ..., Xn ≤ x)
= P (X1 ≤ x)P (X2 ≤ x)...P (Xn ≤ x)
n
= [F (x)]
(même loi).

(indépendance)

De façon similaire on note X(1) = min{X1 , X2 , ..., Xn } la fonction minimum
et, en notant que l’événement (X(1) > x) est équivalent au fait que toutes les
Xi sont supérieures à x, on a :
P (X(1) > x) = P (X1 > x)P (X2 > x)...P (Xn > x)
n
= [1 − F (x)] ,

Statistique − La théorie et ses applications

78
d’où :

n

FX(1) (x) = 1 − [1 − F (x)] .
Déﬁnition 5.11 Soit hk la fonction de Rn dans R qui à (x1 , x2 , ..., xn ) fait correspondre la k-ième valeur parmi x1 , x2 , ..., xn lorsqu’on les range dans l’ordre
croissant.
On appelle alors statistique d’ordre k, la v.a. notée X(k) , déﬁnie par :
X(k) = hk (X1 , X2 , ..., Xn ).
Ceci généralise les notions de minimum (k = 1) et de maximum (k = n).
Proposition 5.12 La fonction de répartition de X(k) est :

n 

n
FX(k) (x) =
[F (x)]j [1 − F (x)]n−j .
j
j=k

Pour montrer cela il suﬃt de noter que l’événement {X(k) ≤ x} est équivalent
au fait qu’au moins k v.a. parmi X1 , ..., Xn soient inférieures à k. Soit X la
v.a. symbolisant la loi mère. Considérons l’expérience de Bernoulli avec pour
«succès» l’événement (X ≤ x) dont la probabilité est F (x). Le nombre de
v.a. parmi X1 , ..., Xn prenant une valeur inférieure ou égale à x est donc une
v.a. de loi binomiale B(n, F (x)). Pour obtenir la probabilité que ce nombre soit
au moins égal à k on est amené à sommer les termes de cette binomiale de k à n.

Note 5.3 Considérons X(i) et X(j) avec i < j. La v.a. U = X(j) − X(i) ne
peut prendre que des valeurs positives et donc P (U ≥ 0) = 1. De façon conventionnelle on écrira P (X(j) ≥ X(i) ) = 1 et même, de façon quelque peu rapide,
X(j) ≥ X(i) . Moyennant cette convention, il est possible, comme dans la plupart des ouvrages, de déﬁnir les statistiques d’ordre X(1) , X(2) , ..., X(n) par une
permutation de (X1 , X2 , ..., Xn ) telle que X(1) ≤ X(2) ≤ ... ≤ X(n) .

5.7

Fonction de répartition empirique

Nous abordons ici une variable aléatoire fonctionnelle, c’est-à-dire dont
les réalisations sont en fait des fonctions. Nous nous contenterons de l’étudier
en un point x ﬁxé pour rester dans le cadre des variables aléatoires prenant
leurs valeurs dans R. Au chapitre 7 traitant des estimateurs fonctionnels on
verra l’intérêt de la fonction de répartition empirique en tant qu’estimateur de
F au même titre que X̄ est un estimateur de μ, par exemple.

Chapitre 5. Lois fondamentales de l’échantillonnage

79

Déﬁnition 5.12 Pour tout x ∈ R, on appelle valeur de la fonction de répartition empirique en x, la statistique, notée Fn (x), déﬁnie par :
1
I(−∞,x] (Xi )
n i=1
n

Fn (x) =

où I(−∞,x] est la fonction indicatrice de l’intervalle (−∞, x], à savoir
I(−∞,x] (u) = 1 si u ∈ (−∞, x] et 0 sinon.
En d’autres termes Fn (x) est la v.a «proportion» des n observations
X1 , X2 , ..., Xn prenant une valeur inférieure ou égale à x. Chaque Xi ayant
une probabilité F (x) d’être inférieure ou égale à x, nFn (x) suit une loi binomiale B(n, F (x)). En conséquence Fn (x) est une v.a discrète prenant les valeurs
k
n , où k = 0, 1, ..., n, avec probabilités :


k
n
k
n−k
[F (x)] [1 − F (x)]
.
P (Fn (x) = ) = P (nFn (x) = k) =
k
n
Note 5.4 A l’issue de l’expérience d’échantillonnage, soit x1 , x2 , ..., xn la réalisation du n-échantillon X1 , X2 , ..., Xn . La fonction de répartition empirique
se réalise comme une fonction réelle déﬁnie sur tout R, croissant de 0 à 1 par
paliers avec un saut de «hauteur» n1 à chaque fois qu’une valeur observée xi
est atteinte.
On peut également la voir comme la fonction de répartition d’une loi discrète qui
donnerait la probabilité n1 à chacune des valeurs x1 , x2 , ..., xn . Cette vision permet
de faire le lien entre
 moment théorique et moment empirique. Le moment théorique
peut s’écrire μr = R xr dF (x) alors que le moment empirique s’écrit en remplaçant
F par Fn dans l’intégrale de Riemann-Stieltjes (introduite en note 2.1) : Mr =
R

xr dFn (x) =

5.8
5.8.1

1
n

n

i=1

xri .

Convergence, approximations gaussiennes,
grands échantillons
Les modes de convergence aléatoires

On considère ici une suite inﬁnie de v.a. {X1 , X2 , ..., Xn , ...} notée en bref
{Xn }. On peut déﬁnir plusieurs modes de convergence pour une telle suite. On
notera FXn la fonction de répartition de Xn .
Déﬁnition 5.13 On dit que {Xn } converge en loi vers la v.a. X si l’on a,
en tout x où sa fonction de répartition FX est continue,
lim FXn (x) = FX (x) ,

n→∞
L

et l’on note Xn −−−−→ X.
n→∞

80

Statistique − La théorie et ses applications

On dira aussi que la loi de X est la la loi limite ou asymptotique de la suite
{Xn }. En pratique la loi limite sera utile pour donner une approximation
pour le calcul de la probabilité d’un événement sur Xn quand n sera
assez grand :
P (Xn ∈ A)  P (X ∈ A).
Pour la convergence en loi comme pour les autres modes de convergence un
cas particulier important est celui où X est une v.a. certaine, c’est-à-dire que
la suite converge vers une constante c. Pour la convergence en loi cela implique
que FXn (x) converge vers 0 si x < c et vers 1 si x ≥ c.
On admettra la proposition suivante, où l’on suppose que les fonctions
génératrices existent dans un voisinage de 0.
Proposition 5.13 La suite de v.a. {Xn } converge en loi vers X si et seulement
si, pour tout t dans un voisinage de 0, limn→∞ ΨXn (t) = ΨX (t), où ΨXn est
la fonction génératrice de Xn et ΨX celle de X.
Cette proposition permet donc d’établir la convergence en loi à partir de la
convergence de la fonction génératrice des moments.
Déﬁnition 5.14 On dit que {Xn } converge en probabilité (ou converge
faiblement) vers la v.a. X si, quel que soit  > 0 donné,
lim P (|Xn − X| < ) = 1 ,

n→∞
p

et l’on note Xn −−−−→ X.
n→∞

Pour ce mode de convergence comme pour les suivants la convergence vers
une constante c s’explicite naturellement en remplaçant X par c.
Déﬁnition 5.15 On dit que {Xn } converge presque sûrement (ou converge
avec probabilité 1, ou converge fortement) vers la v.a. X si, quel que soit  > 0
donné,
lim P ( sup {|Xm − X|} < ) = 1 ,
n→∞

m≥n

p.s.

et l’on note Xn −−−−→ X.
n→∞

Cette déﬁnition est complexe mais on peut voir qu’elle équivaut à dire que la
suite {Mn }, où Mn = sup {|Xm − X|}, converge vers 0 en probabilité. Comme
m≥n

pour tout n, sup {|Xm −X|} ≥ |Xn −X|, il est clair que la convergence presque
m≥n

sûre entraı̂ne la convergence en probabilité (d’où les qualiﬁcatifs de convergence
forte et convergence faible).
On admettra les propositions ci-après, qui pourront nous être utiles par la
suite.

Chapitre 5. Lois fondamentales de l’échantillonnage

81

p.s.

Proposition 5.14 Soit {Xn } telle que Xn −→ X et g une fonction continue
alors :
p.s.
g(Xn ) −−−−→ g(X).
n→∞

p.s.

p.s.

Proposition 5.15 Soit {Xn } telle que Xn −−→ X et {Yn } telle que Yn −−→ Y .
Si f est continue dans R2 alors :
p.s.

f (Xn , Yn ) −−→ f (X, Y ).
Ces deux propositions sont également vraies pour la convergence en probabilité. Elles s’étendent également à des fonctions de k variables aléatoires où
k > 2.
Déﬁnition 5.16 On dit que {Xn } converge en moyenne quadratique vers
la v.a. X si les v.a. X, X1 , X2 , ... ont un moment d’ordre 2 et si
lim E[(Xn − X)2 ] = 0 ,

n→∞
m.q.

et l’on note Xn −−−→ X.
La convergence m.q. est particulièrement facile à manipuler car elle repose sur la convergence usuelle d’une suite de nombres {E[(Xn − X)2 ]}. Nous
y recourrons abondamment, d’autant plus qu’elle implique la convergence en
probabilité.
On admettra la hiérarchie d’implications suivantes (voir certaines démonstrations proposées en exercices) entre les diﬀérents modes de convergence :
p⇒L
m.q. ⇒ p
p.s. ⇒ p (vu ci-dessus).
En outre p ⇐⇒ L dans le cas de la convergence vers une constante. Notons
que, dans le cas général, il n’y a pas, entre convergence m.q. et convergence
p.s., de domination de l’une sur l’autre.

5.8.2

Lois des grands nombres

Théorème 5.3 Soit {Xn } une suite de v.a. indépendantes de même loi admettant une moyenne μ et une variance σ 2 . Alors la suite des moyennes empiriques
{X¯n } converge presque sûrement vers μ, i.e. :
p.s.
X¯n −−−−→ μ.
n→∞

82

Statistique − La théorie et ses applications

Nous énonçons ici la loi dite «forte» des grands nombres. Il existe diﬀérentes
versions de cette loi requérant des conditions plus ou moins restrictives que
celles utilisées ici, dont la loi «faible» concernant la convergence en probabilité.
D’un point de vue concret la loi des grands nombres garantit que la moyenne
empirique se rapproche de plus en plus de la moyenne de la loi dont est issu
l’échantillon quand on augmente la taille de cet échantillon. Aussi, comme on
le verra plus loin, la moyenne empirique X¯n peut-elle prétendre à «estimer» μ.
Historiquement la loi des grands nombres a été introduite par Jakob
Bernoulli (publication posthume Ars conjectandi en 1713) pour déﬁnir la probabilité d’un événement comme étant la limite de sa fréquence relative, au
cours d’une série de répétitions d’une expérience aléatoire à l’inﬁni. Il s’agit
là du cas particulier où les v.a. X1 , X2 , ..., Xn , ... sont les variables indicatrices
de l’occurrence de l’événement(succès) au cours d’un processus de Bernoulli
n
(voir section 4.1.3). Soit Sn = i=1 Xi le nombre total de succès au cours des
n premières répétitions, la fréquence relative des occurrences est la moyenne
empirique Sn /n et donc :
Sn p.s.
−−−−→ p
n n→∞
où p est la probabilité de l’événement. La théorie axiomatique moderne des
probabilités permet d’établir cette propriété originelle intuitive en précisant des
conditions pour qu’elle s’applique, à savoir, dans la version usuelle présentée
ici : indépendance des répétitions successives et constance de la probabilité de
succès au cours de ces répétitions.
Nous nous bornons à montrer la convergence de X¯n vers μ en moyenne
quadratique qui, rappelons-le, garantit la convergence en probabilité. D’après
2
la proposition 5.1 E(X¯n ) = μ pour tout n, d’où E[(X¯n − μ)2 ] = V (X¯n ) = σn
m.q.
qui tend vers 0 quand n → ∞, ce qui établit que X¯n −−−−→ μ.
n→∞

La loi des grands nombres n’a pas d’intérêt pratique pour le calcul statistique, contrairement au théorème central limite ci-après qui vient préciser
la façon dont X¯n converge vers μ. Ce théorème est à la base de nombreuses
propriétés essentielles des échantillons en statistique.

5.8.3

Le théorème central limite

Théorème 5.4 Soit {Xn } une suite de v.a. indépendantes de même loi ad¯n −μ
√
mettant une moyenne μ et une variance σ 2 . Alors la suite X
converge en
σ/ n
loi vers la v.a. de loi N (0 ; 1), ce que nous écrivons conventionnellement :
X¯n − μ L
√ −−−−→ N (0 ; 1).
σ/ n n→∞

Chapitre 5. Lois fondamentales de l’échantillonnage

83

Démonstration : nous supposerons que la loi mère admet une fonction génératrice des moments ΨX . Une démonstration plus générale considèrerait de même
la fonction caractéristique brièvement mentionnée dans la note 2.4 à la ﬁn de
la section 2.5, laquelle existe toujours. Dans cette section nous avons également
mentionné le développement de ΨX (t) en série de Taylor-Mac-Laurin :
ΨX (t) =

∞


μk

k=0

tk
,
k!

soit, en nous limitant à l’ordre 2,
ΨX (t) = 1 + μ1 t + μ2
où

o(t2 )
t2

t2
t2
+ o(t2 ) = 1 + μt + (σ 2 + μ2 ) + o(t2 )
2
2

→ 0 quand t → 0. Soit maintenant :
X¯n − μ
√ =
σ/ n

n

Xi − nμ 
Xi − μ
√
Ti où Ti = √ .
=
σ n
σ n
i=1
n

i=1

Pour tout i, Xi − μ a pour moyenne 0 et pour variance σ 2 , d’où :
t2
+ o(t2 )
2

ΨXi −μ (t) = 1 + σ 2

t
(Xi −μ) σ√
n

et

ΨTi (t) = E e


t
√
= ΨXi −μ
σ n
 2
t
t2
+o
.
=1+
2n
n


D’après la proposition (3.12),
Ψ ni=1 Ti (t) = [1 +

t2
t2
t2 n
t2
+ o( )]n = (1 +
) + o( )
2n
n
2n
n

et, sachant que limn→∞ (1 + na )n = ea , on a :
t2

lim Ψ ni=1 Ti (t) = e 2 ,

n→∞

qui est bien la fonction génératrice de la loi N (0 ; 1).



X̄−μ
√ est centrée-réduite (moyenne
S’il est clair que, pour tout n, la v.a. σ/
n
nulle, variance égale à 1) le théorème central limite indique en plus que sa loi

84

Statistique − La théorie et ses applications

tend à être gaussienne quand n s’accroı̂t et ceci, quelle que soit la loi mère
des Xi .

Application fondamentale
Soit X̄n la moyenne empirique d’un n-échantillon aléatoire de loi mère quelconque, de moyenne μ et de variance σ 2 . Alors si n est assez grand X̄n suit
2
approximativement une loi N (μ, σn ) ce que l’on note :
X̄n

;

approx

N (μ,

σ2
).
n

Dans tous les cas (ou presque, voir ci-après pour la loi mère de Bernoulli) n ≥ 30
suﬃt pour obtenir des approximations de probabilités à 10−2 près. Pour une
loi continue à un seul mode sans queues de distribution trop allongées n = 5
pourra même suﬃre. Si la loi mère est gaussienne nous avons vu en section 5.2
que X̄n est exactement gaussienne pour tout n.
Note 5.5 Comme pour la loi des grands nombres il existe diﬀérentes versions
du théorème central limite partant de conditions plus ou moins restrictives.
En particulier il n’est pas nécessaire que les v.a. soient de même loi ni même
qu’elles soient indépendantes dans la mesure où leur degré de dépendance reste
faible. Ceci explique que certains phénomènes naturels répondent bien à un
modèle gaussien du fait que la variable étudiée résulte de l’addition d’eﬀets
aléatoires multiples.
Ainsi on peut établir un comportement asymptotique gaussien pour d’autres
types de statistiques dans la mesure où elles sont des moyennes de v.a. qui, sans
être nécessairement indépendantes pour n ﬁni, tendent à être i.i.d. quand n →
∞. En particulier ceci est vrai pour la variance de l’échantillon Sn2 pour laquelle
les éléments Xi − X̄n (et donc leurs carrés) tendent à devenir indépendants du
fait que X̄n converge vers μ. Il est toutefois nécessaire que la variance de Sn2
existe et il suﬃt pour cela (voir en ﬁn de section 2.3) que μ4 existe pour la loi
mère (pour le calcul de la variance de la distribution d’échantillonnage de Sn2
voir les exercices).
Cas particulier : processus de Bernoulli
Soit Sn le nombre total de succès au cours de n répétitions. Comme E(Sn ) = np
et V (Sn ) = np(1 − p) on a pour la fréquence relative Sn /n une moyenne p et
une variance p(1−p)
. D’où, pour n suﬃsamment grand,
n
Sn
n

;

approx

ou encore Sn

N (p,

;

approx

p(1 − p)
)
n

N (np, np(1 − p)) .

Chapitre 5. Lois fondamentales de l’échantillonnage

85

Cette deuxième forme constitue l’approximation de la loi binomiale B(n, p)
par la loi de Gauss N (np, np(1−p)). C’est l’approche historique qui a permis
à de Moivre pour p = 12 , puis Laplace pour p quelconque, de mettre initialement
en évidence la loi de Gauss (voir exercice 4.3).
En pratique on admet généralement que l’approximation est satisfaisante
dès lors que np ≥ 5 et n(1 − p) ≥ 5. Ces deux conditions garantissent que la
moyenne de la loi binomiale ne soit ni trop proche de 0, ni trop proche de n,
car dans le cas contraire la loi serait assez nettement asymétrique. Du fait que
l’on passe d’une loi discrète à une loi continue on introduit une correction de
continuité de la façon suivante.
Soit X ; B(n, p) alors :
P (X = k)  P (k −

1
1
< U < k + ) où U ; N (np, np(1 − p)).
2
2

Exemple 5.1 Soit X ; B(20 ;0,3). Nous pouvons recourir à une approximation gaussienne car np = 6 > 5 et n(1 − p) = 14 > 5. Considérons P (X = 8) et
P (X ≤ 8).
P (X = 8)  P (7,5 < U < 8,5) où U ; N (6 ; 4,2)
8,5 − 6
7,5 − 6
<Z< √
) où Z ; N (0 ; 1)
 P( √
4,2
4,2
 P (0,73 < Z < 1,22) = 0,8888 − 0,7673 = 0,1215.
La valeur exacte (lue dans une table binomiale) est 0,1144 .
P (X ≤ 8)  P (U < 8,5)
 P (Z < 1,22) = 0,8888.
La valeur exacte est 0,8866.



Remarque : toutes les lois qui peuvent être déﬁnies comme résultant d’une
somme de variables aléatoires i.i.d. tendent à être gaussiennes quand le nombre
de termes augmente. C’est évidemment le cas de la binomiale B(n , p) quand
n → ∞, comme nous venons de le voir (et par voie de conséquence pour la loi
hypergéométrique quand N → ∞ et M/N → p, voir section 4.1.5), mais aussi
de la loi binomiale négative BN (r, p) quand r → ∞, de la loi Γ(r , λ) quand
r → ∞, de la loi χ2 (ν) quand ν → ∞.
De façon indirecte c’est également vrai pour la loi de Poisson qui peut
être approchée par une somme de v.a. de Bernoulli en découpant l’unité de
temps en petits intervalles (voir section 4.1.7). En pratique on peut approcher
la loi P(λ) par la loi N (λ , λ) dès que λ ≥ 20, les calculs de probabilités étant
corrects à 10−2 près en utilisant la correction de continuité.

86

Statistique − La théorie et ses applications

Exemple 5.2 Soit X ; P(20) . Calculons P (X ≤ 14).
P (X ≤ 14)  P (U < 14,5) où U ; N (20 ; 20)
14,5 − 20
) où Z ; N (0; 1)
 P (Z < √
20
 P (Z < −1,23) = 0,1093.
La valeur exacte (lue dans une table de Poisson) est 0,1049.



Nous proposons dans la section des exercices quelques «exercices appliqués»
permettant de voir des situations pratiques illustrant l’intérêt des résultats
précédents.

5.9

Exercices

Exercice 5.1 Soit X1 , X2 , ..., Xn un échantillon aléatoire de loi mère exponentielle E(λ), montrer que X̄ est de loi Γ(n, nλ).
Exercice 5.2 Soit X1 , X2 , ..., Xn un échantillon aléatoire de loi mère Γ(r , λ).
Quelle est la loi de X̄ ?
Exercice 5.3 * Soit X1 , X2 , ..., Xn un échantillon aléatoire de loi mère de Cauchy
dont la densité est :
1
f (x) =
, x ∈ R.
π(1 + x2 )
Montrer, via la fonction caractéristique des moments, que X̄n suit la même loi (elle
ne converge donc pas vers une constante quand n → ∞).
4
Exercice 5.4 Montrer que la variance de S 2 est égale à n1 (μ4 − n−3
n−1 σ ). Que
vaut-elle dans le cas particulier de la loi de Gauss (voir formule pour μ4 en
section 4.2.4) ?

Exercice 5.5 * (Sondage aléatoire simple sans remise) Soit la suite de v.a.
X1 , X2 , . . . , Xn issue du tirage de n individus sans remise dans une population
, . . . , aN les valeurs dans la population
de taille N. Soit a1 , a2
N de la variable
1
2
2
étudiée. Soient μ = N1 N
a
leur
moyenne
et
σ
=
j
j=1
j=1 (aj − μ) leur
N
variance. Pour des raisons évidentes de symétrie, P (Xi = aj ) reste identique
quels que soient i et j.
En déduire la loi marginale de Xi . Par le même type d’argument déterminer
la loi d’un couple (Xi , Xk ) quels que soient i et k (i = k).
Déterminer alors E(Xi ), V (Xi ) et E(X) où X est la moyenne de l’échantillon
sans remise.
2
Montrer que cov(Xi , Xk ) = − Nσ−1 (aide : partir de la formule de décentrage



et utiliser la relation générale ( j aj )2 = j a2j + j =l aj al ).

Chapitre 5. Lois fondamentales de l’échantillonnage

87

n
Calculer V ( i=1 Xi ) (aide : partir de la formule générale donnée à la ﬁn
de la section 3.8). En déduire que la moyenne d’un échantillon issu d’un tirage
sans remise a pour variance :
σ2 N − n
.
n N −1
(on remarquera que l’on trouve le même facteur correctif de sans remise que
pour la loi hypergéométrique en section 4.1.5)


Exercice 5.6 Montrer que la covariance de X̄ et de S 2 est égale à μ3 /n
(aide : on peut supposer que la loi mère est de moyenne nulle sans nuire à
la généralité). Ce résultat montre que ces deux statistiques sont asymptotiquement «non corrélées».
Exercice 5.7 Déterminer directement la densité d’une loi χ2 (1) par le changement de variable de Z ; N (0 ; 1) à Z 2 .
Exercice 5.8 Établir, par la fonction génératrice, la moyenne et la variance
de la loi χ2 (ν).
Exercice 5.9 * Soit X une v.a. continue de densité fX , de moyenne μ et de
variance σ 2 . Soit g une fonction positive.
1. Soit A = {x|g(x) > k > 0}. Montrer que
 ∞

g(x)fX (x)dx ≥ k
fX (x)dx.
−∞

A

et en déduire que E(g(X)) ≥ kP (g(X) > k).
2. En prenant g(x) = (x − μ)2 montrer l’inégalité de Tchebichev :
P (|X − μ| > ) ≤

σ2
.
2

3. Soit une suite de v.a. {Yn }. En prenant X = |Yn − Y | et g(x) = x2
m.q.
p
montrer que Yn −−−→ Y implique Yn → Y .
Exercice 5.10 Démontrer la loi faible des grands nombres quand la variance
existe.
Aide : utiliser l’inégalité de Tchebichev ci-dessus.
Exercice 5.11 Pour un échantillon de taille n quelle est la probabilité que le
maximum dépasse la médiane de la loi mère ? Quelle est la probabilité que le
maximum dépasse le troisième quartile (i.e. le quantile d’ordre 0,75) de la loi
mère ?

88

Statistique − La théorie et ses applications

Exercice 5.12 Soit un échantillon de taille n issu d’une loi U[0 ; 1]. Déterminer
la fonction de répartition et la densité de la loi du minimum de l’échantillon.
En déduire l’espérance mathématique de ce minimum.
Exercice 5.13 Pour la marche aléatoire présentée dans la section d’exercices
3.10 donner une valeur approchée, pour n suﬃsamment grand, de la probabilité
d’être éloigné de plus de x mètres de la position initiale après n étapes.

Exercices appliqués
Exercice 5.14 Le niveau de bruit d’un type de machine à laver à un certain
régime est une v.a. de moyenne 44 dB et d’écart-type 5 dB. En admettant la
validité de l’approximation gaussienne, donner la probabilité de trouver une
moyenne supérieure à 48 dB pour un échantillon aléatoire de 10 machines.
Exercice 5.15 Un constructeur automobile indique une consommation de 6,3
l/100km pour un type de modèle dans des conditions expérimentales précises.
Pour 30 automobiles (supposées prises au hasard) testées dans ces mêmes conditions on relève une consommation moyenne de 6,42 l/100km et un écart-type
de 0,22 l/100km.
Calculer la valeur prise dans cet échantillon par la statistique de Student
du théorème 5.2.
A quel quantile correspond-elle sur la loi de cette statistique ? (on supposera
que la loi de Student s’applique avec une bonne approximation vu la taille
d’échantillon)
L’indication du constructeur vous paraı̂t-elle plausible ?
Exercice 5.16 Un téléphérique a une capacité de 100 personnes. Dans la population française le poids des personnes est distribué avec une moyenne de
66,3 kg et un écart-type de 15,6 kg. En supposant que les personnes entrant
dans la benne soient prises au hasard dans cette population quelle est, approximativement, la probabilité que le poids total des personnes transportées excède
7 000 kg ?
Exercice 5.17 Un sondage est eﬀectué auprès de 1 000 personnes dans la population française sur la popularité d’une certaine personnalité.
Quelle est la probabilité que le sondage indique une cote de popularité
inférieure ou égale à 42 % si la proportion de personnes favorables à cette
personnalité est de 0,44 au sein de la population ? (aide : on aura avantage
à passer par la loi de Sn , nombre total de succès, pour pouvoir utiliser la
correction de continuité de l’approximation gaussienne, comme dans l’exemple
5.1)

Chapitre 5. Lois fondamentales de l’échantillonnage

89

Exercice 5.18 Une machine en fonctionnement normal produit 9 % de pièces
défectueuses. Un contrôle de qualité consiste à prélever 120 pièces au hasard.
Quelle est la loi du nombre de pièces défectueuses ? Expérience faite, 22 pièces
s’avèrent être défectueuses. A quel quantile correspond cette valeur sur la loi
précédente ? (aide : on recourra à l’approximation gaussienne avec correction
de continuité comme dans l’exemple 5.1)
Qu’en conclure quant au fonctionnement de la machine ?
Exercice 5.19 D’une façon générale on déﬁnit la précision d’une méthode de
mesure par le double de l’écart-type de son erreur aléatoire. L’hypothèse d’une
erreur aléatoire gaussienne est la règle.
Une méthode de mesure d’alcoolémie est réputée avoir une précision de 0,1
mg/l. Sur un même échantillon sanguin on eﬀectue 5 mesures que l’on peut
supposer indépendantes. Quelle est la probabilité de trouver un écart-type des
5 mesures supérieur à 0,077 ? (aide : on passera par la variance)
Exercice 5.20 On cherche à prévoir le nombre de nuitées dans les hôtels d’une
station balnéaire en juillet. D’expérience on a pu constater que le nombre de
nuits passées par un ménage peut être modélisé par une loi de Poisson de
moyenne 4. On fait l’hypothèse d’une fréquentation de 10 000 ménages, quelles
sont la moyenne et la variance de la v.a. «nombre total de nuitées» ?
En utilisant une approximation gaussienne donner un intervalle de probabilité 0,95 pour cette v.a.
Exercice 5.21 Lors de la conversion du franc à l’euro les opérations sont arrondies au centime d’euro le plus proche. On suppose que les décimales de
centime d’euro apparaissent de façon aléatoire uniformément réparties sur l’intervalle [0 , 1]. Quelle est approximativement la loi de l’erreur d’arrondi sur
1 000 opérations ? Donner un intervalle de probabilité 0,95 pour cette erreur.
Exercice 5.22 Un appareil électronique contient 3 accumulateurs. Pour que
l’appareil fonctionne il faut que les 3 accumulateurs fonctionnent. On admet
que la durée de vie d’un accumulateur suit une loi exponentielle de moyenne 2
ans et que les durées des trois éléments sont indépendantes. Quelle est la loi de
la durée de fonctionnement de l’appareil ? Quelle est sa moyenne ? Quelle est
la probabilité qu’elle soit supérieure à un an ?
Exercice 5.23 Un industriel doit livrer 100 pièces. Sachant que le processus
de fabrication produit une pièce défectueuse avec probabilité 0,10 il souhaite
budgéter le nombre de pièces à produire pour être quasiment sûr de fournir 100
bonnes unités.
Un raisonnement simpliste consiste à déclarer que 111 pièces suﬃsent. Quelle
est la probabilité de dépasser 111 pièces pour en obtenir 100 bonnes (on pourra
utiliser une approximation gaussienne) ?

90

Statistique − La théorie et ses applications

Combien doit-on fabriquer de pièces pour être sûr d’en avoir 100 bonnes ?
Combien doit-on fabriquer de pièces pour en avoir 100 bonnes avec une probabilité 0,99 ?

Chapitre 6

Théorie de l’estimation
paramétrique ponctuelle
6.1

Cadre général de l’estimation

Soit X une v.a. associée à un certain phénomène aléatoire observable de
façon répétée comme décrit en section 5.1. Notre objectif est «d’estimer» certaines caractéristiques d’intérêt de sa loi (la moyenne, la variance, la fonction
de répartition, la fonction de densité, etc.) sur la base d’une série d’observations
x1 , x2 , ..., xn . Un cas particulier important est celui du sondage dans une population (voir section 3.7) dont l’objectif est d’estimer diverses caractéristiques
descriptives de celle-ci.
Dans ce chapitre nous considérerons toujours, même si des développements
analogues sont possibles dans d’autres circonstances, que x1 , x2 , ..., xn sont des
réalisations d’un n-échantillon aléatoire X1 , X2 , ..., Xn . Cette hypothèse sur nos
observations qui peut être plus ou moins réaliste est nécessaire pour étudier de
façon simple, en termes probabilistes, la qualité des estimations que l’on cherche
à produire. Ce chapitre ne traite également que du problème de l’estimation
ponctuelle, c’est-à-dire celle qui consiste à attribuer, au mieux de notre savoir, une valeur unique à la caractéristique d’intérêt inconnue. Au chapitre 7
nous aborderons l’estimation par intervalle consistant à donner un encadrement
plausible pour la caractéristique.
La théorie de l’estimation étudie les propriétés des estimations et des méthodes générales d’estimation comme celle dite du «maximum de vraisemblance». L’objectif est de comparer les lois d’échantillonnage des «estimateurs»
pour établir des préférences lorsque plusieurs choix se présentent. La notion
d’estimateur est la notion centrale de ce chapitre alors même qu’elle ne se
déﬁnit pas formellement en termes mathématiques.

92

Statistique − La théorie et ses applications

Déﬁnition informelle d’un estimateur et d’une estimation
Dans le cadre déﬁni ci-dessus, soit à estimer une caractéristique c de la
variable aléatoire X sur la base de la réalisation (x1 , x2 , ..., xn ) du n-échantillon
(X1 , X2 , ..., Xn ). On appellera estimateur toute statistique (donc toute fonction
de X1 , X2 , ..., Xn , voir déﬁnition 5.2) dont la réalisation après expérience est
envisagée comme estimation de c. Un estimateur se déﬁnit donc dans l’intention
de fournir une estimation.
Insistons sur le fait qu’un estimateur est une variable aléatoire, alors qu’une
estimation est une valeur numérique prise par l’estimateur suite à la réalisation du n-échantillon. Si un estimateur est déterminé par une fonction
h(X1 , X2 , ..., Xn ), l’estimation correspondante est évidemment h(x1 , x2 , ..., xn ).
Soit, par exemple, à estimer la moyenne E(X) de la loi de X, un estimateur qui
semble a priori naturel est la moyenne empirique X̄ qui produit une estimation
x̄, moyenne descriptive de la série des valeurs observées.

6.2

Cadre de l’estimation paramétrique

En estimation paramétrique la loi de X est réputée appartenir à une famille
de lois, telles que celles présentées au chapitre 4, qui peut être décrite par
une forme fonctionnelle connue soit de sa fonction de répartition, soit de sa
fonction de densité dans le cas continu, soit de sa fonction de probabilité dans
le cas discret, forme dépendant d’un ou plusieurs paramètres inconnus réels.
De façon générique nous noterons θ ce paramètre ou vecteur de paramètres et
F (x; θ), f (x; θ) ou p(x; θ) les trois formes fonctionnelles précitées. Toutefois,
par simpliﬁcation et sauf mention expresse contraire, nous noterons f (x; θ)
aussi bien la densité du cas continu que la fonction de probabilité du
cas discret.
L’ensemble des valeurs possibles pour θ, appelé espace paramétrique, sera
noté Θ, lequel est inclus dans Rk où k est la dimension du paramètre θ. Le plus
souvent la famille paramétrique à laquelle la loi de X est réputée appartenir sera
décrite par la famille de densités de probabilité (respectivement de fonctions
de probabilité) {f (x; θ); θ ∈ Θ}. Ces fonctions sont implicitement déﬁnies pour
tout x ∈ R. Rappelons ici (voir section 1.4) qu’on appelle support de f (x; θ) (ou
support de la loi) l’ensemble des valeurs de x telles que f (x; θ) > 0. En termes
courants, on parlera de l’ensemble des réalisations (ou valeurs) possibles.
Lorsque nous considérerons une famille de lois usuelles nous reviendrons
aux notations du chapitre 4. Ainsi la famille des lois de Gauss est décrite par
√
2
la famille des densités de la forme (1/( 2πσ)) exp{− (x−μ)
2σ 2 }, pour tout x ∈ R,
où intervient un paramètre (μ, σ 2 ) de dimension 2, l’espace paramétrique étant
la partie de R2 : {(μ, σ 2 ) | μ ∈ R, σ 2 ∈ R, σ 2 > 0}.
Dans ce cadre paramétrique le problème est celui de l’estimation du paramètre θ grâce à laquelle on obtiendra une estimation complète de la loi

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

93

de X et, par voie de conséquence, de toute caractéristique de cette loi. Distinguons bien ici la notion de paramètre d’une loi de celle de caractéristique
(moyenne, variance, médiane, ...) de la loi : le paramètre identiﬁe chaque loi
(chaque membre) dans la famille considérée mais n’est pas nécessairement une
caractéristique usuelle de cette loi. Par contre toute caractéristique usuelle
dépend du membre de la famille et donc du paramètre θ. Aussi le moment
d’ordre k, par exemple, sera-t-il noté μk (θ), la moyenne sera notée μ(θ) et la
variance σ 2 (θ). Si notre objectif principal est d’estimer le paramètre inconnu θ,
il se pourra aussi que nous souhaitions directement estimer une fonction de
θ représentant une certaine caractéristique particulièrement intéressante, sans
nécessairement passer par l’estimation de θ. En particulier, fréquemment on
voudra estimer moyenne et variance de la loi, soit μ(θ) et σ 2 (θ).
Notation pour les estimateurs et estimations
Pour un paramètre désigné par une certaine lettre on note souvent un estimateur par la même lettre surmontée d’un accent circonﬂexe. Pour distinguer la
méthode d’estimation utilisée on pourra ajouter en indice supérieur une lettre
y faisant clairement référence. Ainsi, pour le paramètre générique θ un estima l’estimateur obtenu par la méthode des moments
teur non précisé sera noté θ,
(exposée en section 6.4) sera noté θM et l’estimateur obtenu par la méthode du
maximum de vraisemblance (exposée en section 6.7) sera noté θM V . Selon nos
conventions initiales nous devrions noter ces variables aléatoires avec la lettre
majuscule de θ. Mais dans la mesure où le contexte indique clairement s’il s’agit
d’une variable aléatoire ou de sa réalisation, nous ne ferons pas la distinction entre estimation et estimateur lorsqu’ils sont notés en lettres
grecques. La lettre λ̂M désignera, par exemple, l’estimateur ou l’estimation
des moments pour le paramètre λ de la loi E(λ).
Notons que dans le contexte de l’estimation paramétrique le paramètre θ
est totalement inconnu. Ainsi la v.a. X : «intervalle de temps séparant r occurrences» dans un processus de Poisson suit une loi Γ(r, λ) dont seul le paramètre
λ est inconnu. Nous avons donc aﬀaire à une sous-famille de la famille des lois
Gamma.
Remarquons également qu’il n’y a pas qu’une seule façon de paramétrer
une famille de lois. En particulier toute fonction strictement monotone h(θ) du
paramètre θ permet une reparamétrisation de la famille des densités (respectivement des fonctions de probabilité). Ainsi, nous avons adopté, pour décrire la
famille des lois exponentielles E(λ), la forme fonctionnelle :
f (x; λ) = λe−λx , x ≥ 0, λ > 0
dans laquelle λ est l’inverse de la moyenne de la loi mais correspond à l’intensité du processus de Poisson (nombre moyen d’occurrences par unité de temps).

94

Statistique − La théorie et ses applications

Certains auteurs utilisent la forme :
1 x
f (x; θ) = e− θ , x ≥ 0, θ > 0
θ
où θ = 1/λ est la fonction de reparamétrisation, auquel cas θ est la moyenne
de la loi. Nous garderons à l’esprit ce problème de changement de paramètre
qui permettra de dégager certaines propriétés intéressantes des estimateurs.
Pour clore cette introduction mettons en évidence le fait que, dans le cadre
des échantillons aléatoires, la loi conjointe du n-échantillon (X1 , X2 , ..., Xn )
peut être déﬁnie par la fonction de densité (respectivement la fonction de probabilité) conjointe :
f (x1 , x2 , .., ., xn ; θ) = f (x1 ; θ)f (x2 ; θ)...f (xn ; θ) =

n
$

f (xi ; θ)

i=1

où θ est le paramètre inconnu dans Θ (par commodité, nous désignons ici,
et ferons de même éventuellement plus loin, la densité conjointe par la même
lettre f que la densité de la loi mère). Dans le cas discret cette expression est
la probabilité Pθ (X1 = x1 , X2 = x2 , ..., Xn = xn ).
Pour établir certains résultats nous serons amenés à poser des conditions sur
cette densité (ou fonction de probabilité) conjointe. Ces conditions ne seront
pertinentes qu’aux points (x1 , x2 , .., ., xn ) de Rn correspondant à des valeurs
possibles, en d’autres termes uniquement sur le support de la loi conjointe.
Ce support est évidemment l’ensemble produit n fois du support de f (x; θ).
Pour la famille paramétrique dans son ensemble, il faudra prendre en compte
l’union des supports de tous les membres lorsque θ décrit Θ. Pour la famille
des lois U[0, θ], avec θ > 0, par exemple, cette union est R+ . Donc, par la
suite, les conditions imposées aux densités ou fonctions de probabilités seront
implicitement restreintes à leurs supports.

6.3

La classe exponentielle de lois

Cette classe regroupe des familles paramétriques de lois qui, de par leur
forme particulière, partagent beaucoup de propriétés dans la théorie de l’estimation ou la théorie des tests, du fait que leurs densités peuvent s’écrire sous
une même expression canonique (on parle aussi de la «famille exponentielle»
mais cela prête à confusion avec la famille exponentielle usuelle E(λ)).
Déﬁnition 6.1 Soit une famille paramétrique de lois admettant des fonctions
de densité (cas continu) ou des fonctions de probabilité (cas discret) {f (x; θ); θ ∈
Θ ⊆ Rk }. On dit qu’elle appartient à la classe exponentielle de lois si f (x; θ)
peut s’écrire :
f (x; θ) = a(θ)b(x) exp{c1 (θ)d1 (x) + c2 (θ)d2 (x) + ... + ck (θ)dk (x)}
pour tout x ∈ R.

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

95

Notons qu’il doit y avoir autant de termes de produits dans la partie exponentielle que de dimensions pour θ. En particulier, si θ est de dimension 1, on
a:
f (x; θ) = a(θ)b(x) exp{c(θ)d(x)}.
De plus cette forme canonique doit être eﬀective pour tout x ∈ R. En particulier si le support de f (x; θ), donc l’ensemble des valeurs de x pour lesquelles f (x; θ) > 0 , dépend lui-même du paramètre θ, un terme d’indicatrice
I[α(θ),β(θ)] (x) doit être introduit, ce qui ne peut en aucun cas permettre la
forme déﬁnie ci-dessus. Ces types de lois auront, de ce fait, des propriétés très
spéciﬁques. On donnera pour exemple la famille des lois U[0, θ], où θ > 0 est
inconnu, dont les densités sont de la forme :
f (x; θ) =

1
I[0,θ] (x) , ∀x ∈ R.
θ

Notons encore que les notions de dimension k et de forme canonique ne
concernent que les paramètres inconnus. Ainsi la sous-famille des lois B(n, p)
où n est connu, comme c’est toujours le cas dans les applications statistiques,
appartient à la famille exponentielle (voir exemple 6.1), ce qui ne serait pas le
cas si n était inconnu.
Nous verrons plus loin que les fonctions di (x) jouent un rôle central dans la
recherche des meilleurs estimateurs. Aussi les mettons-nous en évidence dans
les trois exemples qui suivent, puis dans le tableau 6.1 qui servira de référence
par la suite.
Exemple 6.1 Loi B(n, p) avec n connu.
 
n x
p (1 − p)n−x pour x = 0, 1, 2, ..., n
f (x; n, p) =
x
 
p
n n
exp{x. ln
}
= (1 − p)
x
1−p
d’où d(x) = x. Le cas de la loi de Bernoulli B(p) est identique avec n = 1.



Exemple 6.2 Loi P (λ).
e−λ λx
pour x ∈ N
x!
1
= e−λ exp{x. ln λ}
x!

f (x; λ) =

d’où, également, d(x) = x.



96

Statistique − La théorie et ses applications

Exemple 6.3 Loi N (μ , σ 2 ).
1 (x − μ)2
}, x ∈ R
2
σ2
2πσ2
1
μ2
1
μ
=√
exp{− 2 } exp{− 2 x2 + 2 x}
2
2σ
2σ
σ
2πσ

f (x; μ , σ 2 ) = √

1

exp{−

d’où d1 (x) = x2 et d2 (x) = x.



On peut établir aisément les résultats du tableau suivant qui contient la
plupart des lois usuelles.
Tableau 6.1 : Principales lois usuelles appartenant à la classe exponentielle
loi
paramètre d1 (x)
d2 (x)
I 1
B(p)
p
x
[p(1 − p)]−1
B(n, p)
p (n connu)
x
n[p(1 − p)]−1
BN (r, p)
p (r connu)
x
r[p2 (1 − p)]−1
P(λ)
λ
x
1/λ
E(λ)
λ
x
1/λ2
Γ(r, λ)
λ (r connu)
x
r/λ2
2
2
2
N (μ, σ )
(μ, σ )
x
x
P areto(a, θ) θ (a connu) ln(x)
1/θ2
Beta(α, β)
(α, β)
ln(x) ln(1 − x)
1
Information de Fisher, voir section 6.6.3
En revanche les lois hypergéométriques (M inconnu), Weibull et Gumbel
n’appartiennent pas à la classe exponentielle.

6.4

Une approche intuitive de l’estimation : la
méthode des moments

Bien que cette méthode ne soit pas toujours satisfaisante nous l’introduisons
dès maintenant en raison de son côté intuitif. Elle nous servira ainsi, dans la
section suivante, à illustrer les propriétés générales des estimateurs.
Nous commençons par le cas d’un paramètre à une dimension. Pour une
réalisation x1 , x2 , ..., xn de l’échantillon la méthode consiste alors à choisir pour
estimation de θ la valeur telle que la moyenne théorique μ(θ) (ou premier
moment de la loi) coı̈ncide avec la moyenne empirique x. Pour la loi E(λ), par
exemple, l’estimation de λ sera λ̂M telle que 1/λ̂M = x̄, soit λ̂M = 1/x̄. Pour
la loi BN (r, p) avec r connu, l’estimation de p sera p̂M telle que
r
r(1 − p̂M )
.
= x̄, d’où p̂M =
p̂M
r + x̄

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

97

Pour la loi de Poisson la solution est x̄ puisque le paramètre λ est lui-même
la moyenne de la loi. De même, pour la loi de Bernoulli, p est estimé par la
moyenne qui est la fréquence relative observée.
La méthode n’a de sens que s’il y a existence et unicité de la solution
dans l’espace paramétrique Θ, ce que nous supposerons toujours vrai. Ainsi,
de façon générale, nous sommes amenés à résoudre l’équation μ(θ) = x̄ et, en
raison de l’unicité, nous pourrons noter θ̂ M = μ−1 (x̄) l’estimation de θ pour
une réalisation x̄ donnée de X̄. Appliquée maintenant à X̄ (donc dans l’univers
- selon l’acceptation de ce mot donnée en ﬁn de section 5.1 - des échantillons
aléatoires) la fonction μ−1 déﬁnit alors la statistique θ̂M = μ−1 (X̄) appelée
estimateur des moments de θ (rappelons que nous n’utilisons pas de lettre
grecque majuscule pour distinguer estimateur et estimation).
Pour un paramètre de dimension 2 l’estimation résulte de la résolution de
deux équations, l’une reposant sur le premier moment, l’autre sur le moment
d’ordre 2. Prenons le cas de la loi de Gauss avec (μ, σ 2 ) comme paramètre, dont
le premier moment est μ lui-même et le moment d’ordre 2 est E(X 2 ) = μ2 +σ2 .
On résout donc (en passant directement aux v.a.)
⎧
μ
= X̄
⎪
⎨
n

1
⎪
Xi2
⎩ μ2 + σ 2 = n
i=1

n
d’où μ̂M = X̄ et σ̂ 2,M = n1 i=1 Xi2 − X̄ 2 = S'2 . La moyenne et la variance
théoriques sont donc estimées naturellement par la moyenne et la variance
empiriques.
Prenons maintenant le cas moins intuitif de la loi de Gumbel de paramètre
(α, β), dont la moyenne est α + γβ, où γ est la constante d’Euler, et la variance
est π 2 β 2 /6 (voir section 4.2.8). On résout :
⎧
α + γβ
= X̄
⎪
⎨
2 2
n

π β
⎪
Xi2
= n1
⎩ (α + γβ)2 +
6
i=1
ou, de façon équivalente,

⎧
⎨ α + γβ
2 2
⎩ π β
6

ce qui donne la solution β̂ M =

√

6'
π S

= X̄
= S'2

et α̂M = X̄ −

γ

√
π

6

'
S.

98

Statistique − La théorie et ses applications

D’une façon générale l’estimation de θ de dimension 2 par la méthode des
moments est la solution (supposée exister et être unique pour toute réalisation
du n-échantillon aléatoire) du système :
μ(θ) = x̄ 
.
n
μ2 (θ) = n1 i=1 x2i
n
Cette solution appliquée à X̄ et n1 i=1 Xi2 donne l’estimateur de θ correspondant.
Du fait de la correspondance des formules de décentrage pour les moments
empiriques et pour les moments théoriques il est équivalent de résoudre :
μ(θ) = x̄
σ2 (θ) = s'2
où la deuxième équation porte donc sur les moments centrés d’ordre 2. Nous
donnons maintenant une déﬁnition formelle de l’estimateur des moments dans
le cas général où le paramètre est de dimension k quelconque.
Déﬁnition 6.2 Soit un échantillon aléatoire (X1 , X2 , ..., Xn ) dont la loi mère
appartient à une famille paramétrique de paramètre inconnu θ ∈ Θ, où Θ ⊆ Rk ,
et telle que pour tout θ ∈ Θ il existe un moment μk (θ) à l’ordre k. Si, pour
toute réalisation (x1 , x2 , ..., xn ) de (X1 , X2 , ..., Xn ) le système à k équations
⎧
μ1 (θ) = m1
⎪
⎪
⎨
μ2 (θ) = m2
...
⎪
⎪
⎩
μk (θ) = mk
n
(où mr dénote la réalisation du moment empirique d’ordre r : mr = n1 i=1 xri )
admet une solution unique, cette solution est appelée estimation des moments
de θ. La fonction(de Rn dans Rk ) qui à toute réalisation (x1 , x2 , ..., xn ) fait
correspondre cette solution déﬁnit, en s’appliquant à (X1 , X2 , ..., Xn ), une statistique à valeurs dans Rk appelée estimateur des moments de θ.

6.5

Qualités des estimateurs

Un des objectifs essentiels de la théorie de l’estimation, nous l’avons dit,
est d’opérer des choix parmi les diﬀérents estimateurs auxquels on peut penser.
Pour cela il est nécessaire de se donner des critères de qualité pertinents. De
façon générique nous noterons Tn l’estimateur de θ à étudier. Étant donné
que la valeur de θ est inconnue, nous souhaitons que le comportement de Tn
soit satisfaisant quel que soit θ ∈ Θ, c’est-à-dire quelle que soit la loi mère
eﬀective dans la famille paramétrique donnée, et les critères de qualité seront
à étudier comme des fonctions de θ. Les critères déﬁnis ci-après, mis à part

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

99

l’exhaustivité (section 6.5.4), seront appliqués uniquement à un paramètre de
dimension 1 (Θ ⊆ R) et nous commenterons en section 6.6.4 les possibilités
d’extension à une dimension supérieure.

6.5.1

Biais d’un estimateur

Déﬁnition 6.3 Soit une v.a. X de loi de densité (ou fonction de probabilité)
f (x; θ) où θ ∈ Θ ⊆ R. Soit X1 , X2 ..., Xn un n-échantillon issu de cette loi et
Tn un estimateur de θ. On appelle biais de Tn pour θ la valeur :
bθ (Tn ) = Eθ (Tn ) − θ.
Si bθ (Tn ) = 0 quel que soit θ ∈ Θ, on dit que Tn est sans biais pour θ.
Cette déﬁnition s’étend naturellement à l’estimation d’une fonction h(θ). Le
biais caractérise donc l’écart entre la moyenne de Tn dans l’univers de tous les
échantillons possibles et la valeur cible θ. Elle correspond à la notion d’erreur
systématique pour un instrument de mesure. Notons que la moyenne de Tn ,
Eθ (Tn ), est indicée par θ pour rappeler qu’elle est liée à la valeur inconnue de
θ. Ceci sera vrai également plus loin pour la variance et l’erreur quadratique
moyenne de Tn . Pour alléger les écritures, nous omettrons pourtant souvent
cette indexation, notamment dans les illustrations.
Exemple 6.4 Soit la famille des lois continues U [0, θ]. Montrons que n+1
n X(n)
est sans biais pour θ. Nous avons vu en section 5.6 que, pour une loi de fonction
de répartition F (x), la fonction de répartition du maximum de l’échantillon
X(n) est [F (x)]n . Dans la situation particulière considérée F (x; θ) = xθ quand
x ∈ [0, θ] et la densité de X(n) est donc :
x
1
.n
θ
θ
d’où :


Eθ (X(n) ) =

n−1

θ

x.n
0

=n

xn−1
si x ∈ [0, θ],
θn

xn−1
n θn+1
n
θ.
dx
=
=
n
n
θ
n+1 θ
n+1

Donc Eθ ( n+1
n X(n) ) = θ quel que soit θ. Il est intéressant de noter la présence du
qui, en quelque sorte, prend en compte l’écart entre le maximum
facteur n+1
n
observé et la borne supérieure des valeurs possibles.



Exemple 6.5 Considérons l’estimation des moments de la loi mère qui sont
des fonctions de θ. Remarquons que le moment empirique d’ordre r est sans
biais pour le moment théorique d’ordre r de la loi de X. En eﬀet, par déﬁnition,
Eθ (X r ) = μr (θ) et donc, quel que soit θ,
1 r
1
1
Xi ) =
Eθ (Xir ) = nμr (θ) = μr (θ).
n i=1
n i=1
n
n

Eθ (

n

Statistique − La théorie et ses applications

100

Ceci n’est pas vrai pour les moments centrés. Ainsi pour r = 2 nous avons la
2
variance empirique S'2 pour laquelle (voir proposition 5.2) Eθ (S'2 ) = n−1
n σ (θ).
Son biais est :
n−1 2
1
Eθ (S'2 ) =
σ (θ) − σ 2 (θ) = − σ 2 (θ).
n
n
Ceci signiﬁe, qu’en moyenne, la variance empirique sous-estime la variance de
la loi étudiée. C’est pourquoi on lui préfère la statistique
1 
(Xi − X̄)2
n − 1 i=1
n

S2 =

appelée conventionnellement «variance de l’échantillon» qui est sans biais pour
θ (voir section 5.2). Rappelons que cette sous-estimation s’explique par le fait
que les écarts sont mesurés par rapport à la moyenne même des valeurs et non
par rapport à la vraie moyenne μ(θ).

Il est intéressant de remarquer que ces propriétés des moments sont
vraies pour toute loi mère (dans la mesure où les moments existent) indépendamment de tout cadre paramétrique. On dit que ce sont des propriétés non
paramétriques (en anglais : distribution free) que nous développerons au chapitre 8.

6.5.2

Variance et erreur quadratique moyenne d’un
estimateur

La variance Vθ (Tn ) de l’estimateur est un critère important dans la mesure
où elle caractérise la dispersion des valeurs de Tn dans l’univers des échantillons
possibles. Toutefois il s’agit de la dispersion autour de Eθ (Tn ) et non pas autour
de θ. Pour prendre en compte l’écart par rapport à θ on introduit le critère
d’erreur quadratique moyenne.
Déﬁnition 6.4 On appelle erreur quadratique moyenne de Tn par rapport
à θ, la valeur, notée eqmθ (Tn ), déﬁnie par :
eqmθ (Tn ) = Eθ [(Tn − θ)2 ],
et l’on a :
eqmθ (Tn ) = [bθ (Tn )]2 + Vθ (Tn ).
En eﬀet :
Eθ [(Tn − θ)2 ] = Eθ [{Tn − Eθ (Tn ) + Eθ (Tn ) − θ}2 ]
= Eθ [{Tn − Eθ (Tn )}2 ] + [Eθ (Tn ) − θ]2 + 2Eθ [Tn − Eθ (Tn )][Eθ (Tn ) − θ]
= Vθ (Tn ) + [bθ (Tn )]2

car Eθ [Tn − Eθ (Tn )] = 0.

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

101

Comme l’indique son nom ce critère mesure la distance au carré à laquelle
Tn se situe en moyenne par rapport à θ. On peut faire l’analogie avec les
impacts eﬀectués par un tireur sur une cible (même si cela correspond plutôt
à un paramètre de dimension 2). Le tireur cherche à atteindre le centre de la
cible mais ses impacts, au cours des répétitions («univers» de ses tirs), peuvent
être systématiquement décalés, c’est-à-dire que le centre de ceux-ci n’est pas
le centre de la cible. En revanche ses tirs peuvent être très groupés (variance
faible). Un autre tireur peut être bien centré (biais nul ou faible) mais avoir peu
de régularité et donc une forte dispersion de ses tirs (variance élevée). Le choix
du meilleur tireur dépend de l’importance relative du décalage systématique et
de la régularité.
Le critère d’erreur quadratique moyenne (en bref e.q.m.) n’est pas la panacée mais il est préféré parce qu’il s’exprime en fonction des notions simples
de biais et de variance. D’autres critères peuvent paraı̂tre tout aussi naturels,
en particulier l’erreur absolue moyenne Eθ (|Tn − θ|), mais celle-ci est beaucoup
plus diﬃcile à manipuler analytiquement.
En adoptant le critère d’e.q.m. pour juger de la précision d’un estimateur
le problème est de rechercher le meilleur estimateur au sens de ce critère, ce
qui nous conduit aux déﬁnitions suivantes.
Déﬁnition 6.5 On dit que l’estimateur Tn1 domine l’estimateur Tn2 si pour
tout θ ∈ Θ, eqmθ (Tn1 ) ≤eqmθ (Tn2 ), l’inégalité étant stricte pour au moins une
valeur de θ.
L’idéal serait de disposer d’un estimateur qui domine tous les autres. Or
il n’existe pas en général, d’estimateur d’e.q.m. minimale uniformément en θ.
Pour s’en convaincre considérons comme estimateur la v.a. certaine θ0 où θ0
est l’une des valeurs possibles. Pour celui-ci l’e.q.m. en θ = θ0 est nulle alors
que pour tout autre estimateur l’e.q.m. est strictement positive (au moins par
sa variance s’il est véritablement aléatoire ou par son biais s’il est certain). Cet
estimateur particulier ne peut donc être dominé. Néanmoins, si un estimateur
est dominé par un autre estimateur, il n’est pas utile de le retenir.
Déﬁnition 6.6 On dit qu’un estimateur est admissible s’il n’existe aucun
estimateur le dominant.
Ainsi seule est à prendre en compte la classe des estimateurs admissibles.
A partir de là, plusieurs orientations de choix sont possibles, l’une des plus
répandues étant de choisir l’estimateur pour lequel le maximum que peut atteindre l’e.q.m. sur Θ est le plus faible.
Déﬁnition 6.7 On dit que Tn∗ est minimax si pour tout autre estimateur Tn
on a :
sup eqmθ (Tn∗ ) ≤ sup eqmθ (Tn ).
θ∈Θ

θ∈Θ

102

Statistique − La théorie et ses applications

Nous ne poursuivons pas ici la recherche d’estimateurs minimax et nous
nous contenterons d’illustrer par deux exemples les propriétés de dominance et
d’admissibilité.
Exemple 6.6 Soit une loi mère N (μ, σ 2 ) et un échantillon de taille n > 1.
2
2
Montrons que, pour estimer σ 2 , S'2 = (n−1)
n S domine S . Pour ce dernier le
2σ 4
biais est nul et la variance est n−1 (voir section 5.3). D’où :


(n − 1)S 2
n−1 2
=
σ
E(S'2 ) = E
n
n


(n − 1)2 2σ 4
2(n − 1)σ 4
(n − 1)S 2
=
=
V
2
n
n
n−1
n2




2
n−1 2
(n − 1)S 2
2(n − 1)σ4
=
σ − σ2 +
eqm
n
n
n2
4
1
2(n − 1)σ
(2n − 1)σ 4
= 2 σ4 +
=
.
n
n2
n2
La diﬀérence eqm(S 2 ) − eqm(S'2 ) est donc :
(3n − 1)σ 4
2σ 4
(2n − 1)σ 4
=
−
2
n−1
n
(n − 1)n2
qui est toujours positif. Par conséquent S 2 n’est pas admissible.
En fait S'2 introduit un biais, mais celui-ci (au carré) est compensé par une
variance plus faible. Notons que ceci n’est pas vrai pour toute loi mère (voir
exercices).

Exemple 6.7 Soit à estimer le paramètre p d’une loi de Bernoulli (ou, en
situation
npratique, une proportion p par sondage dans une population). Soit
Sn = i=1 Xi le total empirique ou fréquence de succès observée. Montrons
que si p est au voisinage de 1/2 la statistique T = (Sn +1)/(n+2) est préférable,
au sens de l’e.q.m., à la proportion empirique naturelle Sn /n pour estimer p.
Comme Sn suit une loi B(n, p), on a E(Sn ) = np et V (Sn ) = np(1 − p). Pour
la proportion empirique E(Sn /n) = p, le biais est donc nul et l’e.q.m. est égale
à p(1−p)
. Pour le deuxième estimateur T, on a :
n
E(T ) =

V (Sn )
np(1 − p)
np + 1
et V (T ) =
=
n+2
(n + 2)2
(n + 2)2

d’où son e.q.m. :

eqm(T ) =

2

np + 1
−p
n+2

+

np(1 − p)
(1 − 2p)2 + np(1 − p)
=
.
2
(n + 2)
(n + 2)2

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

103

En faisant le rapport de cette e.q.m. à celle de Sn /n on obtient :


(1 − 2p)2
n
n+
.
(n + 2)2
p(1 − p)
2

n
Or pour p = 12 ceci vaut (n+2)
2 < 1 et le rapport ci-dessus étant une fonction
continue de p dans ]0, 1[ , il reste strictement inférieur à 1 dans un certain
voisinage de 1/2. Un calcul plus approfondi montrerait que ce voisinage dépend
de n et est l’intervalle
)
)
n+1 1
n+1
1
, +
[ .
] −
2
2n + 1 2
2n + 1

En conclusion, aucun des deux estimateurs ne domine l’autre.



Dans ces deux exemples on constate que si l’on accepte un certain biais, des
estimateurs apparemment naturels peuvent être moins performants au sens de
l’e.q.m.. Toutefois de nombreux statisticiens privilégient les estimateurs sans
biais signiﬁant ainsi qu’ils ne considèrent pas l’e.q.m. comme la panacée. Si
l’on se restreint à la classe des estimateurs sans biais des résultats tangibles
peuvent être obtenus dans la recherche de l’estimateur optimal et ceux-ci seront
présentés en section 6.6.

6.5.3

Convergence d’un estimateur

Nous considérons ici la suite {Tn } de v.a. à valeurs dans R lorsque la taille
n de l’échantillon s’accroı̂t à l’inﬁni, toujours avec Θ ⊆ R. Pour un estimateur
digne de ce nom on s’attend à ce qu’il se rapproche de plus en plus de θ quand
n → ∞. C’est ce qu’exprime la notion de convergence. Formellement on dira
que l’estimateur Tn est convergent selon un certain mode «m» si :
«m»

Tn −→ θ
n→∞

où «m» est à remplacer par p, p.s ou m.q. respectivement pour la convergence
en probabilité, presque sûre ou en moyenne quadratique. Étant donné qu’il y a
convergence vers une constante, rappelons (voir section 5.8) que la convergence
en loi est équivalente à la convergence en probabilité. Pour Θ ⊆ Rk la convergence en probabilité, donc la convergence en loi, et la convergence presque sûre
s’entendent composante par composante. La convergence en moyenne quadratique se généralise avec la norme . euclidienne usuelle dans Rk .
Nous énonçons tout d’abord une propriété de convergence des moments
empiriques de portée générale, dépassant le cadre paramétrique et que nous
reprendrons donc dans le cadre non paramétrique du chapitre 8.
Proposition 6.1 Si, pour la loi mère, E(|X r |) existe, alors tous les moments
empiriques jusqu’à l’ordre r, simples ou centrés, sont des estimateurs presque
sûrement convergents des moments correspondants de la loi.

104

Statistique − La théorie et ses applications

Il est clair que si les conditions d’application de la loi forte des grands
nombres selon le théorème 5.3 sont réunies pour la v.a. X r (r entier), alors le
moment empirique Mr , comme moyenne des X1r , X2r , ..., Xnr , converge presque
sûrement vers μr , moyenne de la loi de X r . Si nous nous en tenons à l’énoncé
de ce théorème, la condition est que la variance de la loi considérée existe, donc,
pour la loi de X r , que E(X 2r ) existe. Dans la proposition ci-dessus nous avons
indiqué une condition plus faible qui résulte d’une version de la loi forte des
grands nombres due à Kolmogorov.
Les moments d’ordres inférieurs existant a fortiori, ils convergent également.
Quant au moment centré Mk (k ≤ r), il converge en tant que fonction continue
de M1 , M2 , ..., Mk et nécessairement vers μk qui s’exprime par la même fonction
vis-à-vis de μ1 , μ2 , ..., μk (voir la proposition 5.15 sur la convergence d’une
fonction de v.a.).
En particulier si E(X 2 ) existe ou, de façon équivalente, si la variance de
la loi mère existe, la variance empirique S'n2 converge presque sûrement vers la
variance de cette loi (et a fortiori X̄n converge vers sa moyenne). Au passage
notons que ceci vaut aussi pour la variance d’échantillon Sn2 qui ne diﬀère de
n
.
S'n2 que par le facteur n−1
Proposition 6.2 Soit une famille paramétrique de paramètre θ de dimension
k telle que Eθ (|X k |) existe pour tout θ et qu’il existe un estimateur des moments
pour θ. Si les k premiers moments μ1 (θ), ..., μk (θ) sont des fonctions continues
de θ, alors cet estimateur est convergent presque sûrement.
En eﬀet en raison de l’hypothèse de continuité, la résolution du système
d’équations de la déﬁnition 6.2 conduit à un estimateur des moments qui s’exprime comme une fonction continue, de Rk dans Rk , des moments empiriques
μ1 , μ2 , ..., μk . En vertu de la proposition 5.15 il converge donc vers la solution du
système μ1 (θ) = μ1 (θ0 ), μ2 (θ) = μ2 (θ0 ), ..., μk (θ) = μk (θ0 ) où nous distinguons
p.s.
ici θ0 comme étant la vraie valeur de θ pour la loi mère (ainsi Mr −−→ μr (θ0 )
pour r = 1, ..., k). Du fait de l’unicité de solution en θ pour ce système, propre
à l’existence de l’estimateur des moments, cette solution ne peut être que θ0 .
La convergence est une condition sine qua non pour qualiﬁer une statistique
d’estimateur et elle sera normalement vériﬁée pour les estimateurs naturels.
Pour la loi de Cauchy généralisée de paramètre θ déﬁnie par la densité :
f (x; θ) =

1
, x ∈ R,
π[1 + (x − θ)2 ]

(pour θ = 0, c’est la loi de Student à 1 degré de liberté) on a vu dans l’exemple
2.1 que la moyenne n’existe pas. On peut se poser la question de savoir comment se comporte alors la moyenne empirique. On montre (via la fonction
caractéristique des moments, comme proposé dans un exercice du chapitre 5)

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

105

que la moyenne X̄ suit en fait la même loi ! Elle ne converge donc pas vers θ.
En fait pour estimer θ il faut prendre la médiane de l’échantillon, laquelle est
convergente.

6.5.4

Exhaustivité d’un estimateur

S’agissant d’estimer θ, certaines statistiques peuvent être exclues du fait
qu’elles n’utilisent pas de façon exhaustive toute l’information contenue dans
l’échantillon X1 , X2 , ..., Xn . A l’inverse on peut s’attendre à ce qu’un «bon» estimateur soit une statistique qui ne retienne que ce qui est utile de l’échantillon.
Les notions d’exhaustivité et d’exhaustivité minimale viennent préciser cela.
Dans cette section Θ pourra être de dimension quelconque tout comme les
statistiques considérées.
Déﬁnition 6.8 On dit que Tn est une statistique exhaustive pour θ ∈ Θ
⊆ Rk si la loi conditionnelle de (X1 , X2 , ..., Xn ) sachant Tn ne dépend pas de
θ.
Exemple 6.8 Soit la v.a. X de loi continue uniforme sur [0, θ] où θ est inconnu.
Ecrivons sa fonction de densité sous la forme :
f (x; θ) =

1
I[0,θ] (x)
θ

aﬁn d’y intégrer le fait que le support de cette densité est [0, θ], lequel dépend
donc de θ.
Pour un échantillon de taille n la densité conjointe est (en rappelant que
nous la notons également par f pour simpliﬁer, voir section 5.2) :
f (x1 , x2 , ..., xn ; θ) =

n
$

f (xi ; θ)

i=1

=

n
1 $
I[0,θ] (xi )
θn i=1

=

1
I[0,∞[ (x(1) )I]−∞,θ] (x(n) )
θn

où x(1) = min{x1 , x2 , ..., xn } et x(n) = max{x1 , x2 , ..., xn }. Déterminons la densité conditionnelle de X1 , X2 , ..., Xn sachant X(n) = t qui n’est déﬁnie que si
t ∈ [0 , θ]. Par extension à plusieurs variables de l’expression vue en section
3.2, elle est égale au rapport de la densité conjointe de (X1 , X2 , ..., Xn , X(n) ) à
la densité marginale de X(n) . Or la densité conjointe de (X1 , X2 , ..., Xn , X(n) )
en un point quelconque (x1 , x2 , ..., xn , t) est égale à la densité conjointe de
(X1 , X2 , ..., Xn ) en (x1 , x2 , ..., xn ) si t = max{x1 , x2 , ..., xn } et 0 sinon, ce qui
peut s’exprimer par la densité conjointe de l’échantillon multipliée par un facteur s(x1 , x2 , ..., xn , t) valant 1 ou 0 indépendamment de θ. Par ailleurs nous

106

Statistique − La théorie et ses applications

avons vu dans l’exemple 6.4 que la densité de X(n) au point t ∈ [0, θ] vaut
ntn−1 /θn . D’où la densité conditionnelle de X1 , X2 , ..., Xn sachant X(n) = t :
(1/θn ) . I[0,+∞[ (x(1) ) . I]−∞,θ] (t) . s(x1 , x2 , ..., xn , t)
ntn−1 /θn
dans laquelle θ disparaı̂t puisque, nécessairement, t ∈ [0, θ] ce qui entraı̂ne
I]−∞,θ] (t) = 1.
La valeur maximale de l’échantillon est donc une statistique exhaustive
pour l’estimation de θ. Intuitivement on sent bien que, θ devant être supérieur
à toute observation, la valeur maximale observée dans l’échantillon livre toute
l’information utile quant à la valeur possible de θ. Le même résultat peut être
établi, et attendu intuitivement, pour la loi discrète uniforme sur {0, 1, 2, ..., r}.
Supposons qu’on ait dans une urne r jetons numérotés de 1 à r où r est inconnu. On eﬀectue n tirages (en principe avec remise) et l’on note les numéros
x1 , x2 , ..., xn tirés. Il est clair que seul le numéro maximal observé est pertinent
pour estimer r.

Dans cet exemple on a vu que le calcul de la densité conditionnelle est loin
d’être immédiat. Le théorème suivant va nous simpliﬁer la tâche.
Théorème 6.1 Théorème de factorisation.
La statistique Tn = t(X1 , X2 , . . . , Xn ) est exhaustive pour θ si et seulement si
la densité de probabilité (ou fonction de probabilité) conjointe s’écrit, pour tout
(x1 , x2 , ..., xn ) ∈ Rn , sous la forme :
f (x1 , x2 , ..., xn ; θ) = g(t(x1 , x2 , ..., xn ); θ) h(x1 , x2 , ..., xn ).
Nous omettrons la démonstration de ce théorème que l’on trouvera dans
des ouvrages plus avancés (avec d’ailleurs des conditions mineures de validité).
Ce théorème indique que si, dans l’expression de la densité conjointe, θ entre
uniquement dans un facteur contenant une certaine fonction de x1 , x2 , ..., xn
alors cette fonction déﬁnit une statistique exhaustive. Notons, pour mémoire,
que la notion d’exhaustivité et le théorème de factorisation reposent sur la
densité conjointe seulement et, de ce fait, s’appliquent dans un cadre plus vaste
que celui d’un échantillon aléatoire.
Reprenons l’exemple précédent où :
f (x1 , x2 , ..., xn ; θ) =

1
I]−∞,θ] (x(n) ).I[0,+∞[ (x(1) ).
θn

On voit immédiatement et sans calculs que x(n) forme avec θ un facteur isolé
et, donc, que X(n) est exhaustive.

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

107

Exemple 6.9 Prenons le cas de la loi de Gauss N (μ, σ2 ) où le paramètre de
dimension 2, (μ, σ2 ), est inconnu. On a :
f (x1 , ..., xn ; μ, σ 2 ) =

n
$

1 (xi − μ)2
1
exp{−
}.
2
σ2
(2πσ 2 )1/2
i=1

En développant (xi − μ)2 et en regroupant les termes du produit on obtient :
n
!
 n
2
2μ i=1 xi
1
1
nμ2
2
i=1 xi
exp −
−
+ 2
f (x1 , ..., xn ; μ, σ ) =
.
2
σ2
σ2
σ
(2πσ 2 )n/2
n
n
n
n
Comme n’apparaissent que i=1 x2i et i=1 xi , le couple ( i=1 Xi , i=1 Xi2 )
est une statistique exhaustive. Notons qu’ici, et c’est souvent le cas, dans la
factorisation la fonction h est réduite à la constante 1. Les propositions ci-après
montrent que (X̄n , Sn2 ) est également exhaustive. Ceci signiﬁe que dans le cas
où le phénomène étudié peut être considéré comme gaussien, on peut ne retenir
de l’échantillon observé que sa moyenne et sa variance. Cette pratique est très
répandue y compris en dehors du cadre gaussien ce qui peut signiﬁer une perte
d’information pour ce qui concerne l’estimation d’un paramètre inconnu.

Proposition 6.3 Soit Tn une statistique exhaustive et Tn une statistique telle
que Tn soit une fonction de Tn . Alors Tn est également exhaustive.
Pour montrer cela, explicitons les fonctions en jeu avec Tn = u(Tn ) et Tn =
t (X1 , X2 , ..., Xn ), d’où Tn = u(t (x1 , x2 , ..., xn )). Comme Tn est exhaustive la
densité conjointe peut s’écrire :


f (x1 , x2 , ..., xn ; θ) = g(u(t (x1 , x2 , ..., xn )); θ) h(x1 , x2 , ..., xn ).
Le premier facteur contenant θ dépend des observations à travers la fonction
t (x1 , x2 , ..., xn ) qui déﬁnit Tn laquelle est donc exhaustive.
Proposition 6.4 Soit Tn une statistique exhaustive et Tn une statistique telle
que Tn = r(Tn ) où r est une fonction bijective. Alors Tn est aussi exhaustive.
Ceci résulte immédiatement du fait que l’on ait Tn = r −1 (Tn ) et que l’on
puisse appliquer la proposition 6.3.
La proposition 6.3 montre que la notion d’exhaustivité telle que nous l’avons
déﬁnie n’implique pas une réduction au minimum de l’information utile dans
l’échantillon pour estimer θ, mais une réduction suﬃsante (en anglais une statistique exhaustive est appelée suﬃcient statistic). Ainsi l’échantillon dans son
ensemble : (X1 , X2 , ..., Xn ), est une statistique évidemment exhaustive. Or, s’il
s’agit d’estimer un paramètre de dimension k, on peut s’attendre (à condition
que le nombre d’observations n soit supérieur à k, ce que nous supposerons
implicitement) à ce qu’une statistique exhaustive de dimension k procure un
résumé minimal de l’information. Toutefois la proposition 6.4 nous dit que

108

Statistique − La théorie et ses applications

celle-ci sera déﬁnie à une bijection près. Ainsi pour la famille N (μ, σ2 ), la statistique exhaustive (X̄n , Sn2 ) est sans doute minimale pour estimer (μ, σ 2 ). Nous
pouvons déﬁnir formellement la notion d’exhaustivité minimale de la façon suivante.
Déﬁnition 6.9 On dit que la statistique Tn∗ est exhaustive minimale si
elle est exhaustive et si, pour toute statistique exhaustive Tn , on peut trouver
une fonction u telle que Tn∗ = u(Tn ).
La recherche d’une statistique exhaustive minimale ne sera pas abordée ici.
Toutefois nous pourrons admettre intuitivement que, si Θ ⊆ Rk , une statistique exhaustive à valeur dans Rk est en règle générale minimale. La
mise en évidence d’une statistique exhaustive minimale est particulièrement
importante pour l’estimation. En eﬀet une statistique qui contiendrait soit une
partie seulement de l’information relative à θ, soit une part superﬂue, ne saurait
être considérée comme un estimateur adéquat de θ. Nous énonçons donc le principe suivant : tout estimateur pertinent est fonction d’une statistique
exhaustive minimale.
Pour ce qui concerne la classe exponentielle (voir section 6.3) montrons
qu’une telle statistique existe et est aisément identiﬁable.
Proposition 6.5 Soit une loi mère appartenant à une famille paramétrique
de la classe exponentielle, avec un paramètre de dimension k. Alors, dans les
notations de la déﬁnition 6.1, la statistique de dimension k :
 n
n
n



d1 (Xi ),
d2 (Xi ), ...,
dk (Xi )
i=1

i=1

i=1

est exhaustive minimale pour le paramètre inconnu.
Ceci résulte immédiatement du théorème de factorisation. En eﬀet la densité
(ou fonction de probabilité) conjointe f (x1 , x2 , ..., xn ; θ) peut s’écrire :
"
#
n
n
n
n
$
$


n
f (xi ; θ) = [a(θ)]
b(xi ) exp c1 (θ)
d1 (xi ) + · · · + ck (θ)
dk (xi ) .
i=1

i=1

i=1

i=1

Ainsi le tableau 6.1 nous livre directement les statistiques exhaustives minimales pour la plupart des lois usuelles.
Exemple 6.10 Soit la loi de Bernoulli B(p). On peut écrire sa fonction de
probabilité :
f (x; θ) = px (1 − p)1−x , x ∈ {0, 1}
p
},
f (x; θ) = (1 − p) exp{x. ln
1−p

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

109

qui répond à la forme
n de la classe exponentielle avec d(x) = x. On peut vériﬁer
directement que i=1 Xi est exhaustive, car la densité conjointe :
n
$

f (xi ; θ) = p

n

i=1

xi

n

(1 − p)n−

i=1

xi

i=1

ne dépend que de

n

i=1

xi .



Pour les lois binomiale, binomiale négative, de Poisson, exponentielle et
gamma
 avec un seul paramètre inconnu on a également d1 (x) = x, c’est-à-dire
que ni=1 Xi ou, par bijection, X̄ est une statistique exhaustive minimale. L’implication pratique de ce résultat est que les estimateurs pertinents du paramètre
concerné sont à rechercher parmi les fonctions de X̄ uniquement.
n
n
Pour la loi de Gauss le tableau 6.1 indique que le couple ( i=1 Xi2 , i=1 Xi )
est exhaustif minimal ce qui corrobore le résultat trouvé de manière directe dans
l’exemple 6.9.
n
donc i=1 ln Xi est exhaustive minimale
Pour la loi de Pareto*
d1 (x) = ln x,
*
n
n
et, celle-ci s’écrivant ln( i=1 Xi ) , i=1 Xi l’est aussi.
n
n
Enﬁn pour la loi bêta le couple ( i=1 ln X*
ln(1 − Xi )) est exhaustif
i ,
i=1 *
minimal pour (α, β), tout comme le couple ( ni=1 Xi , ni=1 (1 − Xi )).
On peut montrer que pour la classe des densités (ou fonctions de probabilité)
répondant à certaines conditions dites conditions de régularité (précisées
dans la note 6.1 ci-après) une famille de loi dont le paramètre inconnu est
de dimension k ne peut admettre une statistique exhaustive dans Rk que si
elle appartient à la classe exponentielle. Il y a donc équivalence entre appartenance à la classe exponentielle et existence d’une statistique
exhaustive de même dimension que le paramètre inconnu.
Ainsi la famille des lois de Weibull, par exemple, qui répond aux conditions
énoncées ci-après, mais qui n’est pas dans la classe exponentielle, n’admettra
pas de statistique exhaustive de dimension 2. De fait, de par la forme de la
densité :
α
f (x; α, λ) = αλxα−1 e−λx (x > 0),
aucune factorisation (au sens du théorème de factorisation) faisant apparaı̂tre
une fonction de x1 , x2 , ..., xn à valeurs dans Rp avec p < n n’est possible. Une
statistique exhaustive minimale est donc de dimension n. Pour toute situation
de ce type la statistique exhaustive minimale est, en fait, le vecteur des statistiques d’ordre (X(1) , X(2) , ..., X(n) ) et non pas (X1 , X2 , ..., Xn ) lui-même. En
tout état de cause aucun véritable résumé n’est possible si l’on veut conserver
toute l’information utile pour estimer (λ, α). Ceci est également vrai pour le paramètre (α, β) de la loi de Gumbel et, dans le cas discret, pour le paramètre M
de la loi hypergéométrique, ces lois n’appartenant pas à la classe exponentielle.

Statistique − La théorie et ses applications

110

Note 6.1 Conditions de régularité.
Dans le cas d’une densité, des conditions suﬃsantes sont que f (x; θ) soit dérivable deux fois par rapport à x à l’intérieur du support de f et dérivable par
rapport à θ dans Θ (ou par rapport à chacune de ses composantes si k > 1).
Ceci est vériﬁé pour les familles classiques dont les densités reposent toutes
sur des fonctions mathématiques dérivables. Mais ceci exclut les familles
dont le support dépend du paramètre inconnu. Prenons de nouveau la
loi U[0 , θ] dont la densité est f (x; θ) = 1θ I[0,θ] (x). La présence de la fonction
indicatrice empêche la dérivabilité par rapport à θ en tout θ ∈ R+ . En eﬀet,
x étant ﬁxé, pour θ < x la densité vaut 0 et pour θ > x elle passe à 1/θ.
Elle est donc discontinue et a fortiori non dérivable en θ = x. C’est pourquoi
cette famille, quoique n’étant pas dans la classe exponentielle, peut admettre
toutefois une statistique exhaustive de dimension 1, à savoir le maximum des
Xi comme on l’a montré dans l’exemple 6.8.
Pour conclure, la notion d’exhaustivité nous a permis de mettre en évidence,
pour la plupart des lois usuelles, quelles sont les statistiques à retenir qui, à
une fonction bijective près, devraient déboucher sur des estimateurs pertinents
pour θ. Nous nous tournons maintenant vers la classe des estimateurs sans biais
où nous pourrons encore préciser les choses et obtenir des résultats tangibles
dans la recherche des meilleurs estimateurs.

6.6
6.6.1

Recherche des meilleurs estimateurs sans
biais
Estimateurs UMVUE

Si l’on privilégie maintenant un estimateur sans biais l’objectif se ramène,
pour le critère de l’erreur quadratique moyenne, à rechercher l’estimateur dont
la variance, en l’occurrence la dispersion autour de θ lui-même, est minimale.
Toutefois, comme θ est inconnu, cela n’a d’intérêt que si un tel estimateur
domine tous les autres quel que soit θ ∈ Θ, c’est-à-dire uniformément en θ. Il
est possible qu’un tel estimateur n’existe pas, mais nous allons voir dans le cas
de la dimension 1 qu’il existe eﬀectivement et peut être mis en évidence pour
la classe des familles exponentielles qui recouvre la plupart des lois usuelles.
Déﬁnition 6.10 On dit que l’estimateur Tn∗ est UMVUE pour θ (uniformly
minimum variance unbiased estimator) s’il est sans biais pour θ et si pour tout
autre estimateur Tn sans biais on a :
Vθ (Tn∗ ) ≤ Vθ (Tn ) ,

pour tout θ ∈ Θ .

Nous adoptons ici le sigle anglais UMVUE utilisé internationalement.
Proposition 6.6 Si la famille de la loi mère appartient à la classe exponentielle avec paramètre de dimension 1 (Θ ⊆ R) et s’il existe une statistique

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

111

n
fonction de la statistique exhaustive minimale i=1 d(Xi ) qui soit sans biais
pour θ, alors elle est unique et elle est UMVUE pour θ.
La démonstration de ce théorème dépasse le cadre de cet ouvrage. Selon
cette proposition, pour trouver
nle meilleur estimateur, s’il existe, il suﬃt de
rechercher la fonction de
i=1 d(Xi ) qui soit sans biais pour θ, quel
que soit θ ∈ Θ. L’existence d’un estimateur UMVUE est donc subordonnée à
celle d’une fonction de la statistique exhaustive minimale qui soit sans biais
pour θ. Ce théorème vaut aussi pour estimer une fonction h(θ) de θ.
Exemple 6.11 Soit à estimer le paramètre λ > 0 de la loi exponentielle dont
la densité est :
f (x; λ) = λe−λx , si x ≥ 0 (0 sinon) .
Pour cette famille (voir tableau 6.1) on
exhaustive
statistique
na d(x) = x et la
n
minimale est, à une bijection près,
i=1 Xi . Or E(
i=1 Xi ) = nE(X) =
n/λ et cette
nstatistique n’est évidemment pas sans biais pour λ. Examinons
plutôt n/ i=1 Xi = 1/X̄ dont on peut penser qu’elle fasse l’aﬀaire puisque
la moyenne de la loi est 1/λ (c’est l’estimateur par la méthode
des moments).
n
Calculons son espérance mathématique en posant Tn = i=1 Xi . On sait (voir
section 4.2.3) que Tn suit une loi Γ(n , λ) d’où, supposant n > 1 :

E

1
Tn





+∞

1
λ
(λt)n−1 e−λt dt
t (n − 1)!
0
 +∞
λ
λ
(λt)n−2 e−λt dt
=
n−1 0
(n − 2)!
λ
,
=
n−1
=

car on reconnaı̂t que l’expression intégrée est la densité de la loi Γ(n − 1 , λ).
Donc, en fait, il faut choisir (n−1)/Tn pour estimer λ sans
n biais, un résultat qui
n’est absolument pas intuitif. Par conséquent (n − 1)/ i=1 Xi est l’estimateur
UMVUE de λ. On ne peut en conclure directement qu’il domine en e.q.m.
l’estimateur des moments, car celui-ci est biaisé. Toutefois, comme ce dernier
vaut n/(n − 1) fois le premier sa variance est supérieure et il est eﬀectivement
dominé.

nPour la loi de Bernoulli la statistique exhaustive minimale est également
i=1 Xi (voir tableau 6.1) et comme le paramètre p est la moyenne de la
loi, E(X̄) = p et X̄ est l’estimateur UMVUE. Dans une situation de sondage
une proportion observée dans l’échantillon est UMVUE pour la proportion
correspondante dans la population.
Pour la loi de Poisson X̄, le nombre moyen d’occurrences observées par unité
de temps (ou de surface dans un problème spatial), est également l’estimateur
UMVUE pour λ.

112

Statistique − La théorie et ses applications

Pour la loi de Gauss, à supposer que la variance σ 2 soit connue (une situation
assez hypothétique mais souvent envisagée comme cas d’école) on a encore
d(x) = x pour ce qui concerne l’estimation de μ et X̄ est donc UMVUE.
Le cas de la loi Γ(r, λ), où seul λ est inconnu, est de même nature que celui
de la loi E(λ).
Pour 
la loi de Pareto avec seuil a connu on a d(x) = ln x et l’on montre que
n
(n − 1)/ i=1 ln(Xi /a) est UMVUE pour θ (voir exercices).
Prenons encore un exemple utile en pratique, celui de la loi binomiale
négative BN (r, p) où r est connu. En eﬀet dans certaines situations on voudra
estimer la probabilité de succès p dans un processus de Bernoulli en observant le nombre d’essais qu’il aura fallu eﬀectuer jusqu’à voir le r-ième succès
(nous prendrons ci-après la version de la loi binomiale négative où la v.a. est
le nombre total d’essais, voir section 4.1.4).
Exemple 6.12 Soit X qui suit la loi BN (r, p) avec r connu et p inconnu, de
fonction de probabilité :


x−1 r
p (1 − p)x−r , x = r, r + 1, ...
f (x; p) =
x−r

Elle appartient à la classe exponentielle avec d(x) = x et ni=1 Xi est donc
statistique exhaustive minimale. Toutefois comme E(X) = r/p, elle n’est évidemment pas sans biais pour p. L’intuition nous oriente vers r/X̄ (estimateur
de la méthode des moments) mais le calcul montrerait que
ncette statistique
reste biaisée. Nous allons voir qu’il faut prendre (nr − 1)/( i=1 Xi − 1) pour
obtenir la statistique sans biais et donc UMVUE.
n
Remarquons tout d’abord que i=1 Xi correspond au nombre d’essais jusqu’à atteindre le nr-ième succès, car il est licite de mettre les séquences d’essais
bout à bout étant donné la nature du processus de Bernoulli. En conséquence
cette statistique suit une loi BN (nr, p). Le problème à plusieurs observations
est donc identique au problème à une seule observation : il suﬃt de remplacer
r par nr. D’ailleurs, en pratique, on n’eﬀectuerait qu’une série d’essais après
avoir choisi une valeur de r. Restons-en donc à n = 1 et calculons
 

∞
r−1
(x − 1)!
r−1
=
pr (1 − p)x−r
E
X −1
x
−
1
(x
−
r)!(r
−
1)!
x=r
=p
=p

∞


(x − 2)!
pr−1 (1 − p)x−r
(x − r)!(r − 2)!

x=r
∞


(t − 1)!
pr−1 (1 − p)t−(r−1)
(t
−
(r
−
1))!(r
−
2)!
t=r−1

en posant t = x − 1. On reconnaı̂t dans la dernière sommation le terme général
de la loi BN (r − 1 , p), d’où E ((r − 1)/(X − 1)) = p, ce qui prouve que

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

113

(r −1)/(X −1) est l’estimateur recherché, un résultat que l’intuition ne pouvait
laisser prévoir.

Si, sur le plan théorique, le cas des familles de la classe exponentielle est
résolu, la recherche est plus délicate pour d’autres familles comme les lois uniforme U[0, θ], Weibull, Gumbel et hypergéométrique. On montre cependant
que pour les statistiques exhaustives minimales dites complètes - une propriété,
hélas, généralement diﬃcile à vériﬁer - la proposition 6.6 énoncée pour la classe
exponentielle se généralise : une statistique exhaustive complète qui est sans
biais est UMVUE pour θ. Par ailleurs un théorème (dit de Rao-Blackwell)
établit, dans les conditions les plus générales, qu’à partir d’une statistique
sans biais quelconque on peut déduire une statistique sans biais qui domine
la première en la conditionnant sur une statistique exhaustive.
Ainsi, globalement, peut-on conclure qu’on aura toujours avantage à rechercher une fonction d’une statistique exhaustive, minimale si possible, qui soit
sans biais pour θ.
Note 6.2 : Une statistique T est dite complète pour θ s’il n’existe pas de fonction de
T , mis à part la fonction constante, dont l’espérance mathématique soit indépendante
de θ , donc :
Eθ [g(T )] = c pour tout θ ∈ Θ ⇒ g(t) = c pour toute valeur possible t.
Bien que la déﬁnition ne concerne que l’espérance mathématique il s’ensuit, en vérité,
qu’aucune fonction de T non constante ne peut avoir une loi indépendante de θ . Ceci
n’est pas nécessairement vrai pour une statistique exhaustive minimale et le fait pour
une statistique d’être complète signiﬁe une réduction encore plus forte de l’information
utile. Aussi une statistique complète est-elle a fortiori toujours exhaustive minimale.
A titre d’illustration montrons que X(n) est complète dans la famille U [0 , θ] et,
X(n) est sans biais pour θ (voir exemple 6.4), cette fonction de X(n) est
comme n+1
n
donc nécessairement UMVUE. Nous avons vu dans l’exemple 6.4 que la densité de
X(n) est f (t; θ) = ntn−1 θ −n pour θ ∈ Θ. Supposons qu’il existe g(X(n) ) telle que :


E[g(X(n) )] =
ou :

θ
0

θ

g(t)ntn−1 θ −n dt = c pour tout θ > 0

0

[g(t) − c]ntn−1 θ−n dt = 0

⇐⇒

θ
0

[g(t) − c]tn−1 dt = 0.

En dérivant par rapport à θ on obtient :

[g(θ) − c]θn−1 = 0 pour tout θ > 0 ,
ce qui implique g(θ) = c et X(n) est complète. Ainsi, si l’on privilégie le choix d’un
estimateur sans biais, n+1
n X(n) est celui qu’il faut retenir.

114

6.6.2

Statistique − La théorie et ses applications

Estimation d’une fonction de θ et reparamétrisation

Comme nous en avons fait état en section 6.2, il se peut que l’on souhaite
estimer une fonction h(θ) qui corresponde à une valeur caractéristique particulièrement intéressante de la loi mère. Si cette fonction est bijective et deux
fois dérivable tous les résultats ci-dessus restent valables et le problème reste
de rechercher une fonction d’une statistique exhaustive minimale qui soit sans
biais pour h(θ). On peut aussi considérer h(θ) comme une reparamétrisation de
la famille : posant ρ = h(θ) comme nouveau paramètre, il suﬃt de substituer
h−1 (ρ) à θ dans l’expression de f (x; θ).
Exemple 6.13 Soit à estimer e−λ , la probabilité qu’il n’y ait aucune occurrence dans une unité de temps donnée, pour une loi de Poisson. Sachant que
 n−1 T
n
est sans biais
i=1 Xi = T est exhaustive minimale, montrons que
n
pour e−λ , rappelant que T suit une loi P(nλ). D’où :

T
t
∞ 

n−1
n − 1 e−nλ (nλ)t
E
=
n
n
t!
t=0
= e−nλ

∞

[(n − 1)λ]t
t=0

t!

est le développement en série entière de e(n−1)λ .
qui est e−λ car la sommation
n
n−1
Xi
i=1
En conclusion ( n )
est UMVUE pour e−λ .


6.6.3

Borne de Cramer-Rao et estimateurs eﬃcaces

Sous certaines conditions de régularité, à la fois pour la famille étudiée et
pour l’estimateur sans biais considéré, on peut montrer que sa variance ne
peut descendre au-dessous d’un certain seuil qui est fonction de θ. Ce seuil,
appelé borne de Cramer-Rao, est intrinsèque à la forme de la densité (ou de la
fonction de probabilité) f (x; θ). L’intérêt de ce résultat est que, si l’on trouve
un estimateur sans biais dont la variance atteint ce seuil, alors il est le meilleur
possible (UMVUE) parmi les estimateurs sans biais «réguliers».
Théorème 6.2 (Inégalité de Cramer-Rao ou de Fréchet). Soit T un estimateur sans biais pour θ de dimension 1. Sous certaines conditions de régularité
on a nécessairement, pour tout θ ∈ Θ :
Vθ (T ) ≥

1
,
nI(θ)

où I(θ), appelé information de Fisher, vaut :

2 
∂
ln f (X; θ)
.
I(θ) = Eθ
∂θ

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

115

Nous omettrons la démonstration de ce théorème.
Si un estimateur sans biais pour θ atteint la borne de Cramer-Rao, on dit
qu’il est eﬃcace.

Note 6.3 Les conditions de régularité, dans le cas continu, sont les suivantes :
- I(θ) existe pour tout θ ∈ Θ
- la dérivée par rapport à θ d’une intégrale sur la densité conjointe





···

f (x1 , x2 , ..., xn ; θ)dx1 dx2 · · · dxn

peut s’obtenir en dérivant à l’intérieur de l’intégrale
- la dérivée par rapport à θ de Eθ (T ) peut s’obtenir en dérivant à l’intérieur de
l’intégrale correspondante
- le support de f (x; θ) est indépendant de θ.
Dans le cas discret les conditions portent sur les sommations en lieu et place des
intégrations.

Avant d’illustrer cette inégalité de Cramer-Rao montrons succinctement que
I(θ) peut aussi se calculer selon :
 2

∂
I(θ) = −Eθ
ln f (X; θ) ,
∂θ2
ce qui facilitera généralement les calculs (toutefois cela suppose bien sûr que
cette expression existe, mais aussi que l’on puisse dériver deux fois sous le signe
somme comme dans la démonstration qui suit). Nous nous restreindrons au cas
où f est une densité.
Démonstration. Posons :
U=
On a :
Eθ (U ) =


R

∂
f (x; θ)
∂θ

f (x; θ)

∂
f (X; θ)
∂
ln f (X; θ) = ∂θ
.
∂θ
f (X; θ)


f (x; θ)dx =
R

∂
∂
f (x; θ)dx =
∂θ
∂θ


f (x; θ)dx = 0
R

puisque cette intégrale est égale à la constante 1. De plus :
∂2
f (x; θ).f (x; θ)
∂θ 2

∂
− [ ∂θ
f (x; θ)]2
[f (x; θ]2

2
∂2
∂
∂θ 2 f (x; θ)
∂θ f (x; θ
=
,
−
f (x; θ)
f (x; θ)

∂2
ln f (x; θ) =
∂θ2

116

Statistique − La théorie et ses applications

d’où :

Eθ

 2


∂
∂2
∂θ 2 f (X; θ)
ln f (X; θ) = Eθ
− Eθ U 2 .
∂θ2
f (X; θ)

Or :

Eθ

∂2
∂θ 2 f (X; θ)

f (X; θ)




=
R

∂2
∂2
f (x; θ)dx = 2
2
∂θ
∂θ


f (x; θ)dx = 0,
R



ce qui démontre la relation.

L’expression de l’information de Fisher pour certaines lois de la classe exponentielle est donnée dans le tableau 6.1 de la section 6.3.
Exemple 6.14 Soit à estimer le paramètre λ dans la famille des lois E(λ) de
densités f (x; λ) = λe−λx pour x ≥ 0. Déterminons la borne de Cramer-Rao.
On a :
ln f (x; λ) = ln λ − λx
∂
1
ln f (x; λ) = − x,
∂λ
λ
1
∂2
ln f (x; λ) = − 2 .
2
∂λ
λ
D’où :


I(λ) = E

1
λ2


=

1
λ2

et la borne de Cramer-Rao est donc égale à λ2 /n.

Dans l’exemple 6.11 on a vu que (n − 1)/ ni=1 Xi est UMVUE pour λ. Par
le même type d’argument que pour le calcul de l’espérance de cet estimateur
eﬀectué alors dans cet exemple, on montre (en supposant n > 2) que sa variance
est :


λ2
λ2
n−1
=
>
.
V n
n−2
n
i=1 Xi
Comme il s’agit là du meilleur estimateur possible, la borne de Cramer-Rao
n’est donc pas atteignable : il n’existe pas d’estimateur eﬃcace pour λ.

Le problème qui se pose est de savoir si et quand il existe un estimateur
sans biais pour θ, ou éventuellement pour une fonction h(θ) qui atteigne la
borne de Cramer-Rao. La proposition 6.7 apportera la réponse, mais pour bien
la comprendre il n’est pas inutile de voir au préalable les implications d’une

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

117

reparamétrisation de la famille étudiée. Considérons donc un changement de
paramètre ρ = h(θ) qui ne remette pas en cause les conditions de régularité.
1
. Montrons
Pour un estimateur sans biais de ρ la variance est bornée par nI(ρ)
que I(ρ) peut se déduire aisément de I(θ).
En notant simplement par f (x; ρ) la densité (ou fonction de probabilité)
reparamétrisée avec ρ, on a :
∂
dθ
∂
1
∂
ln f (x; ρ) =
ln f (x; θ)
=
ln f (x; θ) 
,
∂ρ
∂θ
dρ
∂θ
h (θ)
d’où :
I(ρ) = Eρ




2 
2 
∂
1
∂
I(θ)
ln f (X; ρ)
ln f (X; θ)
Eθ
,
= 
= 
∂ρ
[h (θ)]2
∂θ
[h (θ)]2

où θ doit être remplacé par h−1 (ρ).
En d’autres termes, la borne de Cramer-Rao pour estimer h(θ) est :
[h (θ)]2
.
nI(θ)
Proposition 6.7 La borne de Cramer-Rao n’est atteinte que :
- si la famille de lois est dans la classe exponentielle
- et pour l’estimation
n d’une fonction de reparamétrisation particulière de θ,
à savoir h(θ) = Eθ ( i=1 d(Xi )) .
Nous admettrons cette proposition. Ainsi il n’existe qu’une fonction de
θ qui puisse être estimée de façon «eﬃcace». Pour déterminer
cette fonc
tion il suﬃt de calculer l’espérance mathématique de ni=1 d(Xi ) qui en est
donc l’estimateur eﬃcace. En réalité, pour être plus précis, cette fonction est
déﬁnie
n à une transformation linéaire près, ah(θ) + b étant estimé sans biais par
a i=1 d(Xi ) + b.
Cette proposition montre que, malgré tout, le résultat de Cramer-Rao est
d’un intérêt limité.
Exemple 6.15 (suite de l’exemple 6.14) Pour la loi exponentielle la borne
de Cramer-Rao
ne peut
la fonction de λ pour

être atteinte que pour estimer 
laquelle ni=1 d(Xi ) = ni=1 Xi est sans biais, à savoir E( ni=1 Xi ) = nE(X) =
n
. En reparamétrant avec θ = h(λ) = λ1 , θ est alors la moyenne de la loi et X̄
λ
est l’estimateur qui atteint la borne de Cramer-Rao pour θ. Celle-ci est :
[h (λ)]2
θ2
(−1/λ2 )2
1
= .
=
=
2
2
nI(λ)
n(1/λ )
nλ
n
On vériﬁe directement que V (X̄) =
variance de la loi.

1
nV

(X) =

θ2
n

puisque θ 2 =

1
λ2

est la


118

Statistique − La théorie et ses applications

Pour la loi de Bernoulli et la loi de Poisson où l’on a également d(x) = x,
la moyenne X̄ estime «eﬃcacement» les paramètres respectifs p et λ qui sont
les moyennes théoriques.
Pour la loi BN (r, p) avec r connu, on a également d(x) = x. La fonction
n
ou, plus
de p qui est estimée eﬃcacement est donc E( i=1 Xi ) = nr(1−p)
p
simplement, E(X̄) =

r(1−p)
p

.

n
ln Xi estime
Pour la loi de Pareto (a connu, θ inconnu), d(x) = ln x et i=1 
sans biais nθ + n ln a (voir exercices) ou, de façon équivalente, n1 ni=1 ln( Xai )
estime sans biais 1θ , de façon eﬃcace.
Ces deux derniers cas illustrent l’importance très relative de la notion d’efﬁcacité dans la mesure où elle est réalisée pour des fonctions du paramètre ne
présentant pas nécessairement un intérêt central.

6.6.4

Extension à un paramètre de dimension k > 1

Evacuons tout d’abord le cas où l’on s’intéresse à une fonction h(θ) à valeurs dans R. En eﬀet les notions de biais, de convergence, d’erreur quadratique, d’estimateur UMVUE restent valables. Il faut cependant noter que les
qualités de sans biais ou de variance minimale doivent être vériﬁées pour tout
θ ∈ Θ ⊆ Rk , ce qui peut poser problème. On peut, par exemple, vouloir esti2
2
mer le quantile d’ordre 0,95 de
√ la loi N (μ, σ ) où (μ, σ ) est inconnu, soit la
fonction g(μ, σ 2 ) = μ + 1, 645 σ 2 . Les qualités d’un estimateur doivent alors
être examinées quel que soit le couple (μ, σ 2 ).
Nous considérons ici l’estimation simultanée de toutes les composantes
θ1 , θ2 , ..., θk de θ. Le critère d’exhaustivité a déjà été traité avec k > 1. De
même la généralisation de la notion de convergence a été évoquée en section
6.5.3. Un estimateur T = (T1 , T2 , ..., Tk ) étant un vecteur aléatoire dans Rk le
biais est naturellement déﬁni par le vecteur Eθ (T )−θ de composantes Eθ (T1 )−
θ1 , ..., Eθ (Tk )−θk (voir l’espérance mathématique d’un vecteur aléatoire en section 3.8). Pour ce qui concerne l’extension de la notion de variance nous pouk
vons prendre la somme des variances des composantes j=1 Vθ (Tj ), le critère
d’écart quadratique correspondant étant T − θ2 où . est la norme euclidienne usuelle dans Rk . En eﬀet l’e.q.m. devient :
⎛
⎞
k
k


Eθ (T − θ2 ) = Eθ ⎝ (Tj − θj )2 ⎠ =
Eθ ([Tj − θj ]2 )
j=1

=

k

j=1

(Eθ (Tj ) − θj )2 +

j=1
k


Vθ (Tj )

j=1

où le premier terme est le carré de la norme du vecteur des biais et le deuxième
la variance globale retenue.

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

119

Toutefois ce critère présente deux inconvénients majeurs. Le premier est
qu’il est sensible aux diﬀérences d’échelle entre les composantes. Ceci peut
être atténué en introduisant des pondérations de réduction de ces échelles. Le
deuxième est qu’il ne tient pas compte des covariances existant généralement
entre les composantes Tj du fait que ces Tj sont des statistiques fonctions des
mêmes observations X1 , X2 , ..., Xn .
Il est donc préférable d’utiliser une notion de dispersion fondée sur la matrice des variances-covariances Vθ (T ) du vecteur aléatoire T (voir section 3.8).
La plus répandue est celle du déterminant de cette matrice, lequel mesure à un
facteur de proportionnalité près le volume d’ellipsoı̈des de concentration autour
du point moyen Eθ (T ) dans Rk . Avec ce critère on a un résultat analogue à la
proposition 6.6 : si la famille appartient à la classe exponentielle et s’il existe
un estimateur fonction du vecteur des statistiques exhaustives minimales
 n


d1 (Xi ),

i=1

n


d2 (Xi ), ...,

i=1

n


dk (Xi )

i=1

qui soit sans biais pour θ, alors il minimise ce critère uniformément en θ parmi
les estimateurs sans biais.
En fait il minimise également le critère de la somme simple (et même

pondérée) des variances kj=1 Vθ (Tj ). De plus il fournit l’estimateur UMVUE
pour chaque composante θj prise isolément. En règle générale on aura avantage,
comme pour k = 1, à rechercher une statistique vectorielle qui soit fonction des
statistiques exhaustives minimales et qui soit sans biais pour θ.
2
Exemple 6.16 Soit la famille N (μ, σ 2 ) où
n(μ, σ ) est
 inconnu, appartenant
à la classe exponentielle. La statistique ( i=1 Xi , ni=1 Xi2 ) est exhaustive
minimale tout comme (X̄, S 2 ), laquelle est sans biais pour (μ, σ2 ) (voir exemple
6.9). (X̄, S 2 ) est donc l’estimateur qui, parmi tous les estimateurs sans biais, a
le déterminant de sa matrice des variances-covariances le plus faible, quel que
soit (μ, σ 2 ). On a vu au chapitre 5 que V (X̄) = σ 2 /n, V (S 2 ) = 2σ 4 /(n − 1)
ainsi que l’indépendance de X̄ et S 2 . Ce déterminant est donc :


det

σ2
n

0

0
4

2σ
n−1

=

2σ 6
.
n(n − 1)

De plus X̄ est UMVUE pour μ et S 2 est UMVUE pour σ 2 .



Exemple 6.17 Soit la famille U[a, b], où (a, b) est inconnu, qui n’appartient
pas à la classe exponentielle. On démontre que (X(1) , X(n) ) est exhaustive minimale (et complète). A partir des densités de X(1) et X(n) on établit aisément

120

Statistique − La théorie et ses applications

les deux équations suivantes :
⎧
⎪
⎪
⎪ E(X(1) )
⎨
⎪
⎪
⎪
⎩ E(X(n) )

=a+

b−a
n+1

b−a
=b−
n+1

.

Pour trouver la fonction dans R2 sans biais pour (a, b) il suﬃt de résoudre ce
système d’équations en a et b. La solution étant
⎧
nE(X(1) ) − E(X(n) )
⎪
⎪
⎪
⎨ a =
n−1
⎪
⎪
⎪
⎩ b
nX

−X

nX

=

nE(X(n) ) − E(X(1) )
n−1

−X

(1)
(n)
(1)
, (n)
est sans biais pour (a, b) et donc optimal au
le couple
n−1
n−1
même sens que dans l’exemple précédent.


Pour ﬁnir décrivons la généralisation de l’inégalité de Cramer-Rao pour
k > 1.
On introduit la matrice d’information I(θ) symétrique d’ordre k dont l’élément en position (i, j) est :


∂
∂
ln f (X; θ)
ln f (X; θ) ,
Eθ
∂θi
∂θj
moyennant les mêmes types de conditions de régularité que
k = 1. On
+ pour
,
2
montre que cet élément peut aussi se calculer par −Eθ ∂θ∂i ∂θj ln f (X; θ) .
Alors, pour tout estimateur sans biais T , la variance de toute combinaison
linéaire ut T des composantes de T , où u est un vecteur quelconque de Rk , reste
−1
supérieure ou égale à ut [I(θ)]
u. Sachant que Vθ (ut T ) = ut Vθ (T )u où Vθ (T )
n
est la matrice des variances-covariances de la statistique T (voir section 3.8),
il est équivalent de dire que Vθ (T ) − n1 [I(θ)]−1 est une matrice semi-déﬁnie
positive, ce que l’on note Vθ (T ) ≥ n1 [I(θ)]−1 , quel que soit θ.
Exemple 6.18 Prenons le cas de la loi N (μ, σ 2 ) où (μ, σ 2 ) est inconnu. On a,
en posant v = σ2 :
!
1
1
f (x; μ, v) = √
exp − (x − μ)2
2v
2πv
1
1
1
ln f (x; μ, v) = − ln(2π) − ln v − (x − μ)2
2
2
2v
∂
1
∂
1
(x − μ)2
ln f (x; μ, v) = (x − μ)
ln f (x; μ, v) = − +
.
∂μ
v
∂v
2v
2v2

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

121

En position (1,1) de la matrice I(μ, σ 2 ) on trouve :

1
1
1
1
2
E 2 (X − μ) = 2 v = = 2 ,
v
v
v
σ


et en position (2,2) :

E

(X − μ)2
1
− +
2v
2v2

2 



1
(X − μ)2
(X − μ)4
=E
−
+
4v 2
2v 3
4v 4
=



1
v
3v 2
1
1
−
+
= 2 =
2
3
4
4v
2v
4v
2v
2σ 4

sachant que E (X − μ)4 = 3σ4 . En position (1,2) ou (2,1) on a :

E


1
1
(X − μ)2
)
=0
(X − μ)(− +
v
2v
2v 2

car cette expression ne contient que des moments centrés d’ordre impair. D’où :

I(μ, σ ) =
2

1
σ2

0

0
1
2σ 4


et

I(μ, σ 2 )
n



−1

=

σ2
n

0

0
2σ 4
n

.

Cette matrice est précisément la matrice des variances-covariances du couple
(X̄, S 2 ) qui est donc non seulement optimal au sens vu dans l’exemple 6.16
mais, de plus, «eﬃcace» pour estimer (μ, σ 2 ).


6.7

L’estimation par la méthode du maximum
de vraisemblance

Nous abordons maintenant deux méthodes générales qui, comme la méthode
des moments vue en section 6.4, apportent des solutions dans des situations
variées : l’approche par le maximum de vraisemblance et l’approche bayésienne.
Nous commençons par celle du maximum de vraisemblance qui est la plus
universelle (y compris pour des modèles complexes) pour deux raisons :
1. Elle est facile à mettre en oeuvre, se ramenant à un problème classique
de résolution numérique.
2. Elle est optimale et même «eﬃcace» asymptotiquement, i.e. quand la
taille de l’échantillon tend vers l’inﬁni. D’un point de vue pratique, pour
un échantillon suﬃsamment grand (disons n > 30 pour ﬁxer les idées),
elle fournit des estimateurs de très bonne qualité.

Statistique − La théorie et ses applications

122

6.7.1

Déﬁnitions

Déﬁnition 6.11 Soit un échantillon aléatoire (X1 , X2 , ..., Xn ) dont la loi mère
appartient à une famille paramétrique de densités (ou fonctions de probabilité)
{f (x; θ) , θ ∈ Θ} où Θ ⊆ Rk . On appelle fonction de vraisemblance de θ
pour une réalisation donnée (x1 , x2 , ..., xn ) de l’échantillon, la fonction de θ :
L(θ; x1 , x2 , ..., xn ) = f (x1 , x2 , ..., xn , θ) =

n
$

f (xi , θ).

i=1

L’expression de la fonction de vraisemblance est donc la même que celle
de la densité (ou fonction de probabilité) conjointe mais le point de vue est
diﬀérent. Ici les valeurs x1 , x2 , ..., xn sont ﬁxées (ce seront les valeurs eﬀectivement observées) et on s’intéresse à la façon dont varie la valeur de la densité
(ou fonction de probabilité) associée à une série d’observations donnée suivant
les diﬀérentes valeurs de θ. Dans le cas discret il s’agit directement de la probabilité Pθ (X1 = x1 , X2 = x2 , ..., Xn = xn ). S’il n’y a pas d’ambiguité possible,
on notera la fonction de vraisemblance simplement L(θ). On dira que la valeur θ1 de θ est plus vraisemblable que la valeur θ2 si L(θ1 ) > L(θ2 ). En ce
sens il devient naturel de choisir pour θ la valeur la plus vraisemblable, disons
θM V , c’est-à-dire telle que la loi f (x; θM V ) correspondante confère la plus forte
probabilité (ou densité de probabilité) aux observations relevées.
Déﬁnition 6.12 On appelle estimation du maximum de vraisemblance
une valeur θ̂M V , s’il en existe une, telle que :
L(θ̂M V ) = sup L(θ).
θ∈Θ

Une telle solution est fonction de (x1 , x2 , ..., xn ), soit θ̂ M V = h(x1 , x2 , ..., xn ).
Cette fonction h induit la statistique (notée abusivement, mais commodément,
avec le même symbole que l’estimation) θ̂ M V = h(X1 , X2 , ..., Xn ) appelée estimateur du maximum de vraisemblance (EMV).
Cette déﬁnition appelle quelques remarques :
– θ̂ M V est une fonction de Rn dans Rk , associant à tout échantillon particulier une valeur particulière de θ ;
– généralement l’EMV existe et il est unique, i.e. quel que soit (x1 , x2 , ..., xn )
il y a un et un seul maximum pour L(θ). On verra cependant dans
l’exemple 6.22 un cas où il y a plusieurs solutions ;
– la déﬁnition de l’EMV s’étend à des variables aléatoires non i.i.d. car elle
ne repose que sur la notion de densité (fonction de probabilité) conjointe.
Elle s’étend même dans un cadre non paramétrique (voir section 8.5.3) ;

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

123

– une fois la réalisation (x1 , x2 , ..., xn ) observée, l’estimation est facilement
obtenue, y compris pour des situations complexes. Il suﬃt d’utiliser un
algorithme de maximisation numérique comme on en trouve dans tous
les logiciels mathématiques.
Quand les densités (fonctions de probabilité) conjointes sont des produits
de fonctions puissances et exponentielles, ce qui est le cas la plupart du temps,
on a plutôt intérêt à maximiser ln L(θ), appelée log-vraisemblance, ce qui est
équivalent puisque la fonction logarithmique est strictement croissante. Dans
les cas «réguliers» où L(θ) est continûment dérivable et le support pour la
famille de lois considérée est indépendant de θ, l’estimation par le maximum
de vraisemblance (MV) vériﬁe (pour Θ ⊆ R) :

ou
ou

∂
ln L(θ) = 0
∂θ
n

$
∂
ln
f (xi , θ) = 0
∂θ
i=1
n

∂
ln f (xi , θ) = 0.
∂θ
i=1

Cette dernière égalité s’appelle l’équation de vraisemblance. Dans le cas où
θ possède k dimensions (θ1 , θ2 , ..., θk ), on résout un système de k équations obtenues en dérivant par rapport à chacune des composantes. Mathématiquement,
le fait d’être solution de l’équation (ou du système d’équations) de vraisemblance n’est pas une condition suﬃsante pour être un maximum. Toutefois étant
donné que L(θ) admet une borne supérieure en tant que probabilité (cas discret) mais aussi, généralement, en tant que densité de probabilité (cas continu),
et qu’elle est le plus souvent concave, l’équation admettra une solution unique
qui sera alors nécessairement un maximum. Dans les exemples, pour alléger
l’exposé, nous n’examinerons pas dans le détail si la solution de l’équation (ou
du système d’équations) correspond eﬀectivement à un maximum.

6.7.2

Exemples et propriétés

Exemple 6.19 Soit la famille de Pareto où a > 0 est connu et θ est inconnu.
On a :
f (x; θ) = θaθ x−(θ+1) , si x ≥ a et θ > 0
ln f (x; θ) = ln θ + θ ln a − (θ + 1) ln x
1
∂
ln f (x; θ) = + ln a − ln x.
∂θ
θ
L’équation de vraisemblance s’écrit :
n

n
+ n ln a −
ln xi = 0
θ
i=1

n 
xi
−
ln( ) = 0
θ i=1
a
n

ou

124

Statistique − La théorie et ses applications

d’où l’estimation :
et l’EMV :

θ̂M V = n
θ̂M V = n

n
i=1

ln( xai )

n
i=1

−1

ln( Xai )

−1

.



Exemple 6.20 Soit la famille N (μ, σ 2 ) où (μ, σ 2 ) est inconnu. On a, en posant
v = σ2 ,
1
1
exp{− (x − μ)2 }
f (x, μ, v) = √
2v
2πv
1
1
1
ln f (x; μ, v) = − ln(2π) − ln v − (x − μ)2 .
2
2
2v
En dérivant par rapport à μ d’une part et par rapport à v d’autre part, puis
en remplaçant x par xi et en sommant sur i = 1, ..., n, on obtient le système
d’équations de vraisemblance :
⎧ 

n 
xi − μ
⎪
⎪
=0
⎪
⎨
v
i=1

n
⎪
1
n
⎪
⎪
(xi − μ)2 = 0
⎩ − + 2
v
v i=1

d’où la solution μ̂ = x̄ et v̂ =
est donc (X̄, S'2 ).

1
n

n

i=1 (xi

− x̄)2 . L’EMV du paramètre (μ, σ 2 )


On constate sur ce dernier exemple que l’EMV peut avoir un biais. Dans
les deux exemples il ne dépend que des statistiques exhaustives minimales, ce
qui est une propriété générale.
Proposition 6.8 Si T est une statistique exhaustive pour θ alors θ̂ M V , s’il
existe, est fonction de T .
Ceci résulte immédiatement du théorème de factorisation. Si la statistique
T = t(X1 , X2 , ..., Xn ) est exhaustive, alors la densité (fonction de probabilité)
conjointe est de la forme :
g(t(x1 , x2 , ..., xn ), θ) h(x1 , x2 , ..., xn ) .
La maximisation vis-à-vis de θ ne concerne que la fonction g et la solution ne
dépend donc des observations qu’à travers la fonction t. Cette proposition est
vraie pour toute statistique exhaustive et en particulier pour une statistique
exhaustive minimale. Remarquons que, bien que très intéressante, cette propriété n’entraı̂ne pas que l’EMV soit UMVUE car, nous l’avons vu, il peut être
biaisé.
La proposition suivante, que nous admettrons, montre l’intérêt de l’EMV
dans le cas où il existe un estimateur eﬃcace (pour Θ ⊆ R).

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

125

Proposition 6.9 Si la famille de lois considérée répond à certaines conditions
de régularité et si elle admet un estimateur sans biais eﬃcace pour θ, alors
l’EMV existe et est cet estimateur.
Les conditions sont analogues à celles du théorème 6.2 auxquelles s’ajoute
le fait que L(θ) admette une dérivée seconde continue. On pourra vériﬁer que
l’EMV est X̄ pour le paramètre λ de la loi de Poisson et pour le paramètre
p de la loi de Bernoulli. Pour la loi exponentielle cela est vériﬁé pour la reparamétrisation θ = 1/λ. Prenons maintenant deux exemples de cas non réguliers.
Exemple 6.21 Soit la famille de loi U[0, θ]. Reprenant l’exemple 6.8 on voit
que la fonction de vraisemblance est :
L(θ) =

1
I[0,+∞[ (x(1) ).I]−∞,θ] (x(n) ) .
θn

Elle contient 1/θ n qui est une fonction décroissante de θ, mais à partir du moment où I]−∞,θ] (x(n) ) = 1, c’est-à-dire θ ≥ x(n) . Par conséquent, le maximum
est atteint pour θ = x(n) , puisque pour θ < x(n) la fonction de vraisemblance
est nulle. L’EMV est donc X(n) . Nous avons vu (voir note 6.2) que l’estimateur
X(n) . L EMV en est proche, mais il est légèrement biaisé (si
UMVUE est n+1
n
n n’est pas trop petit). Il est beaucoup plus pertinent que celui de la méthode
M
des moments obtenu par X̄ = θ̂2 , soit θ̂ M = 2X̄, qui ne repose pas sur une
statistique exhaustive et est intuitivement peu convaincant (voir exercices). 
Exemple 6.22 Considérons la loi de Laplace, ou loi exponentielle double (voir
exercices du chapitre 2), de densité :
f (x; μ) =

1 −|x−μ|
e
pour x ∈ R .
2

Nous ne sommes pas ici dans un cas régulier car cette densité n’est pas dérivable
quand x = μ. La fonction de vraisemblance :
L(μ) =

1 −  ni=1 |xi −μ|
e
2n

n’est donc pas dérivable par rapport à μ pour μ = x1 , μ = x2 , ..., μ = xn .
Considérons la log-vraisemblance :
ln L(μ) = −n ln 2 −

n


|xi − μ|.

i=1

n
Elle est maximale quand i=1 |xi − μ| est minimale. La dérivée de |xi − μ|,
pour μ = xi , est égale au signe de (xi − μ). On peut donc annuler la dérivée de
ln L(μ), si n est pair, en prenant pour valeur de μ une médiane de l’échantillon,
soit tout point dans l’intervalle (x( n2 ) , x( n2 +1) ). Admettant qu’on atteigne bien

126

Statistique − La théorie et ses applications

ainsi un minimum (ce qui est intuitif, la médiane étant au «centre» des observations) on voit que l’estimateur du MV n’est pas unique. Dans le cas où n
est impair la solution reste la médiane, qui est alors unique. La méthode des
moments donnerait l’estimateur X̄ puisque, par symétrie, μ est nécessairement
la moyenne de la loi. En fait on peut montrer que la médiane est un meilleur
estimateur, au sens de l’e.q.m., que la moyenne pour cette loi. En particulier le
rapport de la variance de la médiane empirique à celui de la moyenne empirique
tend vers 2/3 quand n → ∞.


6.7.3

Reparamétrisation et fonctions du paramètre

Une des propriétés séduisantes de l’EMV est qu’aucun nouveau calcul n’est
nécessaire en cas de changement de paramètre. On l’appelle la propriété d’invariance.
Proposition 6.10 Soit ρ = h(θ) une reparamétrisation, alors l’EMV de ρ est
ρ̂M V = h(θ̂M V ).
En eﬀet, soient Lθ et Lρ les vraisemblances respectives de θ et ρ. Comme
h est une bijection, l’ensemble des valeurs prises par L(θ) quand θ décrit Θ est
aussi l’ensemble des valeurs prises par ρ quand il décrit son espace paramétrique
que nous notons Ω. Posons ρ̂ = h(θ̂ M V ). On a donc :
Lρ (ρ̂) = Lθ (h−1 (ρ̂)) = Lθ (h−1 (h(θ̂ M V ))) = Lθ (θ̂ M V )
qui reste supérieur ou égal à Lθ (θ) pour θ ∈ Θ et donc à Lρ (ρ) pour ρ ∈ Ω.
ρ̂ est donc la valeur (unique) où Lρ atteint son maximum.
Ainsi, par exemple, X̄ étant l’EMV du paramètre p de la loi de Bernoulli,
X̄/(1 − X̄) est l’EMV du rapport p/(1 − p). Dans la classe exponentielle nous
avons mis en évidence (voir proposition 6.7) une reparamétrisation qui admet
un estimateur eﬃcace. Celui-ci, par la proposition 6.9, est l’EMV pour le paramètre correspondant. Pour la loi exponentielle X̄ est eﬃcace pour θ = 1/λ
et est donc nécessairement l’EMV de θ (et 1/X̄ est celui de λ).

Pour la loi de Pareto avec a connu, comme ni=1 ln( Xai ) estime de façon efn
−1
(voir
ﬁcace θ1 (voir exercices), on trouve pour EMV de θ : n1 i=1 ln(Xi /a)
+
,−1

n
1
exemple 6.19) alors que l’estimateur UMVUE est n−1
. Ici
i=1 ln(Xi /a)
encore on constate qu’on a un léger biais mais qu’on reste proche de l’estimateur
sans biais optimal.
Selon la propriété d’invariance, l’EMV du couple (μ, σ) de la loi N (μ, σ 2 )
' Signalons en passant que si (X̄, S 2 ) est l’estimateur UMVUE pour
est (X̄, S).
2
(μ, σ ), cela n’est pas vrai de (X̄, S) pour (μ, σ) qui est également biaisé (voir
exercices). Dans le domaine du contrôle de qualité on utilise souvent X(n) −X(1) ,

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

127

avec un coeﬃcient qui dépend de n et est tabulé, pour estimer σ, ce qui est
moins eﬃcace que S mais évidemment plus rapide.
Note 6.4 On convient d’appeler h(θ̂M V ) l’estimateur du maximum de
vraisemblance pour la fonction h(θ) du paramètre, qu’elle soit bijective
ou non. Ainsi, pour la loi de Gauss de paramètre (μ, σ 2 ), S' est l’EMV de
σ et X̄+1,645 S' est l’EMV du quantile d’ordre 0,95 : μ+1,645 σ. On donne
une légitimité à cette appellation en introduisant la fonction de vraisemblance
L1 (δ) déﬁnie sur l’ensemble des valeurs δ atteintes par h(θ), qui prend, pour
un δ donné, la valeur maximale de L(θ) pour l’ensemble des valeurs θ telles que
h(θ) = δ, i.e. :
L1 (δ) =

sup

L(θ).

θ|h(θ)=δ

Alors h(θ̂M V ) est la valeur qui maximise cette fonction de vraisemblance L1 (δ)
induite par la fonction h(θ).

6.7.4

Comportement asymptotique de l’EMV

Dans les exemples qui précèdent nous avons pu constater que, sans être
totalement optimal, l’EMV restait très proche de l’estimateur UMVUE quand
il existait et ceci d’autant plus que n était grand. Ceci se généralise par la
propriété essentielle suivante.
Proposition 6.11 Soit l’échantillon X1 , X2 , ..., Xn issu de la densité (ou fonction de probabilité) f (x; θ) où θ ∈ Θ ⊆ R, répondant à certaines conditions de
régularité qui garantissent notamment l’existence d’un EMV θ̂nM V pour tout n.
On considère la suite {θ̂nM V } quand n croı̂t à l’inﬁni. Alors cette suite est telle
que :
√ MV
1
L
n(θ̂n − θ) −
→ N (0,
).
I(θ)
Nous admettrons ce résultat qui est en fait une application indirecte de la loi
des grands nombres et du théorème central limite. Les conditions de régularité
sont celles de la proposition 6.9 complétées d’autres que nous n’expliciterons
pas car elles peuvent varier selon le type de démonstration. Le résultat énoncé
dans cette proposition implique les propriétés suivantes :
1. θ̂nM V est asymptotiquement sans biais, i.e. Eθ (θ̂nM V ) −−−−→ θ.
n→∞

θ̂nM V

se rapproche de 1/(nI(θ)).
2. pour n tendant vers l’inﬁni, la variance de
On dit que θ̂nM V est asymptotiquement eﬃcace.
3. des propriétés 1 et 2 on déduit que θ̂nM V converge vers θ en moyenne quadratique (avec un choix adéquat de conditions de régularité on démontre
que θ̂nM V converge presque sûrement).

Statistique − La théorie et ses applications

128

4. θ̂nM V tend à devenir gaussien quand n s’accroı̂t.
On résume ces propriétés en disant que l’EMV est un estimateur BAN (Best
Asymptotically Normal).
L’intérêt de ce résultat est double. D’une part il garantit que l’EMV, moyennant des conditions de régularité, soit de très bonne qualité pour les grands
échantillons (disons n > 30), d’autre part il va permettre une approximation
de sa distribution d’échantillonnage par une gaussienne, ce qui sera très utile
pour établir des intervalles de conﬁance (voir chapitre 7). La méthode des moments est loin d’oﬀrir les mêmes garanties et c’est pourquoi la méthode du
MV est la méthode de référence, notamment dans les logiciels statistiques.
Nous attirons l’attention sur le fait que les dites conditions de régularité, si
elles sont suﬃsantes pour que l’EMV soit BAN dans un cadre général, ne sont
pas nécessaires. Ainsi pour la loi exponentielle double, vue dans l’exemple 6.22,
la médiane empirique se trouve être également BAN bien que la fonction de
vraisemblance ne soit pas dérivable partout. Ceci tient au fait que la dérivabilité
n’est pas assurée uniquement pour un ensemble discret de points.
Pour ﬁnir signalons deux types d’extension de la proposition 6.11 :
1. elle reste valable pour estimer une fonction h(θ) deux fois dérivable, en
2
substituant [h (θ)] /I(θ) à 1/I(θ)
2. elle s’étend à un paramètre à k dimensions : l’EMV θ̂nM V est un vecteur
√
aléatoire tel que n(θ̂nM V −θ) tende en loi vers la loi de Gauss multivariée
à k dimensions de moyenne nulle et de matrice des variances-covariances
égale à [I(θ)]−1 , l’inverse de la matrice d’information. Nous verrons une
application de ce type pour le modèle de régression logistique au chapitre
11.

6.8

Les estimateurs bayésiens

Nous abordons ici l’approche bayésienne qui relève d’une philosophie particulière de la statistique. D’une façon générale on qualiﬁe ainsi toute approche
qui confère à tout paramètre inconnu un statut de variable aléatoire
en stipulant pour celui-ci une distribution sur Θ appelée loi a priori. Cette loi
peut résulter de la connaissance que l’on peut avoir acquise antérieurement sur
le phénomène ou être un simple artiﬁce permettant de mener à bien les calculs.
En général on tendra à utiliser une loi a priori à laquelle le résultat ﬁnal sera
relativement peu sensible (on déﬁnit notamment des lois a priori dites «non
informatives»). L’espace paramétrique étant généralement continu déﬁnissons
cette loi par une densité, notée π(θ). Pour simpliﬁer on supposera le paramètre
de dimension 1, mais l’extension à une dimension quelconque ne présente pas
de diﬃcultés.

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

129

Dans ce cadre, f (x; θ) doit être considérée maintenant comme une densité
(ou fonction de probabilité) conditionnelle pour la v.a. X étudiée, étant donné
une valeur ﬁxée du paramètre θ (il serait donc approprié de l’écrire f (x|θ) ).
En suivant la formule de Bayes qui permet de passer de la loi de probabilité
d’un événement A sachant B à la probabilité de B sachant A selon :
P (B|A) =

P (A|B)P (B)
,
P (A)

on déﬁnit la loi a posteriori de θ, c’est-à-dire après avoir pris connaissance des
réalisations (x1 , x2 , ..., xn ) de l’échantillon (X1 , X2 , ..., Xn ). Ci-après le vecteur
des réalisations sera noté x et l’échantillon sera noté X. Par transcription de
la formule de Bayes la densité a posteriori est1 :
f (x; θ)π(θ)
f (x; θ)π(θ)dθ
Θ

πθ|X=x (θ) = 

où le dénominateur est la densité marginale de X au point x ∈ Rn pour le
(n + 1)-uplet aléatoire (X, θ). Notons que dans cette formule f (x; θ) peut être
aussi bien une densité qu’une fonction de probabilité.
On prend alors comme estimation bayésienne θ̂B de θ, la moyenne de la
loi a posteriori. L’estimateur bayésien s’obtient en appliquant à X la fonction
associant à une valeur de x quelconque la valeur θ̂ B correspondante.
Les avantages de cette approche sont multiples du fait que l’on dispose d’une
loi pour θ. Entre autres :
– on peut déterminer aisément un intervalle de valeurs plausibles pour θ
(voir chapitre 7 sur l’estimation par intervalles),
– on peut estimer θ selon divers critères d’erreur. Le critère des moindres
carrés, par exemple, choisit le nombre θ̂B minimisant E[(θ − θ̂B )2 ], où
ici θ est aléatoire, ce qui correspond à la moyenne de la loi a posteriori
de θ. Nous l’avons privilégié car il est le plus répandu. Mais on pourrait
souhaiter minimiser E(|θ − θ̂ B |) ce qui débouche sur la médiane de la loi
a posteriori.
– on peut estimer toute fonction de θ en calculant directement, pour le
critère des moindres carrés, l’espérance de h(θ) sur la loi a posteriori,
soit :

h(θ) πθ|X=x (θ)dθ.
E (h(θ)) =
Θ

Exemple 6.23 Soit une loi de Bernoulli B(p). La densité conjointe au point
(x1 , x2 , ..., xn ) étant donné p ∈ [0 , 1] est :
n
$

f (xi ; θ) = p

n

i=1

xi

(1 − p)n−

n

i=1

xi

= ps (1 − p)n−s

i=1
1 Voir

à ce propos l’expression d’une densité conditionnelle en ﬁn de section 3.2.

130

Statistique − La théorie et ses applications

n
où s = i=1 xi est le nombre total de succès observé. Supposons que l’on soit
dans l’ignorance totale des valeurs préférentielles pour p et prenons une loi a
priori U[0, 1], donc : π(p) = 1 pour p ∈ [0 , 1]. La densité a posteriori est :
πp|X=x (p) =  1
0

ps (1 − p)n−s .1
ps (1 − p)n−s .1 dp

= cps (1 − p)n−s

où c est la constante appropriée pour avoir une densité. Cette densité est celle
d’une loi Beta(s, n − s) vue en section 4.2.9.
D’où l’espérance sur cette loi a posteriori que nous choisissons comme estimation de p :
 1 s+1
p (1 − p)n−s dt
.
p̂B = 0 1
s (1 − p)n−s dt
p
0
En admettant la formule d’intégration :


1

0

xa (1 − x)b dx =

Γ(a + 1)Γ(b + 1)
Γ(a + b + 2)

où Γ(r + 1) = r! si r est entier, on obtient :
p̂B =

(s + 1)!(n − s)! (n + 1)!
s+1
.
=
.
(n + 2)!
s!(n − s)!
n+2

Cette estimation correspond à l’estimateur étudié dans l’exemple 6.7 pour lequel il a été montré que l’erreur quadratique moyenne était meilleure que celle

de l’estimateur UMVUE Sn /n lorsque p se situe autour de 1/2.
On peut démontrer diverses propriétés générales des estimateurs bayésiens.
En particulier qu’ils sont convergents quelle que soit la loi a priori π(θ) choisie
(mais ayant pour support Θ) et même BAN (best asymptotically normal ) sous
des conditions de régularité de la famille {f (x; θ)}. On peut également voir sur
la formule de la densité a posteriori, qu’en raison du théorème de factorisation,
cette dernière ne dépendra que d’une statistique exhaustive minimale.
Note 6.5 Étant donné que π(θ) ﬁgure au numérateur et au dénominateur de
la densité a posteriori, il est possible de ne la déﬁnir qu’à une constante près.
On peut même envisager des fonctions qui ne sont pas des densités (pourvu
qu’elles soient positives) considérées alors comme des fonctions de pondération
des diﬀérentes valeurs possibles de θ. Dans l’exemple ci-dessus on pourrait ainsi
prendre la fonction [p(1−p)]−1 qui n’est pourtant pas intégrable sur [0, 1]. Cette
fonction donne d’ailleurs s/n comme estimation.
Dans ce chapitre nous avons traité de l’estimation ponctuelle en cherchant
à dégager les meilleurs estimateurs. Dans le chapitre suivant on considère des

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

131

«fourchettes» d’estimation où la qualité de précision des estimateurs, en particulier la variance pour les estimateurs sans biais, jouera un rôle central.
Pour approfondir l’estimation ponctuelle on pourra consulter l’ouvrage de
Lehmann et Casella (1998).

6.9

Exercices

Exercice 6.1 Montrer que la famille de lois BN (r, p) avec r connu appartient
à la classe exponentielle et que d(x) = x. Faire de même pour les lois de Poisson,
exponentielle et gamma (avec r connu).
Exercice 6.2 Montrer que la famille des lois de Pareto avec a connu appartient
à la classe exponentielle et mettre en évidence sa fonction d(x). L’estimateur
des moments dépend-il d’une statistique exhaustive minimale ?
Exercice 6.3 Montrer que la famille des lois bêta appartient à la classe exponentielle et mettre en évidence les fonctions d1 (x) et d2 (x). Donner l’estimateur
des moments de (α, β). Dépend-il d’une statistique exhaustive minimale ?
Exercice 6.4 Soit la famille N (μ, σ 2 ) où (μ, σ 2 ) est inconnu. Déduire la loi
de S de celle de (n − 1)S 2 /σ 2 . Montrer que S est un estimateur biaisé de σ et
proposer un estimateur sans biais.
Aide : calculer E(S) directement sur la loi de (n − 1)S 2 /σ 2 .
Exercice 6.5 Soit une famille de lois {f (x; θ)} telle que f (x; θ) s’écrive f (x−θ)
où f ne dépend pas de θ. (On dit que θ est un paramètre de positionnement et
on note qu’on a également F (x; θ) = F (x − θ) pour la fonction de répartition).
Supposons, de plus, que f soit une fonction paire (lois symétriques par rapport
à θ). Les résultats suivants seront établis pour n impair (mais sont valables
pour n = 2k en déﬁnissant la médiane de façon unique par 12 (X(k) + X(k+1) )).
1. Établir la fonction de répartition, puis la densité de la médiane empirique
d’un n-échantillon.
2. Montrer que sa loi est également symétrique par rapport à θ.
3. En déduire que la médiane empirique est un estimateur sans biais pour
θ.
Exercice
n 6.6 Soit la famille de loi E(λ). Comparer en e.q.m. les estimateurs
X̄ et i=1 Xi /(n + 1) pour estimer λ1 .
Aide : utiliser les résultats de l’exemple 6.11.
Exercice 6.7 Pour une loi mère quelconque ayant un moment d’ordre 4 comparer les e.q.m. de S 2 et de S'2 et donner une condition pour que le second
domine le premier.
Aide : voir la variance de S 2 dans l’exercice 5.4.

132

Statistique − La théorie et ses applications

Exercice 6.8 Montrer que X(n) est convergent en probabilité pour θ dans la
famille U[0, θ].
Aide : à partir de la fonction de répartition de X(n) écrire la probabilité de
(|X(n) − θ| < ).
Exercice 6.9 Montrer pour la famille E(λ) que l’estimateur UMVUE de λ
domine (en e.q.m.) l’estimateur des moments. On supposera n > 2.
Aide : utiliser les résultats de l’exemple 6.11.
Exercice 6.10 Soit la famille de lois (de Raleigh) de densités :
!
x
x2
f (x; θ) = exp −
, x ≥ 0, θ > 0 .
θ
2θ
Appartient-elle à la classe exponentielle ? Donner une statistique exhaustive
minimale. Calculer son espérance mathématique et en déduire un estimateur
sans biais, eﬃcace pour θ.
Exercice 6.11 Soit la famille de Pareto de paramètre a connu et θ inconnu.
1. Montrer que ln( X
a ) suit une loi E(θ).
2. Montrer que

n−1
i=1 ln(Xi /a)

n

est UMVUE pour θ.
Aide : en calculer l’espérance en s’inspirant du résultat de l’exemple 6.11.

3. Montrer que n1 ni=1 ln( Xai ) est sans biais et eﬃcace pour estimer 1/θ.
Exercice 6.12 Soit la famille de lois de Pareto où θ est connu mais a est
inconnu.
1. Constater qu’elle n’appartient pas à la classe exponentielle.
2. Donner l’estimateur des moments pour a.
3. Trouver une statistique exhaustive minimale.
4. Identiﬁer la loi de X(1) . En déduire un estimateur sans biais fonction de
X(1) .
Exercice 6.13 Calculer la borne de Cramer-Rao pour la famille N (μ , 1) où
μ est inconnu. Même question pour la famille N (0 , σ 2 ) où σ 2 est inconnu.
Dans chaque cas montrer que l’estimateur naturel est eﬃcace.
Exercice 6.14 Soit la famille des lois de Bernoulli B(p). Donner la borne de
Cramer-Rao pour un estimateur sans biais du rapport p/(1 − p).

Chapitre 6. Théorie de l’estimation paramétrique ponctuelle

133

Exercice 6.15 Calculer la borne de Cramer-Rao pour la famille des lois de
Cauchy
1
, x ∈ R.
f (x; θ) =
π[1 + (x − θ)2 ]
Aide :

1
8

arctan x +

2
1 x(x −1)
8 (x2 +1)2

est primitive de

x2
(1+x2 )3

.
√
Sachant que la médiane empirique Mn est telle que n(Mn − θ) converge en
2
loi vers la loi N (0 , π4 ) vériﬁer qu’elle n’est pas un estimateur BAN.
Exercice 6.16 Pour la famille U[0 , θ] comparer les e.q.m. des 3 estimateurs :
moments, EMV et n+1
n X(n) qui est la statistique UMVUE (voir note 6.2).
Exercice 6.17 On considère la loi de Poisson tronquée de la valeur 0. Déterminer sa fonction de probabilité. Trouver l’estimation du MV pour un échantillon
de taille 15 de moyenne 3 (ceci nécessite une approximation numérique).
Exercice 6.18 Trouver l’EMV pour le paramètre θ de la loi de Pareto avec
a connu. Quel est son biais ? (Aide : voir d’abord l’exercice 6.11). Calculer la
borne de Cramer-Rao et constater que l’EMV est asymptotiquement eﬃcace.
Exercice 6.19 Soit la famille de densités
f (x; ρ) = ρ(ρ + 1)x(1 − x)ρ−1 pour x ∈ [0, 1] ,
où ρ > 0. Donner l’EMV pour ρ.
Exercice 6.20 Soit la famille BN (r, p) où r est connu. Donner l’EMV pour p
sur la base d’une seule observation. Même question pour la famille B(n, p) où
n est connu.
Exercice 6.21 (capture-recapture) Un étang contient N poissons où N est
inconnu. M (connu) poissons ont été marqués. On pêche (sans remise) jusqu’à
ce qu’on obtienne le premier poisson marqué. Soit X le nombre de poissons
qu’on doit ainsi pêcher. Donner la loi de X en supposant un tirage aléatoire
sans remise. En déduire l’équation de vraisemblance de N associée à une (seule)
observation x de X. Application : résoudre numériquement avec M = 100 et
x = 3 pour donner une estimation de N .
Exercice 6.22 Donner l’estimateur du MV pour le paramètre λ d’une loi
Γ(r, λ) où r est connu. Donner une approximation de sa loi pour n grand.
Exercice 6.23 Soit la famille des lois de Bernoulli B(p). Donner
% la loi a posteriori pour p en utilisant une loi a priori proportionnelle à p(1 − p) et en
déduire l’estimation bayésienne de p.
Aide : on utilisera la formule d’intégration de l’exemple 6.23 et la relation
Γ(a + 1) = aΓ(a). Généraliser à une densité a priori Beta( α, β).

Chapitre 7

Estimation paramétrique
par intervalle de conﬁance
7.1

Déﬁnitions

Dans le chapitre précédent, l’objectif était de donner une valeur unique pour
estimer le paramètre inconnu θ. Dans ce chapitre, nous souhaitons donner un
ensemble de valeurs plausibles pour θ essentiellement sous forme d’un intervalle. Dans le vocabulaire courant, pour les sondages notamment, c’est l’idée
de «fourchette».
Il y a évidemment un lien entre l’approche ponctuelle et l’approche par intervalle, la seconde s’appuyant pour beaucoup sur les résultats de la première.
Si l’on s’en tient aux estimateurs sans biais, un estimateur de variance minimale
restera au plus proche de θ et on imagine qu’il sera un bon point de départ
pour fournir un encadrement. D’autre part, on s’attend à ce que sa variance
soit déterminante pour la largeur de l’intervalle. Cependant nous n’approfondirons pas vraiment la notion d’optimalité pour un intervalle de conﬁance et
consacrerons l’essentiel des développements à la construction de tels intervalles.
Pour celle-ci nous verrons tout d’abord une méthode générale exacte, mais qui
est subordonnée à l’existence d’une «fonction pivot», et ensuite une méthode
asymptotique de portée plus générale, reposant sur une approximation gaussienne en particulier via l’estimateur du maximum de vraisemblance.
Après l’approche générale nous établirons les intervalles de conﬁance classiques pour les moyennes et variances dans le cas gaussien et pour les proportions dans le cas Bernoulli. La méthode des quantiles sera développée pour
indiquer une procédure applicable aux petits échantillons (notamment dans les
cas Bernoulli et Poisson).
Par ailleurs, un mode de construction, fondé sur une procédure de test, sera
vu ultérieurement dans le chapitre 9 (section 9.8).

136

Statistique − La théorie et ses applications

Déﬁnition 7.1 Soit X1 , X2 , · · · , Xn un échantillon aléatoire issu d’une loi de
densité (ou fonction de probabilité) f (x; θ) où θ ∈ Θ est un paramètre inconnu
de dimension 1. On appelle procédure d’intervalle de conﬁance de niveau
γ tout couple de statistiques (T1 , T2 ) tel que, quel que soit θ ∈ Θ, on ait :
Pθ (T1 ≤ θ ≤ T2 ) ≥ γ .
En pratique on choisira γ assez élevé : couramment γ = 0,95. Ainsi, il y
a une forte probabilité pour que l’intervalle à bornes aléatoires [T1 , T2 ]
contienne la vraie valeur de θ. De façon imagée, on peut dire que dans l’univers
des échantillons possibles, pour une proportion au moins γ d’entre eux, on
obtient un intervalle qui contient θ.
Dans certaines situations, on peut n’être intéressé qu’à établir une borne
inférieure ou une borne supérieure pour θ, T1 ou T2 étant rejeté à l’inﬁni. On
parle alors d’intervalle de conﬁance unilatéral (par opposition à « bilatéral »).
Exemple 7.1 Prenons l’exemple quelque peu artiﬁciel d’une loi mère gaussienne dont la variance serait connue et supposons qu’elle soit égale à 1. On a,
par centrage-réduction de la moyenne empirique du n-échantillon :
X̄ − μ
√1
n

; N (0 ; 1)

d’où :
P (−1,96 <

X̄ − μ
√1
n

< 1,96) = 0,95

1,96
1,96
P (− √ < X̄ − μ < √ ) = 0,95 .
n
n
√
√
L’événement (−1,96/ √n < X̄ − μ) est équivalent√à (μ < X̄+1,96/ n) et, de
même, (X̄ − μ <1,96/ n)
√ équivaut à (X̄−1,96/
√ n < μ). On voit ﬁnalement
que l’événement
(−1,96/
n
<
X̄
−
μ
<1,96/
n) est identique à l’événement
√
√
(X̄−1,96/ n < μ < X̄+1,96/ n), d’où :
1,96
1,96
P (X̄ − √ < μ < X̄ + √ ) = 0,95
n
n
√
√
et ceci quel que soit μ, ce qui prouve que [X̄−1,96/ n , X̄ +1,96/ n] constitue
une procédure d’intervalle de conﬁance (IC) de niveau 0,95. On voit sur √
cet
exemple que la «largeur» de l’intervalle est proportionnelle à l’écart-type 1/ n
de l’estimateur ponctuel X̄ pour μ.


Chapitre 7. Estimation paramétrique par intervalle de conﬁance

137

Note 7.1 Pour les cas continus, comme dans l’exemple précédent, on peut
espérer atteindre exactement le niveau γ que l’on s’est ﬁxé, du fait de la continuité des fonctions de répartition. Pour les cas discrets, cependant, un niveau
de probabilité donné peut ne pas être atteint en raison des sauts de discontinuité. Nous donnerons plus loin un exemple illustrant cela. On se devra alors
d’avoir une attitude conservatrice, c’est-à-dire d’utiliser une procédure garantissant que [T1 , T2 ] ait une probabilité de couvrir θ qui soit au moins égale au
niveau nominal γ. C’est pourquoi il est nécessaire que γ apparaisse comme une
borne minimale de probabilité dans la déﬁnition 7.1. Notons encore, au vu de
l’exemple 7.1, que le choix de l’intervalle n’est pas unique. On aurait également
pu prendre :
X̄ − μ
< z0,98 ) = 0, 95
P (z0,03 <
1
√

n

comme point de départ ou tout autre couple de quantiles (zα , z0,95+α ) avec
α ∈ [0 ;0,05]. L’usage veut, même si la procédure n’est pas nécessairement
optimale, que l’on choisisse, comme nous l’avons fait, le couple (z 1−γ , z 1+γ ).
2
2
En fait ce choix est celui qui donne la largeur minimale lorsque la densité de la
loi utilisée pour les quantiles est symétrique et n’a qu’un seul mode.
Déﬁnition 7.2 Dans le contexte de la déﬁnition 7.1, soit x1 , x2 , · · · , xn une
réalisation de X1 , X2 , · · · , Xn conduisant à la réalisation (t1 , t2 ) de (T1 , T2 ).
Alors l’intervalle [t1 , t2 ] est appelé intervalle de conﬁance de niveau γ
pour θ et l’on note :
ICγ (θ) = [t1 , t2 ] .
L’intervalle de conﬁance est donc l’application numérique de la procédure suite à la réalisation de l’échantillon. Supposons que dans l’exemple
précédent, avec un échantillon de taille 9, on ait observé la valeur 6 pour la
moyenne de cet échantillon, alors :


1,96
1,96
√
√
IC0,95 (μ) = 6 −
,6 +
 [5,35 ; 6,65].
9
9
On ne peut dire à proprement parler (même si la tentation est forte) que
cet IC contient μ avec probabilité 0,95 du fait qu’il s’agit d’une réalisation. Soit
il contient μ, soit il ne le contient pas. C’est la procédure choisie en amont qui
garantit a priori une telle probabilité. C’est pourquoi on parle d’un niveau de
conﬁance et non de probabilité pour un IC.
Note 7.2 Lorsque l’intervalle sera symétrique par rapport à l’estimation ponctuelle on pourra aussi noter, comme pour l’application ci-dessus :
1,96
IC0,95 (μ) = 6 ± √ .
9

Statistique − La théorie et ses applications

138

Note 7.3 On remarquera que le fait d’augmenter le niveau de conﬁance accroı̂t
la largeur de l’intervalle et qu’il n’est pas possible de donner un intervalle certain
autre que Θ dans sa totalité.
Note 7.4 S’il s’agit d’estimer une fonction h(θ) bijective, par exemple strictement croissante, du paramètre θ, il suﬃt de prendre l’intervalle [h(t1 ), h(t2 )].
Nous introduisons maintenant la méthode de la fonction pivot, qui permet
de résoudre la plupart des cas classiques.

7.2

Méthode de la fonction pivot

Déﬁnition 7.3 Soit le contexte de la déﬁnition 7.1.
Une fonction g(X1 , X2 , · · · , Xn ; θ) est appelée fonction pivot si :
1. la loi de g(X1 , X2 , · · · , Xn ; θ) est connue et ne dépend pas de θ ,
2. pour tous réels u1 et u2 tels que u1 ≤ u2 et tout (x1 , x2 , · · · , xn ) ∈ Rn , la
double inégalité
u1 ≤ g(x1 , x2 , · · · , xn ; θ) ≤ u2
peut se résoudre (ou «pivoter») en θ selon :
t1 (x1 , x2 , · · · , xn ) ≤ θ ≤ t2 (x1 , x2 , · · · , xn ).
X̄−μ
√ était une fonction pivot car
Dans l’exemple 7.1, la variable aléatoire 1/
n
pour toute valeur x̄ on peut résoudre l’inégalité :

u1 <
en :

x−μ
√ < u2
1/ n

u1
u2
x̄ − √ < μ < x̄ − √ .
n
n

Notons que dans cette déﬁnition, on peut évidemment se restreindre aux
valeurs (x1 , x2 , · · · , xn ) appartenant à l’ensemble des réalisations possibles pour
θ quelconque. Remarquons aussi qu’une fonction pivot n’est pas une statistique
car elle contient le paramètre inconnu θ.
Proposition 7.1 L’existence d’une fonction pivot assure une procédure d’intervalle de conﬁance de niveau donné quelconque.
En eﬀet, il suﬃt de choisir, sur la loi connue, des quantiles u1 et u2 tels
que :
P (u1 < g(X1 , X2 , · · · , Xn ; θ) < u2 ) ≥ γ
puis de faire « pivoter », pour encadrer θ. C’est ce qui a été eﬀectué dans
l’exemple 7.1. Donnons un autre exemple.

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

139

Exemple 7.2 Soit X1 , X2 , · 
· · , Xn un échantillon de loi mère E(λ). Nous avons
n
vu en section 4.2.3 que T = i=1 Xi suit une loi Γ(n, λ) et il n’est pas inutile
de rappeler que cette statistique est exhaustive minimale. Sa densité est :
fT (t) =

λn
tn−1 e−λt
(n − 1)!

(si t ≥ 0).

Considérons la variable aléatoire λT et déterminons sa fonction de densité fλT .
Avec des notations évidentes on a :
FλT (t) = P (λT ≤ t) = P (T ≤

t
t
) = FT ( ).
λ
λ

D’où, par dérivation :
fλT (t) =

1
t
1
fT ( ) =
tn−1 e−t
λ
λ
(n − 1)!

(si t ≥ 0)

qui est la densité de la loi Γ(n, 1) et ne dépend pas de λ. De toute évidence,
la double inégalité u1 ≤ λT ≤ u2 peut «pivoter» pour isoler le paramètre
λ selon uT1 ≤ λ ≤ uT2 . Pour obtenir une procédure d’intervalle de conﬁance
de niveau, disons, 0,95 il suﬃt de choisir pour u1 et u2 respectivement, les
quantiles d’ordre 0,025 et 0,975 de la loi Γ(n, 1), c’est-à-dire les valeurs u1 et
u2 telles que :

0

u1

1
un−1 e−u du = 0, 05
(n − 1)!



u2

et
0

1
un−1 e−u du = 0,975
(n − 1)!

que l’on doit lire dans les tables de la fonction de répartition des lois gamma ou
déterminer via une fonction ad hoc d’un logiciel statistique (souvent appelée
fonction gamma inverse). On a alors :


IC0,95 (λ) = n

i=1



u2

u1
Xi

, n

i=1

Xi

.


Nous verrons d’autres illustrations de fonctions pivots dans la section 7.4
qui concerne les IC classiques.
En dehors de ces cas classiques, il n’existera généralement pas de fonction
pivot et il est nécessaire d’avoir une procédure de type universel pour couvrir
les situations les plus variées, et même complexes. La méthode qui a la portée
la plus générale est la méthode asymptotique qui fournit une approximation
d’IC d’une façon que nous allons préciser.

Statistique − La théorie et ses applications

140

7.3

Méthode asymptotique

Plaçons-nous dans le cas le plus général et supposons qu’il existe un estimateur Tn de θ tel que :
Tn − θ
sn (θ)

L

−→

N (0 ; 1)

n→∞

où sn (θ) est une fonction appropriée de θ : le plus souvent l’écart-type de Tn ou
−θ
pivote pour isoler
une fonction équivalente quand n → ∞. Si la fonction Tsnn(θ)
θ on a la procédure d’IC approchée recherchée. Sinon, Tn étant convergente
pour θ, moyennant la continuité de la fonction sn (évidemment quel que soit
convergera aussi en loi vers la loi normale centrée-réduite. Alors le
n), sTnn(T−θ
n)
pivotement est immédiat pour donner l’IC approximatif :
ICγ (θ)  [tn − z 1+γ sn (tn ) ; tn + z 1+γ sn (tn )]
2

2

où tn est la réalisation de Tn .
Cet intervalle est approximatif dans le sens où la procédure correspondante
ne garantit pas exactement le niveau γ quel que soit θ pour n ﬁni. Bien qu’il soit
diﬃcile de donner un seuil pour n à partir duquel on sera suﬃsamment proche
du niveau γ (disons à 10−2 près), on se référera à la règle n ≥ 30 indiquée
pour le théorème central limite. En eﬀet il est clair que c’est ce théorème qui
est susceptible de nous fournir un estimateur approprié comme dans l’exemple
ci-après.
Exemple 7.3 (IC pour λ de P(λ)) Nous avons vu (section 6.6.3) que X̄n ,
la moyenne de l’échantillon, est un estimateur eﬃcace pour λ du fait que pour
cette famille de lois on a d(x) = x. Cette statistique
√ est donc particulièrement
intéressante pour envisager un IC pour λ. Comme λ est l’écart-type de la loi,
le théorème central limite indique que :
X̄n − λ
(
λ
n

L

−→

N (0 ; 1) .

n→∞

Choisissant un niveau de conﬁance γ, on a :
P (−z 1+γ ≤
2

X̄n − λ
(
≤ z 1+γ )  γ.
2

λ
n

La double inégalité se ramène à une inégalité du second degré en λ que l’on
peut résoudre :
(X̄n − λ)2
λ
n

ou

λ2 − λ

≤

z 21+γ



2

z 21+γ
2X̄n +

2

n

+ X̄n2 ≤ 0 .

Chapitre 7. Estimation paramétrique par intervalle de conﬁance
Or :


Δ=

2

z 21+γ

2X̄n +

− 4X̄n2 = 4

2

n

X̄n z 21+γ
2

n

141

z 41+γ
+

2

>0

n2

et le polynôme du second degré en λ est négatif entre les racines, d’où la
procédure d’IC approximatif :
⎛
⎜
P ⎝X̄n +

z 21+γ
2

2n


−

X̄n z 21+γ
2

n

z 41+γ
+

2

4n2



z 21+γ
< λ < X̄n +

2

2n

+

X̄n z 21+γ
2

n

z 41+γ
+

2

4n2

En négligeant sous la racine le terme en n12 par rapport à celui en
celui en n1 par rapport à celui en √1n on obtient ﬁnalement :

)
)
X̄n
X̄n
P X̄n − z 1+γ
< λ < X̄n + z 1+γ
 γ.
2
2
n
n

⎞
⎟
⎠ γ.

1
n,

puis

Cet intervalle est précisément celui que l’on aurait obtenu en substituant
à λ l’estimateur X̄n dans l’expression de la variance nλ (conformément à la
substitution sn (Tn ) pour sn (θ) évoquée plus haut). On voit donc que cette
substitution est une approximation du second ordre par rapport au résultat
du théorème central limite. Généralement, il en sera ainsi et nous verrons un
cas similaire pour l’IC classique sur le paramètre p d’une loi de Bernoulli (voir
section 7.4.5). Pour conclure, retenons la formule suivante pour le paramètre λ
de la loi de Poisson :
)
x̄
IC0,95 (λ) = x̄ ± 1,96
n
qui,
en
règle
pratique,
donne
une
approximation
satisfaisante dès que l’on a
n
n
x
>
30.
Pour
x
plus
petit
on
applique
la procédure qui sera
i
i
i=1
i=1
développée au cours de l’exemple 7.6 via la méthode des quantiles.

La question qui semble se poser pour la mise en œuvre de la méthode
asymptotique est celle de l’existence d’un estimateur du type de Tn . En fait,
l’estimateur du maximum de vraisemblance, moyennant certaines conditions de régularité, fera l’aﬀaire. Nous avons même vu (voir proposition 6.11)
qu’il est un estimateur BAN (best asymptotically normal) et l’on peut montrer
qu’il fournira des IC qui auront une certaine optimalité asymptotique, notamment en termes de largeur d’intervalle. Dans les notations de la section 6.7,
nous avons (proposition 6.11) :
θ̂nM V − θ
)
1
nI(θ)

L

−→

n→∞

N (0 ; 1).

142

Statistique − La théorie et ses applications

Mis à part quelques cas simples, l’expression I(θ) de l’information de Fisher sera
telle qu’elle ne permettra pas le pivotement. On lui substituera donc I(θ̂nM V )
pour obtenir ﬁnalement la formule générale suivante (où θ̂ M V désigne cette fois
l’estimation du MV) :
⎤
⎡
z 1+γ
z 1+γ
⎦.
, θ̂M V + ( 2
ICγ (θ)  ⎣θ̂M V − ( 2
M
V
nI(θ̂
)
nI(θ̂ M V )
Exemple 7.4 (Paramètre θ de la loi de Pareto avec a connu)
n
Il a été indiqué (section 6.7.3) que l’EMV pour θ est θ̂M V = [ n1 i=1 ln( Xai )]−1 .
Par ailleurs, on peut montrer (voir exercices du chapitre 6) que la borne de
2
Cramer-Rao est θn . On a donc :
θ̂M V − θ
θ
√
n

;

approx

N (0 ; 1).

Cette expression permet le pivotement sans qu’il soit nécessaire de recourir
à la substitution de I(θ̂ M V ) pour I(θ). En eﬀet (en prenant γ = 0,95 pour
simpliﬁer) :
⎛
⎞
⎜
⎟
θ̂M V − θ
P⎜
< 1,96⎟
⎝−1,96 <
⎠  0,95
θ
√
n
⎞
⎛
⎜ θ̂M V
θ̂ M V ⎟
⎟  0,95.
<
θ
<
⇔P⎜
⎝
1,96
1,96 ⎠
1+ √
1− √
n
n
En utilisant l’approximation [1 +
1
n

par rapport au terme en

P

√1 ,
n

1,96
√ ]−1
n

 1−

1,96
√
n

qui néglige le terme en

et en faisant de même pour [1 −

1,96
√ ]−1 ,
n

on a :


1, 96
1, 96
θ̂M V (1 − √ ) < θ < θ̂ M V (1 + √ )  0, 95.
n
n

Une fois encore, cette expression est celle que l’on aurait obtenue en substiMV

tuant d’emblée θ̂ n à nθ pour la variance asymptotique de θ̂ M V .
La construction d’un IC à partir de l’EMV se heurte dans les situations
non standard à une diﬃculté pratique, à savoir la détermination de l’information de Fisher I(θ). Nous indiquons la façon dont les logiciels statistiques
qui fournissent des IC sur les paramètres des divers modèles qu’ils proposent,
contournent ce problème y compris lorsque les observations ne sont pas i.i.d.

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

143

(par exemple : modèles de régression, modèles de séries chronologiques), la seule
exigence étant de connaı̂tre la forme de la densité conjointe f (x1 , x2 , · · · , xn ; θ).
Nous verrons une situation de ce type en régression logistique au chapitre 11.
Résolution numérique pour l’IC fondé sur l’EMV
Reprenons l’expression de I(θ) donnée en section 6.6.3. Sous certaines conditions de régularité pour la densité f (x; θ) on a :
 2

∂
I(θ) = −Eθ
ln f (X; θ) .
∂θ2
Il est clair que le calcul d’une telle espérance mathématique peut être inextricable. Toutefois, en vertu de la loi des grands nombres, la v.a.
1  ∂2
ln f (Xi ; θ)
n i=1 ∂θ2
n

−

converge en probabilité (voire même presque sûrement quand la variance de
∂2
∂θ 2 ln f (X; θ) existe) vers I(θ) quand n → ∞. Ceci reste en fait valable en
remplaçant θ par θ̂nM V et on estimera donc I(θ) par l’expression ci-dessus calculée au point θ = θ̂nM V et pour les réalisations xi des Xi , soit :
1 ∂2
1  ∂2
MV
ln
f
(x
;
θ̂
)
=
−
ln L(θ̂nM V ),
i
n
n
∂θ2
n ∂θ2
n

−

i=1

où ln L(θ̂nM V ) est la log-vraisemblance de θ en θ̂nM V . Numériquement, ceci peut
être accompli de façon précise sans calculer explicitement la dérivée seconde de
la log-vraisemblance, en donnant de très faibles variations à θ autour de θ̂nM V .
Le principe de calcul approché se généralise aisément à un paramètre de
dimension k > 1 à partir de la matrice d’information de Fisher. L’élément (i, j)
de cette matrice est estimé par :
−

1 ∂2
ln L(θ̂nM V ),
n ∂θi ∂θj

où θi et θj sont, respectivement, la i-ème et la
des paramètres. On obtient des IC sur chacune
considérant les dérivées secondes par rapport à
plus loin la notion de région de conﬁance dans
simultanée de toutes les composantes.

j-ème composante du vecteur
des composantes isolément en
chaque composante. On verra
Rk pour une prise en compte

Nous abordons maintenant la construction d’IC dans les situations les plus
courantes. Elle reposera soit sur la méthode du pivot, soit sur l’approche asymptotique.

Statistique − La théorie et ses applications

144

7.4

Construction des IC classiques

Sous le terme de « classique » nous présentons les cas de la moyenne et
de la variance d’une loi mère gaussienne et le cas du paramètre p d’une loi de
Bernoulli. Nous verrons que le résultat pour la moyenne d’une gaussienne peut
servir d’approximation pour une loi mère quelconque. Quant au cas de Bernoulli
il est d’une grande importance pratique puisqu’il traite des « fourchettes »
d’estimation de proportions dans les sondages. Nous rencontrerons également
des situations nouvelles de comparaisons entre deux lois (ou, en pratique, deux
populations) distinctes. Les résultats qui suivent exploitent ceux établis au
chapitre 5 sur les distributions d’échantillonnage. Pour simpliﬁer les écritures,
nous prendrons, comme c’est l’usage, des IC de niveau γ = 0,95 faisant donc
intervenir les quantiles d’ordres 0,025 et 0,975 , le passage à une autre valeur
de γ étant évident.
Nous proposons dans la section des exercices quelques « exercices appliqués »
permettant d’illustrer l’intérêt des intervalles de conﬁance.

7.4.1

IC pour la moyenne d’une loi N (μ, σ2 )

Nous abordons d’emblée le cas où (μ, σ 2 ) est un paramètre de dimension 2
inconnu, mais nous nous intéresserons ici uniquement à un encadrement pour
μ indépendamment de σ 2 . Nous reviendrons ensuite brièvement sur le cas plus
simple, mais peu réaliste, où σ 2 est supposé connu. Rappelons le résultat du
théorème 5.2 :
X̄ − μ
t(n − 1).
;
S
approx
√
n
Cette v.a. est de toute évidence une fonction pivot pour μ et nous obtenons un IC comme suit, les développements étant de même nature que dans
l’exemple 7.1 :
⎛
⎞
⎜ (n−1)
X̄ − μ
(n−1) ⎟
⎟
<
t
P⎜
−t
<
0,975
0,975 ⎠ = 0,95
⎝
S
√
n
(n−1)

est la notation adoptée pour le quantile d’ordre α de la loi de Student
où tα
à n − 1 d.d.l. (degrés de liberté) dont nous rappelons que, comme la loi de
Gauss, elle est symétrique par rapport à 0. Il s’ensuit que :


(n−1) S
(n−1) S
= 0,95
P X̄ − t0,975 √ < μ < X̄ + t0,975 √
n
n
et le résultat très classique :


(n−1) s
(n−1) s
IC0,95 (μ) = x̄ − t0,975 √ , x̄ + t0,975 √ .
n
n

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

145

Notons que les quantiles des lois de Student sont donnés dans toutes les
tables statistiques usuelles. La largeur de cet IC, dont on peut montrer qu’elle
est minimale par rapport à d’autres éventuelles procédures, dépend d’une part
de la taille d’échantillon et d’autre part de la dispersion même de la loi mère (ou,
en pratique, de la variable étudiée dans la population) à travers l’estimation s de
son écart-type σ. Plus la population est homogène et plus la taille d’échantillon
est élevée, plus l’estimation sera précise.
Les praticiens utilisent cette formule sans se soucier de la « normalité » de
la loi mère. Ceci est, de fait, justiﬁé d’une part grâce au théorème central
X̄−μ
√
soit
limite qui assure, avec des conditions généralement réalistes, que σ/
n
à peu près gaussien dès lors que n est assez grand (rappelons que n > 30
est, en pratique, bien suﬃsant) et, d’autre part, grâce à la convergence de la
variance d’échantillon Sn2 en tant qu’estimateur de σ 2 . En réalité le deuxième
point n’est pas si clair. En eﬀet, si cette convergence est opérante pour de
grands échantillons, disons plus d’une centaine d’observations (auquel cas on
applique simplement une approximation par la loi de Gauss), on peut se poser
la question pour de plus petits échantillons dans la mesure où, pour une loi
2
(n−1)Sn
peut suivre une loi qui s’écarte sensiblement de la loi
mère quelconque,
σ2
χ2 (n − 1) théoriquement requise (voir théorème 5.1). Néanmoins on a montré
que l’approximation de Student reste relativement satisfaisante.
Ceci explique le caractère quasi universel de la formule pour IC0,95 (μ),
dont on peut dire qu’elle fournit un IC approché dans des situations non paramétriques puisqu’elle est valable pour une grande variété de lois (pour le
théorème central limite, il suﬃt que la variance existe, ce qui est aussi sufﬁsant pour la convergence de Sn2 , selon la proposition 6.1). Cependant, dans
les cas paramétriques simples, les formules d’IC établies en tenant compte des
spéciﬁcités de la famille considérée seront plus précises. Ainsi, s’il s’agit d’estimer la moyenne λ d’une loi de Poisson, le résultat obtenu dans l’exemple 7.3
est préférable car il intègre le fait que la variance de la loi est λ et qu’elle n’a
pas besoin d’être estimée indépendamment par la variance de l’échantillon.
Revenons maintenant sur la situation où σ 2 est connu qui, bien que présentée dans tous les ouvrages, est un cas d’école car rares sont les situations
pratiques de ce type. Elles ne sont toutefois pas inexistantes. Ainsi certaines
machines-outils devant usiner des pièces selon une certaine cote provoquent,
lorsqu’elles se dérèglent, un déplacement de la valeur moyenne mais conservent
le même aléa, c’est-à-dire la même variance.
En fait, le cas où σ 2 est connu a été traité dans l’exemple 7.1 où, par
commodité, on a supposé σ2 = 1. L’IC obtenu est donc :


σ
σ
IC0,95 (μ) = x̄ − 1,96 √ , x̄ + 1,96 √ .
n
n
Par contraste avec le précédent IC où σ 2 est inconnu, les quantiles sont à
lire sur la loi de Gauss du fait que σ 2 n’a pas à être estimé.

Statistique − La théorie et ses applications

146

7.4.2

IC pour la variance σ 2 d’une loi de Gauss

Nous supposerons que μ est également inconnu. Le cas d’école où μ est
connu est proposé en exercice et s’opère par une voie analogue. Reprenons le
résultat du théorème 5.1 :
(n − 1)S 2
; χ2 (n − 1),
σ2
d’où :



(n − 1)S 2
2 (n−1)
<
< χ0,975
σ2

2 (n−1)

P

χ0,025


= 0,95

2 (n−1)

dénote le quantile d’ordre α de la loi χ2 (n − 1). Ces quantiles se
où χα
trouvent dans les tables statistiques ordinaires. On peut directement isoler σ 2
pour obtenir :

P

(n − 1)S 2
2 (n−1)

< σ2 <

(n − 1)S 2

χ0,975
et


2

IC0,95 (σ ) =

2 (n−1)

= 0,95

χ0,025

(n − 1)s2
2 (n−1)

χ0,975

,

(n − 1)s2
2 (n−1)


.

χ0,025

Cet intervalle de conﬁance est peu robuste vis-à-vis de l’hypothèse gaussienne, contrairement à celui sur μ. On ne peut donc l’utiliser dans des situations où la loi mère diﬀère d’une loi normale. Ceci est vrai même pour une
grande taille d’échantillon car on montre
(voir section 8.2.2) que la loi asymp√
totique de S 2 (plus précisément de n(S 2 − σ 2 ) ) dépend de la loi mère.
Note 7.5 Suivant la note 7.4, on peut déduire un IC pour l’écart-type σ de
celui sur la variance :
⎡%
⎤
%
(n − 1)s
(n − 1)s ⎦
IC0,95 (σ) = ⎣ (
,(
.
2 (n−1)
2 (n−1)
χ0,975
χ0,025
Au passage, on peut comparer la variabilité de l’écart-type empirique S et
de la moyenne empirique X̄, pour une loi de Gauss tout du moins. En première
approximation, en appliquant la formule pour une fonction d’une v.a. établie en
2
σ2
alors que V (X̄) = σn . La ﬂuctuation
section 2.6, V (S)  4σ1 2 V (S 2 ) = 2(n−1)
de S est plus faible, ce qui se retrouve au niveau des précisions des IC (voir
exercices).

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

7.4.3

147

IC sur la diﬀérence des moyennes de deux lois de
Gauss

Nous considérons ici deux lois mères (en pratique, souvent, deux populations) et souhaitons construire un IC sur la diﬀérence de leurs moyennes. Citons
comme exemples l’écart entre la taille moyenne des ﬁlles et des garçons à l’âge de
douze ans, l’écart de revenu moyen des actifs entre telle et telle région. Pour cela
on dispose de deux échantillons aléatoires indépendants pris dans chaque
population (le fait de prendre une sœur et un frère pour l’exemple de la taille
ne respecterait pas cette hypothèse d’indépendance des deux échantillons).
La procédure classique que nous allons développer suppose que les deux
lois ont même variance σ 2 . Soit un échantillon de taille n1 issu de la loi
N (μ1 , σ 2 ) et un échantillon, indépendant du premier, de taille n2 , issu de
la loi N (μ2 , σ 2 ). Soit X̄1 et S12 , moyenne et variance empiriques du premier
échantillon et de même X̄2 et S22 pour le deuxième échantillon. On a :
σ2
)
n1
σ2
X̄2 ; N (μ2 , )
n2
X̄1 ; N (μ1 ,

et
X̄1 − X̄2 ; N (μ1 − μ2 , σ 2 (

1
1
+
))
n1
n2

(X̄1 − X̄2 ) − (μ1 − μ2 )
)
; N (0 ; 1) .
1
1
σ
+
n1
n2
Le problème qui se pose est celui de l’estimation de σ que l’on eﬀectue, en
(n −1)S 2
(n −1)S 2
fait, via σ 2 . Sachant que 1 σ2 1 ; χ2 (n1 − 1) et 2 σ2 2 ; χ2 (n2 − 1),
l’indépendance des deux échantillons entraı̂ne (voir proposition 5.7) que :
(n1 − 1)S12 + (n2 − 1)S22
; χ2 (n1 + n2 − 2) .
σ2
En faisant le rapport de la v.a. X̄1 − X̄2 centrée-réduite à la racine carrée de
la v.a. ci-dessus divisée par ses degrés de liberté, et en posant :
Sp2 =
on obtient :

(n1 − 1)S12 + (n2 − 1)S22
,
n1 + n 2 − 2

(X̄1 − X̄2 ) − (μ1 − μ2 )
)
; t(n1 + n2 − 2)
1
1
Sp
+
n1
n2

148

Statistique − La théorie et ses applications

(voir les développements similaires du théorème 5.2). La fonction ci-dessus est
une fonction pivot qui aboutit immédiatement à :
)
1
1
(n1 +n2 −2)
sp
+
IC0,95 (μ1 − μ2 ) = (x̄1 − x̄2 ) ± t0,975
n1
n2
où :

(n1 − 1)s21 + (n2 − 1)s22
n1 + n2 − 2
est la variance empirique pondérée en fonction des tailles d’échantillon respectives.
s2p =

Qu’en est-il de la condition très restrictive d’égalité des variances ? En fait,
on a pu montrer que celle-ci n’est pas si cruciale si les tailles d’échantillons
n1 et n2 diﬀèrent peu. Dans ce cas un facteur 2 pour le rapport des variances
reste acceptable. En revanche si n1 et n2 diﬀèrent substantiellement la formule
ci-dessus s’applique mal quand les variances ne sont pas proches. Alors on
peut eﬀectuer les mêmes développements que précédemment en introduisant
les variances respectives des deux lois σ12 et σ22 pour obtenir :
(X̄1 − X̄2 ) − (μ1 − μ2 )

; N (0 ; 1)
σ22
σ12
+
n1
n2
et, si les tailles d’échantillons sont élevées, disons au-delà d’une centaine, conserver une approximation raisonnable en substituant à σ12 et σ22 leurs estimations
s21 et s22 , d’où :

s2
s21
IC0,95 (μ1 − μ2 )  (x̄1 − x̄2 ) ± 1,96
+ 2.
n1
n2
On remarquera que si n1 = n2 cette formule est identique à celle du cas
où σ12 = σ22 (mis à part les quantiles d’ordre 0,975 qui seront cependant pratiquement identiques pour les grands échantillons). D’autres formules d’approximation plus précises ont été développées, mais elles donnent des résultats
numériques proches de ceux obtenus avec l’hypothèse d’égalité des variances ce
qui encourage peu leur utilisation par les praticiens.
Indiquons qu’il existe un usage assez répandu consistant à eﬀectuer au
préalable un test de l’hypothèse d’égalité des variances comme proposé en section 9.7.4. Même si l’on peut admettre que cela a l’avantage de constituer un
garde-fou, cette procédure ne fournit pas une garantie suﬃsante quant à l’applicabilité de la formule classique en cas d’acceptation de l’hypothèse par le
test.
Quant à l’usage de celle-ci en dehors des conditions de «normalité» des deux
lois, il est acceptable pour les mêmes raisons que celles exposées dans le cas

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

149

d’une seule loi (section 7.4.1). En résumé, le point critique est une diﬀérence
trop sensible des dispersions des deux lois.
Cas des échantillons appariés
Dans la mesure où cela est possible, on gagnera en précision (i.e. en largeur
d’intervalle) en associant les deux échantillons par paires ayant les mêmes valeurs sur une ou plusieurs variables auxiliaires, dites variables de contrôle. Le
gain sera d’autant plus important que ces variables auxiliaires seront liées à
la variable étudiée. S’il s’agit, par exemple, de comparer les eﬀets de deux
molécules sur la réduction de l’hypertension on mettra en œuvre un plan
d’expérience associant des paires d’individus de même âge, même sexe, même
niveau d’hypertension initial. Souvent il s’agit de mesures répétées sur le même
échantillon, l’appariement étant alors parfait.
Les développements précédents ne sont plus possibles du fait que les deux
échantillons ne sont plus indépendants. On contourne ce problème en raisonnant
sur la v.a. D «diﬀérence entre individus appariés» pour se ramener au cas d’une
seule loi.
Ceci est justiﬁé du fait que E(D) = E(X1 − X2 ) = μ1 − μ2 . En notant d¯
la moyenne des diﬀérences entre les n paires observées et sd l’écart-type de ces
diﬀérences, on a :
(n−1) sd
IC0,95 (μ1 − μ2 ) = d¯ ± t0,975 √ .
n
Les considérations de «robustesse» de la formule par rapport à l’hypothèse
de normalité vues en section 7.4.1 restent valables.

7.4.4

IC sur le rapport des variances de deux lois de
Gauss

On considère le rapport

σ12
σ22

pour les lois N (μ1 , σ1 ) et N (μ2 , σ2 ). Comme :

(n1 − 1)S12
; χ2 (n1 − 1)
σ12

et

(n2 − 1)S22
; χ2 (n2 − 1) ,
σ22

on a, par application des résultats de la section 5.5 :
S12 /σ12
S22 /σ22
D’où :


P

(n −1,n2 −1)

1
F0,025

;

<

F (n1 − 1, n2 − 1).

S12 /σ12
(n1 −1,n2 −1)
< F0,975
S22 /σ22


= 0,95 ,

150

Statistique − La théorie et ses applications

soit ﬁnalement après pivotement et compte tenu du fait (voir proposition 5.11)
(ν ,ν )
(ν2 ,ν1 )
que Fα 1 2 = 1/F1−α
,
 2

σ12
s1 (n2 −1,n1 −1) s21 (n2 −1,n1 −1)
, 2 F0,975
.
IC0,95 ( 2 ) = 2 F0,025
σ2
s2
s2
Comme pour la procédure relative à une variance (section 7.4.2), cette formule n’est pas robuste lorsque les lois s’écartent de lois gaussiennes. Son usage
est donc très limité.

7.4.5

IC sur le paramètre p d’une loi de Bernoulli

Rappelons, pour les applications, que p peut être la probabilité à estimer
pour l’occurrence d’un événement (succès). Dans le cas d’un sondage aléatoire
simple sans remise dans une grande population (taux de sondage inférieur
à 0,10, voir section 3.7), p est la proportion d’individus possédant un certain caractère dans cette population. C’est pourquoi on parle généralement
d’intervalle de conﬁance pour une proportion. Les enquêtes estimant
pour l’essentiel des proportions (ou pourcentages), les résultats qui suivent
vont fournir les «fourchettes» de précision des sondages.
La statistique sur laquelle se fondent les résultats est Sn le nombre total de
succès parmi les n répétitions dont la loi exacte est B(n, p). C’est une statistique
exhaustive minimale pour le paramètre p de la loi mère de Bernoulli. Comme
c’est le plus souvent le cas pour des distributions discrètes, on ne peut mettre
en évidence une fonction pivot. Pour obtenir des IC exacts on doit recourir
à la méthode des quantiles exposée en section 7.5. Cette méthode préside à
l’élaboration d’abaques et de tables ainsi qu’aux résultats fournis par les logiciels.
Pour l’heure nous présentons le résultat classique obtenu par l’approche
asymptotique qui s’applique dans la plupart des cas du fait des tailles d’échantillons courantes. Suite au théorème central limite nous avons vu (section 5.8.3)
que la loi B(n, p) de Sn peut être approchée convenablement par la loi de Gauss
N (np, np(1 − p)) pourvu que np > 5 et n(1 − p) > 5. D’où :
Sn − np
%
np(1 − p)

;

approx

N (0 ; 1)

qui met en évidence une v.a. de loi asymptotiquement indépendante de p et
ainsi, en négligeant la correction de continuité (voir section 5.8.3) :

Sn − np
< 1,96  0,95.
P −1,96 < %
np(1 − p)
Pour ce qui concerne le pivotement nous sommes dans une situation analogue à celle rencontrée pour le paramètre λ de la loi de Poisson de l’exemple 7.3,

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

151

à savoir que la double inégalité se ramène à une inégalité du second degré en p
que l’on peut résoudre :
(Sn − np)2
≤ (1,96)2 .
np(1 − p)
Donnons la solution ﬁnale pour l’IC, en fonction de la fréquence relative
observée p̂, réalisation de Pn = Sn /n :
)
2
p̂ + (1,96)
(1,96)4
1
(1,96)2
2n
p̂(1 − p̂) +
±
(1,96)2
(1,96)2
n
4n2
1+
1+
n

n

qui, bien que développée dans divers ouvrages, n’est jamais utilisée.
En eﬀet, les conditions de validité de l’approximation gaussienne np > 5 et
n(1 − p) > 5 n’étant pas vériﬁables puisque p est inconnu, on leur substitue des
conditions reposant sur p̂, lesquelles doivent être plus restrictives du fait que p̂
est une estimation de p. Une règle simple consiste à vériﬁer que np̂(1 − p̂) > 12.
Comme p̂(1 − p̂) reste inférieur ou égal à 1/4, cette règle implique 4/n <
2
1/12. Alors on peut négliger le terme (1,96)
devant 1, donc devant p̂ et devant
n
4p̂(1 − p̂), pour ne conserver ﬁnalement que la formule très classique :
)
p̂(1 − p̂)
IC0,95 (p)  p̂ ± 1,96
.
n
Comme pour l’exemple 7.3 on obtient directement cette formule en estimant
la variance np(1 − p) par nP̂n (1 − P̂n ), car :
⎞
⎛
n
P̂
−
np
n
< 1,96⎠  0,95
P ⎝−1,96 < (
nP̂n (1 − P̂n )
entraı̂ne :
⎛
P ⎝P̂n − 1,96




P̂n (1 − P̂n )
< p < P̂n + 1,96
n

⎞
P̂n (1 − P̂n ) ⎠
 0,95.
n

Cette approche par le théorème central limite coı̈ncide avec l’approche par
1
pour la loi de Bernoulli. Notons que la prél’EMV du fait que I(p) = p(1−p)
(
p̂)
cision absolue (terme consacré en méthodologie des sondages) : 1,96 p̂(1−
,
n
diminue quand p̂ se rapproche de 0 (n étant ﬁxé), mais que la précision relative :

%
1, 96 1 − p̂
1, 96 p̂(1 − p̂)
√
= √
p̂
p̂
n
n
tend vers l’inﬁni.

152

Statistique − La théorie et ses applications

Exemple 7.5 Soit un sondage auprès d’un échantillon de 1 000 personnes que
l’on supposera avoir été sélectionnées au hasard dans la population française
des personnes âgées de 18 ans et plus. A la question «Avez-vous une activité
sportive au moins une fois par semaine ?» 415 personnes ont répondu aﬃrmativement. Le pourcentage réel dans la population est donc estimé par :
)
(0, 415)(0, 585)
,
0,415 ± 1,96
1 000
soit 0,415±0,031.
La précision relative est 0,031
0,415 =0,074 ou 7,4 %. Comme la fonction p̂(1 − p̂)
reste entre 0,24 et 0,25 pour p̂ ∈ [0,40 ; 0,60] on peut retenir que la précision d’un
sondage auprès de 1 000 personnes est (au mieux, étant donné les imperfections
pratiques) de 0,03 soit, en pourcentage, de 3% pour une proportion située entre
40% et 60%.

Un problème classique est celui du calcul de la taille d’échantillon pour
atteindre une précision absolue donnée et nous prendrons 1% (ou 0,01) pour
exemple. Si l’on n’a aucune idée de la valeur de p on peut utiliser le fait que
p̂(1 − p̂) ≤ 14 , le maximum étant atteint pour p̂ = 12 , et la précision sera au pire
(
1
. Donc, en prenant n tel que :
de 1, 96 4n
)
1,96

soit n =

1
= 0,01
4n

(1,96)2
ou n  9600,
4(0,01)

on est sûr d’atteindre la précision souhaitée. Si l’on a une connaissance a priori
sur l’ensemble des valeurs plausibles de p (et donc, par assimilation, sur p̂)
on eﬀectue le même calcul en remplaçant p̂(1 − p̂) par son maximum sur cet
ensemble.

7.4.6

IC sur la diﬀérence des paramètres de deux lois de
Bernoulli

Soient les deux lois de Bernoulli B(p1 ) et B(p2 ) et deux échantillons indépendants issus respectivement de celles-ci, de tailles n1 et n2 . On s’intéresse à
un IC sur p1 − p2 . Les applications sont fréquentes dans les sondages pour comparer les proportions de deux sous-populations dans le choix d’une modalité de
réponse à une question donnée. On a donc aussi coutume de parler d’intervalles de conﬁance sur la diﬀérence de deux proportions . Pour respecter l’hypothèse d’indépendance des échantillons, les deux sous-populations

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

153

doivent être totalement distinctes de façon à donner des sous-échantillons euxmêmes totalement distincts. Dans ce qui suit, n1 et n2 sont supposées ﬁxées,
ce qui n’est pas forcément le cas dans cet exemple de sondage où seule la
taille globale de l’échantillon n est ﬁxée et les tailles de sous-échantillons sont
le résultat du hasard, mais ceci n’a pas vraiment d’incidence sur les résultats
établis ci-après (voir à ce propos la note 9.6).
Nous ne donnerons qu’un développement asymptotique qui suppose que les
quatre expressions n1 p1 , n1 (1 − p1 ), n2 p2 et n2 (1 − p2 ) soient toutes supérieures
ou égales à 5. Les paramètres p1 et p2 étant inconnus on peut utiliser en substitution les conditions np̂1 (1 − p̂1 ) > 12 et np̂2 (1 − p̂2 ) > 12.
Soit P̂1 et P̂2 les v.a. «proportions de succès» respectives de chaque échantillon. On a alors :




p1 (1 − p1 )
p2 (1 − p2 )
et P̂2 ; N p2 ,
,
P̂1 ; N p1 ,
approx
approx
n1
n2
puis :


P̂1 − P̂2 ; N
approx

p1 − p2 ,

p1 (1 − p1 ) p2 (1 − p2 )
+
n1
n2


.

Les variances s’additionnent en raison de l’indépendance des deux échantillons
et donc des statistiques P̂1 et P̂2 . En estimant les variances par
P̂1 (1 − P̂1 )/n1 et P̂2 (1 − P̂2 )/n2 , puis en centrant et réduisant, on a :
⎞
⎛
(
P̂
−
P̂
)
−
(p
−
p
)
1
2
1
2
≤ 1,96⎠  0,95 ,
P ⎝−1,96 ≤ (
P̂1 (1−P̂1 )
P̂2 (1−P̂2 )
+
n1
n2
ce qui pivote immédiatement pour isoler p1 −p2 et donne ﬁnalement la formule :

p1 (1 − p1 ) p2 (1 − p2 )
p1 − p2 ) ± 1,96
+
.
IC0,95 (p1 − p2 ) = (
n1
n2
On trouvera dans les exercices des applications illustrant l’intérêt de cette
formule.
Signalons, sans développer, qu’il existe une procédure exacte pour les petits
échantillons fondée sur la procédure exacte de test correspondante (voir section
9.7.6). Il existe de même une procédure pour le cas d’échantillons appariés.

7.5

IC par la méthode des quantiles

Nous présentons cette méthode, même si elle n’est pas d’un usage très
répandu, parce qu’elle est à la base des tables et abaques donnant des IC pour

154

Statistique − La théorie et ses applications

des petits échantillons, notamment pour la loi de Bernoulli et pour la loi de
Poisson. Nous exposerons la démarche dans le cas continu qui est plus simple
et indiquerons son adaptation au cas discret.
La méthode exige que l’on dispose d’un estimateur T de θ dont l’expression
de la densité fT (t; θ) est connue. Il sera bien sûr avantageux que cet estimateur
soit de bonne qualité. Il est également nécessaire que la fonction de répartition
FT (t; θ) soit, pour t ﬁxé et quelconque, une fonction strictement monotone de
θ et nous supposerons qu’elle soit, par exemple, strictement décroissante. Cela
signiﬁe que le graphe de la densité se déplace vers la droite quand θ augmente.
Déﬁnissons la fonction t0,025 (θ) qui à chaque valeur de θ associe le quantile
d’ordre 0,025 de la loi correspondante. Cette fonction est strictement croissante.
En eﬀet, pour θ  > θ, on a :
FT (t0,025 (θ); θ) = 0, 025 > FT (t0,025 (θ); θ  )
et t0,025 (θ) est donc un quantile d’ordre inférieur à 0,025 pour la loi correspondant à θ . Ainsi t0,025 (θ ), le quantile d’ordre 0,025 de cette dernière, est
supérieur à t0,025 (θ). Déﬁnissons de même la fonction t0,975 (θ) qui à chaque
valeur de θ associe le quantile d’ordre 0,975 de la loi correspondante. Cette
fonction est également strictement croissante.
Ayant observé T = t, l’IC à 95% pour θ par la méthode des quantiles
est [θ1 , θ2 ] où θ1 est tel que t0,975 (θ1 ) = t et θ2 est tel que t0,025 (θ2 ) = t
(voir la ﬁgure 7.1). En d’autres termes, θ1 est la valeur dans Θ dont la loi
correspondante a pour quantile d’ordre 0,975 la valeur observée t et de même
pour θ2 avec le quantile d’ordre 0,025. Les fonctions t0,975 (θ) et t0,025 (θ) étant
−1
monotones on peut écrire θ1 = t−1
0,975 (t) et θ2 = t0,025 (t).
Montrons qu’on a bien en amont une procédure d’intervalle de conﬁance de
−1
niveau 0,95. Considérons donc l’intervalle aléatoire [t−1
0,975 (T ) , t0,025 (T )]. On a :


−1
Pθ t−1
0,975 (T ) < θ < t0,025 (T ) = Pθ (t0,025 (θ) < T < t0,975 (θ))
ce qui, quel que soit θ, est, par déﬁnition des quantiles, égal à 0,95. L’application
de cette procédure à la loi U[0, θ] est proposée en exercice.
Cas d’une loi discrète
Intéressons-nous maintenant à une famille de lois discrètes pour laquelle
la statistique T sera également discrète de fonction de probabilité pT (x; θ)
et de fonction de répartition FT (t; θ) strictement décroissante en θ comme
précédemment. De plus, pour simpliﬁer les écritures nous supposerons que pour
tout θ l’ensemble des valeurs possibles de T est N. La procédure ci-dessus n’est
plus possible car, en raison des sauts de discontinuité de FT (t; θ), on ne peut
pas systématiquement associer à un θ donné un quantile d’ordre exactement

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

t

155

t0,975(θ)
t0,025(θ)

t observé

θ1

θ2

IC0,95

θ

Figure 7.1 - Intervalle de conﬁance par la méthode des quantiles.

égal à 0,025 ou 0,975. En revanche, pT (t; θ) étant généralement continue en θ,
pour t donné on peut toujours trouver θ1 tel que :
FT (t; θ1 ) =

t


pT (x; θ1 ) = 0, 975

x=0

et de même on peut trouver θ2 tel que FT (t; θ2 ) = 0, 025.
−1
En d’autres termes, pour tout t ∈ N, t−1
0,975 (t) et t0,025 (t) sont déﬁnis. En fait
on montre (voir la note 7.6) que si l’on veut garantir pour tout θ une probabilité
au moins égale à 0,95 (on dit alors que la procédure est conservatrice) on
doit prendre, ayant observé t, comme intervalle de conﬁance :

IC0,95 (θ) = [θ1 , θ2 ]
où :
θ1 est tel que
θ2 est tel que

t−1

x=0
t

x=0

pT (x; θ1 ) = 0,975 ,
pT (x; θ2 ) = 0,025.

156

Statistique − La théorie et ses applications

Exemple 7.6 Soit la famille des lois de Poisson et un échantillon de taille 7
issu de la loi P(λ) où λ est inconnu. La statistique eﬃcace (voir section 6.6.3)
7
T = i=1 Xi suit une loi P(7λ). Le graphe de la fonction de probabilité d’une
loi de Poisson P(ρ) se déplaçant vers la droite quand ρ augmente, la fonction de répartition FT (t; ρ) doit être strictement décroissante en ρ pour t ﬁxé.
Montrons-le rigoureusement. On a :
FT (t; ρ) =

t

e−ρ ρx
x!
x=0

t

∂
xe−ρ ρx−1 − e−ρ ρx
FT (t; ρ) = −e−ρ +
∂ρ
x!
x=1

=

t

e−ρ ρx−1

(x − 1)!

−

x=1
−t t

=−

e ρ
t!

t

e−ρ ρx
x!
x=0

< 0.

Supposons que l’on ait observé un total des observations
L’IC pour λ à 95% est donné par [λ1 , λ2 ] où :
λ1 est tel que
λ2 est tel que

17

e−7λ1 (7λ1 )x
x=0
18


x!

7
i=1

xi = 18.

= 0,975 ,

e−7λ2 (7λ2 )x
= 0,025.
x!
x=0

En recourant à un logiciel mathématique on trouve les solutions 7λ1 =10,7 et
7λ2 =28,4 soit ﬁnalement :


10,7 28,4
IC0,95 (λ) =
;
= [1,53 ; 4,06] .
7
7
En exercice 7.4, on montrera comment on peut également résoudre les deux
équations ci-dessus à l’aide d’une table des lois du Khi-deux.
Comparons ce résultat avec celui de la formule asymptotique de l’exemple 7.3 :
)
18/7
18
± 1, 96
IC0,95 (λ) =
7
7
= [1,38 ; 3,76].
Ce dernier est un peu plus étroit mais, étant approché, on ne peut garantir
le niveau 0,95 , à savoir que la probabilité de couverture de λ par la méthode
asymptotique n’est pas nécessairement égale ou supérieure à 0,95 pour tout

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

157

7
λ. Si l’on avait trouvé i=1 xi = 50, la procédure conservatrice aurait donné
l’intervalle [5,29 ; 9,41] et la procédure asymptotique [5,16 ; 9,12]. De fait, les
procédures se rapprochent quand λ augmente.

Dans les tables ou les logiciels statistiques
n élaborés on obtient directement
les valeurs nλ1 et nλ2 pour les valeurs de i=1 xi de 0 à 50. Au-delà, on peut
utiliser l’IC asymptotique.
L’approche est identique pour un IC sur le paramètre p dela loi de
n
Bernoulli pour laquelle statistique T est également la somme
i=1 Xi (le
nombre totalde succès), de loi B(n, p). Ici il faut tenir compte à la fois de
n
la valeur de i=1 xi et de celle de n. C’est pourquoi les bornes de l’IC sont
données sous forme d’abaques. Un exemple pour cette loi est proposé dans les
exercices.
Note 7.6 Montrons que la procédure adoptée pour le cas discret est conservatrice.
Pour la valeur t observée, la borne θ1 est telle que FT (t − 1; θ1 ) = 0, 975. Calculons
pour un θ quelconque Pθ (θ1 (T ) ≤ θ). L’événement {θ1 (T ) ≤ θ} est la partie A de N
déﬁnie par A = {t | |θ1 (t) ≤ θ} ou, puisque FT (u; θ) est strictement décroissante en θ
pour u ﬁxé quelconque, de façon équivalente,

A = {t | FT (t − 1; θ1 ) ≥ FT (t − 1; θ)} = {t | FT (t − 1; θ) ≤ 0, 975}.
A est donc constitué de toutes les valeurs de t de 0 à t0 où t0 est la première valeur
telle que FT (t0 ; θ) > 0, 975. D’où P (A) = FT (t0 ; θ) > 0, 975.
Par la même argumentation on peut montrer que Pθ (θ ≤ θ2 (T )) > 0, 975 pour
tout θ, d’où Pθ (θ1 (T ) < θ < θ2 (T )) > 0, 975 − 0, 025 = 0, 95.

7.6

Approche bayésienne

Dans cette approche nous avons vu en section 6.8 que le paramètre θ avait
un statut de variable aléatoire. A la notion d’intervalle de conﬁance de niveau γ
on substitue la notion d’intervalle de probabilité γ sur la loi a posteriori de θ.
On encadrera donc simplement θ avec les quantiles d’ordre 0,025 et 0,975 sur
cette loi.
Exemple 7.7 Reprenons l’exemple 6.23 de l’estimation du paramètre p d’une
loi de Bernoulli avec une loi a priori U[0, 1]. La loi a posteriori est une loi
Beta(s, n − s), où s est le nombre de succès, dont on peut trouver les quantiles
dans les logiciels statistiques ou dans les tables. Supposons que pour n = 20
répétitions on ait observé s = 8 succès. On lit dans une table les quantiles
d’ordres 0,025 et 0,975 de la loi Beta(8; 12) : 0,22 et 0,62 respectivement. D’où
l’intervalle de probabilité 0,95 pour p : [0,22 ; 0,62], à comparer avec l’intervalle
[0,19 ; 0,64] indiqué par la méthode des quantiles dans l’exercice 7.5.


158

7.7

Statistique − La théorie et ses applications

Notions d’optimalité des IC

Le premier critère d’optimalité est celui de largeur minimale des intervalles
produits par la procédure. C’est d’ailleurs cette idée qui nous a conduit à choisir
les quantiles de façon symétrique sur les extrémités de la distribution (soit les
quantiles d’ordre 1−γ
et 1+γ
pour un IC de niveau 1 − γ) dans la mesure
2
2
où l’on obtient ainsi l’intervalle le plus court lorsque la distribution concernée
est symétrique avec un seul mode (voir note 7.1). Nous déﬁnissons ci-après la
notion de procédure de largeur minimale qui explicite le fait que l’IC doit être
le plus court quelle que soit la réalisation (x1 , x2 , · · · , xn ).
Déﬁnition 7.4 Une procédure d’IC est dite de largeur minimale au niveau
γ si la largeur de son IC de niveau γ : [t1 (x1 , x2 , · · · , xn ), t2 (x1 , x2 , · · · , xn )],
est inférieure à celle de tout autre IC dérivé d’une procédure de niveau égal ou
supérieur à γ, et ceci pour toute réalisation (x1 , x2 , · · · , xn ).
Il n’est évidemment pas aisé de dégager une telle procédure. Cela ne peut
être fait que dans quelques cas simples mais, en général, une telle procédure
n’existera pas. Un critère plus faible consiste à raisonner non pas pour toute
réalisation, mais par rapport à l’espérance mathématique de la largeur de l’intervalle Eθ [t2 (X1 , X2 , · · · , Xn ) − t1 (X1 , X2 , · · · , Xn )] quel que soit θ.
Cependant il existe un résultat asymptotique intéressant concernant la
procédure de la section 7.3 reposant sur l’estimateur du maximum de vraisemblance, résultat qui découle de l’optimalité asymptotique de celui-ci (voir
proposition 6.11) : sous certaines conditions de régularité cette procédure fournira une largeur d’intervalle qui, en espérance mathématique, tendra à être
minimale pour n → ∞.
Un autre critère d’optimalité est fourni par la notion de procédure uniformément plus précise. Cette notion semblera ici quelque peu complexe mais
elle deviendra plus claire lorsqu’aura été vue la notion duale de test uniformément plus puissant au chapitre 9.
Déﬁnition 7.5 Une procédure d’IC (T1∗ , T2∗ ) est dite uniformément plus
précise au niveau γ qu’une procédure (T1 , T2 ) si, étant toutes deux de niveau
γ, on a :
Pθ (T1∗ < θ  < T2∗ ) ≤ Pθ (T1 < θ  < T2 )
pour tout θ ∈ Θ et pour tout θ ∈ Θ diﬀérent de θ, l’inégalité étant stricte pour
au moins une valeur de θ.
En d’autres termes, la procédure sera plus précise si la probabilité d’encadrer une valeur θ autre que la vraie valeur de θ reste plus faible. L’objectif sera
alors de rechercher, si elle existe, la procédure uniformément la plus précise (en
anglais : uniformly most accurate ou UMA) parmi l’ensemble des procédures
de niveau égal (ou supérieur) à γ.

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

159

Enﬁn, on peut souhaiter d’une procédure que la largeur des IC fournis tende
vers 0 quand la taille de l’échantillon s’accroı̂t.
Déﬁnition 7.6 Soit une procédure d’IC fondée pour n ∈ N∗ sur l’intervalle
aléatoire [T1,n , T2,n ]. On dit que cette procédure est convergente en probabilité
si la suite {T2,n − T1,n } est telle que :
T2,n − T1,n

p

−→

n→∞

0.

Étant donné que, pour chaque n, l’intervalle [T1,n , T2,n ] doit contenir la vraie
valeur de θ avec une forte probabilité, cet intervalle se réduira, à la limite, à
cette valeur. Prenons par exemple l’intervalle :


(n−1) Sn
(n−1) Sn
X̄n − t0,975 √ , X̄n + t0,975 √
n
n
de la procédure classique pour la moyenne μ de la loi N (μ , σ2 ) vue en sec(n−1) Sn
tion 7.4.1. La largeur de l’intervalle est 2 t0,975 √
. Comme Sn2 converge en
n
2
probabilité vers σ , Sn converge vers σ et la largeur converge en probabilité
vers 0. Par ailleurs X̄n converge vers μ et cet intervalle se réduit à μ à l’inﬁni.
Cette propriété de convergence est vériﬁée pour tous les intervalles classiques
que nous avons présentés. Elle est également vraie pour la procédure asymptotique par l’EMV (voir section 7.3) dans la mesure où I(θ̂nM V ) converge vers
I(θ0 ) où θ0 est la vraie valeur de θ.

7.8

Région de conﬁance pour un paramètre de
dimension k > 1

Pour simpliﬁer nous prendrons k = 2, l’extension à k quelconque ne présentant pas de diﬃcultés particulières. Soit donc θ = (θ1 , θ2 ) le paramètre
inconnu. Le problème est maintenant de déterminer une région aléatoire du
plan qui contienne θ avec une probabilité donnée quel que soit θ.
Supposons d’abord que l’on sache construire séparément pour chaque composante une procédure d’IC de niveau γ et soit I1 et I2 les intervalles aléatoires
correspondants. Pour tout θ ∈ Θ ⊆ R2 , on a alors :
Pθ (θj ∈ Ij ) = γ,

j = 1, 2,

en admettant, également pour simpliﬁer, que la probabilité γ est exactement
atteinte pour chaque θ. Considérons la région aléatoire constituée du rectangle
I1 × I2 . Pour θ ﬁxé on a :
Pθ (θ ∈ I1 × I2 ) = Pθ (θ1 ∈ I1 , θ2 ∈ I2 ) = Pθ ((θ1 ∈ I1 ) ∩ (θ2 ∈ I2 ))

160

Statistique − La théorie et ses applications

qui sera toujours inférieur à γ (sauf cas très particulier où l’un des événements
implique l’autre). Généralement ces deux événements seront dépendants (du
fait qu’ils reposent sur les mêmes observations) et il sera diﬃcile de déterminer
cette probabilité et donc, en prenant la valeur minimale quand θ décrit Θ,
de connaı̂tre le niveau de conﬁance exact associé à la procédure consistant
à prendre le rectangle au croisement de deux intervalles. Toutefois montrons
que l’on peut donner une borne inférieure pour cette probabilité. Pour ce faire,
posons α = 1−γ qui correspond au risque d’erreur de la procédure pour chaque
composante.
Soit E1 et E2 deux événements quelconques. Le complémentaire de E1 ∩ E2
est E 1 ∪ E 2 . Par ailleurs (voir section 1.1) :
P (E 1 ∪ E 2 ) = P (E 1 ) + P (E 2 ) − P (E 1 ∩ E 2 ) ≤ P (E 1 ) + P E 2 ).
D’où l’inégalité générale :
P (E1 ∩ E2 ) ≥ 1 − P E 1 ) + P (E 2 ) .
Appliquant celle-ci aux événements (θ1 ∈ I1 ) et (θ2 ∈ I2 ) on en déduit :
Pθ ((θ1 ∈ I1 ) ∩ (θ2 ∈ I2 )) ≥ 1 − 2α.
Ainsi, si l’on vise un niveau de conﬁance 1 − α, on peut le garantir en prenant
pour chaque composante un niveau de conﬁance 1 − α2 . (Pour le niveau courant
de 0,95 on utilisera les IC de niveau 0,975 sur chaque composante). Pour une
dimension k quelconque la méthode ci-dessus s’applique en prenant un niveau
1 − αk sur chaque composante.
Toutefois cette procédure peut s’avérer très conservatrice, au sens où le niveau réel sera supérieur et la région du plan sera donc plus vaste que nécessaire.
Il n’est donc pas inutile de rechercher de façon directe une région de niveau γ.
Nous illustrons une approche exacte pour le paramètre (μ, σ 2 ) de la loi N (μ , σ 2 ).
Nous avons vu (proposition 5.3) que X̄ et S 2 sont indépendants, ce qui
2
X̄−μ)
√
et (n−1)S
le sont également. Ces deux dernières v.a. étant,
implique que (σ/
σ2
n
respectivement, de lois N (0 ; 1) et χ2 (n − 1) on a, quel que soit (μ, σ 2 ) :

X̄ − μ
√ < 2,24 = 0,975
σ/ n

(n − 1)S 2
2 (n−1)
<
<
χ
= 0,975
0,9875
σ2


P

P

2 (n−1)

χ0,0125

−2,24 <

et la probabilité que ces deux événements aient lieu simultanément est donc
(0, 975)2  0, 95. Ainsi une région de conﬁance de niveau 0,95 est obtenue en

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

161

prenant l’ensemble des points (μ, σ 2 ) du plan tels que :
⎧
x̄ − μ
⎪
√ < 2,24
⎨ −2,24 <
σ/ n
2
⎪
⎩ χ2 (n−1) < (n − 1)s < χ2 (n−1)
0,0125
0,9875
σ2
ou, de façon équivalente :
⎧
n
⎪
(μ − x̄)2 > 0
σ2 −
⎪
⎪
(2,24)2
⎨
(n − 1)s2 .
(n − 1)s2
⎪
⎪
< σ 2 < 2 (n−1)
⎪
⎩
2 (n−1)
χ0,9875
χ0,0125
La première inégalité correspond, en coordonnées (x, y), à l’intérieur de la parabole y = a(x − x̄)2 centrée sur x̄ où a vaut n/(2,24)2 . La seconde découpe une
2
tranche de cet intérieur entre les droites horizontales d’équations y = (n−1)s
2 (n−1)
et y =

(n−1)s2
2 (n−1)
χ0,0125

χ0,9875

comme indiqué sur la ﬁgure 7.2 .

2,24

(n − 1)
0,0125

Région de confiance à 95%
pour (μ, σ2)
(n − 1)
0,9875

Figure 7.2 - Région de conﬁance pour le paramètre (μ, σ 2 ) d’une loi de Gauss.

Signalons brièvement les éléments permettant d’obtenir des régions de conﬁance approximatives dans Rk à partir des propriétés√du vecteur estimateur du
MV. Nous avons indiqué en ﬁn de section 6.7.4 que n(θ̂M V − θ) converge en

162

Statistique − La théorie et ses applications

loi vers une loi normale à k dimensions de vecteur des moyennes nul et de matrice des variances-covariances [I(θ))]−1 . Comme la matrice I(θ) est symétrique
et déﬁnie strictement positive il existe une matrice symétrique et déﬁnie stric1
tement positive dont le carré est égal à I(θ) et nous la désignons par I(θ) 2 .
1
1
Ceci est également applicable à [I(θ)]−1 et I(θ)− 2 est l’inverse de I(θ) 2 , i.e.
1
1
I(θ) 2 I(θ)− 2 = Ik , la matrice identité d’ordre k.
√
1
Posons X = n(θ̂M V − θ) et soit Y = I(θ) 2 X. Selon la proposition 3.13,
1
E(Y ) = I(θ) 2 E(X) = 0 et :
1

1

1

1

V(Y ) = I(θ) 2 V(X) I(θ) 2 = I(θ) 2 I(θ)−1 I(θ) 2 = Ik .
√
1
Ainsi n I(θ) 2 (θ̂M V − θ) a une loi asymptotique N (0 , Ik ), c’est-à-dire que
toutes les composantes de ce vecteur aléatoire tendent à être indépendantes et
de loi N (0 ; 1) (voir les développements analogues en section 3.9). Par conséquent la somme des carrés de ses composantes, égale à :
,t +√
,
+√
1
1
n I(θ) 2 (θ̂ M V − θ)
n I(θ) 2 (θ̂M V − θ) = n(θ̂M V − θ)t I(θ) (θ̂M V − θ) ,
suit approximativement une loi du khi-deux à k degrés de liberté. En remplaçant, en deuxième approximation, I(θ) par I(θ̂M V ) et en passant à la réalisation
de θ̂ M V , l’inéquation en θ :
n(θ̂ M V − θ)t I(θ̂M V ) (θ̂M V − θ) ≤ χ20,95 (k)
déﬁnit l’intérieur d’un ellipsoı̈de centré sur θ̂ M V qui est une région de conﬁance
de niveau approximatif 0,95.
Appliquant ceci à (μ, σ2 ) dans le cas gaussien on a (voir exemple 6.18) :
 1
 2


σ
0
0
2
2
2 −1
σ
I(μ, σ ) =
et [I(μ, σ )] =
.
0 2σ 4
0 2σ1 4
D’où la région de conﬁance au niveau 0,95 (où ici la substitution I(x̄, s̃2 ) pour
I(μ, σ 2 ) n’est pas nécessaire) :
n
n
(μ − x̄)2 + 4 (σ 2 − s̃2 )2 ≤ χ20,95 (2) = 5,99
σ2
2σ
qui correspond à l’intérieur d’une ellipse centrée sur l’estimation du MV :
(x̄, s̃2 ). En fait, on montre que cette région est plus intéressante que celle obtenue plus haut car elle est (en espérance mathématique, et pour n pas trop
petit) de surface inférieure.
Grâce à la résolution numérique vue en section 7.3 permettant d’accéder
à I(θ̂ M V ), les logiciels peuvent, pour k = 2, tracer les ellipses contenant le
paramètre à un niveau de conﬁance donné.

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

7.9

163

Intervalles de conﬁance et tests

Nous verrons en section 9.8 qu’il y a une dualité entre une procédure d’IC
et une procédure de test. Comme il est généralement plus facile d’élaborer une
procédure de test, nous montrerons comment, à partir de celle-ci, construire
un intervalle de conﬁance. Cela permettra de couvrir des situations encore plus
diverses que celles envisageables par les approches directes du présent chapitre.
Pour approfondir la théorie des intervalles de conﬁance (et, plus généralement, la théorie de l’estimation et des tests) on pourra consulter le livre de Cox
et Hinkley (1979) ou celui de Shao (1999).

7.10

Exercices

Exercice 7.1 Pour la loi de Gauss N (μ, σ 2 ) où (μ, σ 2 ) est inconnu comparer
la largeur de l’IC obtenu pour μ à celle de l’IC obtenu pour σ quand n est
grand.
Aide : pour σ on partira de la formule de la note 7.5 et on utilisera l’approximation de la loi du χ2 par une loi de Gauss.
Exercice 7.2 Soit la loi de Gauss N (μ, σ 2 ) où μ est connu – on prendra μ = 0
– et σ 2 inconnu. Donner un IC pour σ 2 .

X2
Aide : montrer que ni=1 σ2i est une fonction pivot.
Exercice 7.3 (méthode des quantiles) Donner un IC à 95% pour θ de la
loi U[0, θ] en utilisant la loi de la statistique exhaustive minimale X(n) .
Exercice 7.4 (méthode des quantiles) * Soit Fχ2 (x; 2n) la fonction de répartition de la loi du khi-deux à 2n degrés de liberté. Montrer que :
Fχ2 (x; 2n) = 1 −

n−1

k=0

e− 2 ( x2 )k
.
k!
x

Aide : on calculera l’intégrale de la densité en intégrant par parties et en
exploitant la relation de récurrence obtenue.
Soit FP (x; λ) la fonction de répartition de la loi de Poisson P(λ). Montrer
que FP (x; λ) = 1 − Fχ2 (2λ; 2x + 2) et que, grâce à cette relation, on peut
résoudre immédiatement les deux équations de la méthode des quantiles pour
la loi de Poisson, à l’aide d’une table du khi-deux. Consulter une telle table
pour vériﬁer le résultat de l’exemple 7.6.
Exercice 7.5 (méthode des quantiles) Soit un
néchantillon de la loi de
Bernoulli de taille 20 pour lequel on a observé
i=1 Xi = 8 (soit 8 succès
au cours de 20 répétitions). Montrer, en résolvant les équations de la méthode

164

Statistique − La théorie et ses applications

des quantiles au moyen d’un logiciel mathématique, qu’on obtient un IC à 95%
égal à [0,19 ; 0,64]. Comparer à l’IC de la formule asymptotique.
Exercice 7.6 Soit la loi mère N (μ , σ 2 ) où μ est inconnu mais σ 2 est connu.
Dans une approche bayésienne on se donne une loi a priori N (μ0 , σ02 ) pour μ.
Montrer que la loi a posteriori de μ est gaussienne de moyenne :
σ02 x̄ + σ 2 μ0 /n
σ02 + σ 2 /n
et de variance :

σ02 σ 2 /n
+ σ 2 /n

σ02

où x̄ est la moyenne observée sur un échantillon de taille n. En déduire un intervalle de probabilité de niveau 0,95 et le comparer à l’IC classique avec σ2 connu
(voir ﬁn de section 7.4.1). Montrer que les deux intervalles sont équivalents
quand n → ∞.
Exercice 7.7 Montrer que la procédure d’IC proposée en section 7.4.5 pour
le paramètre p d’une loi de Bernoulli est convergente en probabilité.

Exercices appliqués1
Exercice 7.8 On veut estimer le rendement d’un engrais pour la culture du
blé. Sur douze parcelles expérimentales, on a trouvé les rendements suivants en
tonnes par hectare :
7.7 8.4 7.8 8.2 7.9 8.5 8.4 8.2 7.6 7.8 8.4 8.3
Donner un intervalle de conﬁance à 95% pour le rendement moyen de l’engrais (on supposera que le rendement à l’hectare est une v.a. gaussienne).
Exercice 7.9 Un hôpital souhaite estimer le coût moyen d’un patient, sachant
que le coût par jour est de 200 euros. Pour un échantillon aléatoire de 500
patients on a observé une durée de séjour moyenne de 5,4 jours avec un écarttype de 3,1 jours. Donner un intervalle de conﬁance à 90 % pour la durée
moyenne de séjour d’un patient et en déduire un intervalle pour le coût moyen
d’un patient.
Exercice 7.10 Une société d’assurance doit évaluer, en ﬁn d’année, la provision à faire au bilan pour les sinistres en cours n’ayant pas encore fait l’objet
d’un règlement. Elle sélectionne au hasard 200 dossiers qui sont évalués en
moyenne à 9 944 euros, l’écart-type des valeurs étant égal à 1 901 euros. Sachant que 11 210 dossiers sont en cours, donner un intervalle de conﬁance sur
la provision totale à eﬀectuer.
1 Un ou deux de ces exercices appliqués sont des emprunts dont nous avons perdu la source.
Nous nous en excusons auprès des involontaires contributeurs.

Chapitre 7. Estimation paramétrique par intervalle de conﬁance

165

Exercice 7.11 Pour évaluer le nombre de mots d’un livre on tire 20 pages au
hasard et on y compte le nombre de mots. On trouve, pour les 20 valeurs, une
moyenne de 614 mots et un écart-type de 26 mots. Donner un intervalle de
conﬁance à 95 % pour le nombre total de mots du livre sachant qu’il a 158
pages (on admettra que l’approximation gaussienne est satisfaisante).
Exercice 7.12 On souhaite évaluer le gain de consommation obtenu avec un
nouveau carburant pour automobile. Un test en laboratoire est eﬀectué sur 20
moteurs du même type. Dix moteurs sont alimentés en carburant traditionnel
et donnent sur une durée donnée une consommation moyenne de 10,8 litres avec
un écart-type de 0,21 litre. Pour les dix autres moteurs le nouveau carburant
est utilisé et l’on observe une consommation moyenne de 10,3 litres avec un
écart-type de 0,18 litre. Donner un intervalle de conﬁance à 90 % sur le gain
moyen procuré par le nouveau carburant (on supposera que les approximations
gaussiennes sont satisfaisantes).
Exercice 7.13 Un stock comporte 10 000 pièces. Pour évaluer le nombre de
pièces défectueuses dans le stock on tire au hasard 400 pièces dont on constate
que 45 sont défectueuses.
Donner un intervalle de conﬁance à 99 % pour le nombre total de pièces
défectueuses.
Exercice 7.14 Un sondage auprès de 1 500 ménages tirés au hasard dans
la population française a indiqué que 20 % de ceux-ci prévoient d’acheter
une nouvelle voiture dans les douze prochains mois. Estimer par un intervalle
de conﬁance à 95 % le pourcentage de ménages de la population française
prévoyant d’acheter une nouvelle voiture dans les douze mois.
Exercice 7.15 On veut évaluer la diﬀérence des proportions de pièces défectueuses dans deux procédés de fabrication diﬀérents. Pour cela on tire au hasard
1 000 pièces réalisées selon le premier procédé. Les ayant testées on en a trouvé
86 défectueuses.
On opère de même pour 800 pièces réalisées selon le deuxième procédé et
on en trouve 92 défectueuses.
Donner un intervalle de conﬁance sur la diﬀérence des proportions de pièces
défectueuses dans les deux procédés.
Exercice 7.16 Dans une ville on donne la répartition du nombre de jours
sans accident, avec un accident, etc. parmi 50 jours d’observation au cours
d’une même année :
Nbre accidents
Nbre jours

0
21

1
18

2
7

3
3

4
1

On suppose que le nombre d’accidents par jour suit une loi de Poisson.
Donner un intervalle de conﬁance de niveau 0,95 pour le nombre moyen
d’accidents par jour (on utilisera une approximation asymptotique).

166

Statistique − La théorie et ses applications

Exercice 7.17 Dix bouteilles d’eau minérale provenant d’une source donnée
sont analysées. On relève les taux de nitrates suivants, en mg/l :
3,61 3,56 3,67 3,56 3,64 3,62 3,44 3,52 3,55 3,52
Donner un intervalle de conﬁance à 95% pour l’écart-type du taux de nitrates dans les bouteilles produites (on supposera ce taux gaussien).

Chapitre 8

Estimation non
paramétrique et estimation
fonctionnelle
8.1

Introduction

Nous considérons maintenant que la loi mère ne fait pas partie d’une famille
paramétrable de lois, c’est-à-dire que nos connaissances sur la nature de cette loi
sont beaucoup plus ﬂoues, ce qui correspond d’ailleurs souvent plus à la réalité,
notamment lorsqu’il s’agit d’un sondage dans une population. Tout au plus
ferons-nous ici ou là l’hypothèse que sa fonction de répartition, ou sa densité
(cas continu), ou sa fonction de probabilité (cas discret) répond à des conditions
de régularité, principalement la dérivabilité et l’existence de moments jusqu’à
un certain ordre.
Il ne peut donc plus s’agir ici d’estimer un paramètre qui déterminerait
totalement la loi et par suite toute caractéristique de celle-ci. Dès lors deux
orientations sont possibles. Soit on s’intéresse uniquement à quelques valeurs
caractéristiques de la loi (ou de la population dans la situation de sondage) :
moyenne, variance ou écart-type, médiane ou tout autre quantile, et dans ce cas
nous sommes dans un contexte d’estimation non paramétrique ponctuelle. Soit,
ce qui est nouveau par rapport à l’estimation paramétrique, on veut estimer
la loi dans sa globalité par sa fonction de répartition ou sa densité, ou sa
fonction de probabilité (quoique, on le verra, le cas discret soit peu concerné)
et l’on parle alors d’estimation fonctionnelle. Pour illustrer cette deuxième
orientation, disons déjà que l’histogramme utilisé en statistique descriptive est
une façon rudimentaire d’approcher la densité de la loi, dont nous étudierons
d’ailleurs l’eﬃcacité.

Statistique − La théorie et ses applications

168

Nous commençons par l’estimation ponctuelle en nous bornant aux caractéristiques mentionnées ci-dessus. Nous reprendrons certains résultats des
chapitres précédents dont nous avons pu dire qu’ils étaient en fait de portée
générale et non pas limitée au cadre paramétrique. Dans la mesure du possible
nous traiterons simultanément le problème de la construction d’un intervalle
de conﬁance.
Dans ce chapitre, comme précédemment, la loi mère sera symbolisée par la
v.a. générique X, sa fonction de répartition par F, sa fonction de densité ou de
probabilité par f .

8.2
8.2.1

Estimation de la moyenne et de la variance
de la loi
Estimation de la moyenne μ

Nous avons vu en section 6.5.1 que les moments empiriques simples (s’ils
existent) sont des estimateurs sans biais des moments correspondants de la
loi mère et ceci quelle que soit la nature de cette loi. En section 6.5.3 on a
vu encore qu’en conséquence de la loi des grands nombres, ces estimateurs
sont convergents presque sûrement. Pour que le moment d’ordre k converge il
suﬃt que E(|X k |) existe (voir
6.1). Cela s’applique évidemment à
proposition
n
la moyenne empirique X = i=1 Xi . Notons que, pour un sondage dans une
population ﬁnie, ces conditions d’existence des moments sont nécessairement
réunies, ceux-ci étant des caractéristiques descriptives de la population dans son
ensemble. Pour X nous avions directement établi en section 5.2 (proposition
5.1) que (si σ 2 existe) :
E(X) = μ

et

V (X) =

σ2
,
n

la première relation reﬂétant un biais nul et la deuxième montrant directement
la convergence en moyenne quadratique.
En conclusion nous utiliserons naturellement la moyenne empirique pour
estimer la moyenne de la loi, nous satisfaisant de ces propriétés. Il n’est pas
possible de dire si tel est le meilleur choix possible, sauf à imposer des conditions restrictives sur la nature de la loi mère, ce qui n’est pas dans l’esprit de
l’estimation non paramétrique.
En section 7.4.1 nous avons noté que, dès lors que n est assez grand, on a :
X −μ
√
S/ n

;

approx

t(n − 1)

en vertu du théorème central limite et de la convergence de S 2 vers σ 2 . Nous
en avons déduit que l’intervalle de conﬁance classique (à 95 %) propre au cas

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

169

d’une loi mère gaussienne :
(n−1) s
(n−1) s
IC0,95 (μ) = [x − t0,975 √ , x + t0,975 √ ]
n
n

fournit une bonne approximation pour une loi quelconque.
Inﬂuence de valeurs extrêmes ou aberrantes
Il a été dit qu’en principe n ≥ 30 suﬃt. Cependant l’approximation par une
loi de Student posera problème pour des v.a. dont les queues de distribution
sont allongées et peuvent produire des observations très éloignées du centre. Si
l’on étudie, par exemple, le niveau maximum annuel de crue d’une rivière sur les
cent dernières années celui-ci reste en général assez semblable mais on trouvera
quelques cas de valeurs exceptionnelles. Une valeur très excentrée va inﬂuencer
fortement la valeur de x et encore plus, car interviennent des écarts au carré,
celle de la variance s2 , rendant ainsi ces statistiques trop instables pour garantir l’approximation par une loi de Student si la taille de l’échantillon n’est
pas très élevée. Pour les mêmes raisons, si les observations sont contaminées
par des valeurs aberrantes l’approximation sera défaillante. Ceci peut provenir,
par exemple, d’erreurs dans le recueil des informations ou de présences de valeurs étrangères au phénomène étudié (dans les sondages, présence d’individus
distincts n’appartenant pas à la population). Si l’on soupçonne la présence de
valeurs très extrêmes ou aberrantes on peut soit éliminer purement et simplement les valeurs trop éloignées par examen de la distribution des observations
(histogramme), soit réduire leurs poids dans le calcul de la moyenne et de la
variance. On déﬁnit ainsi des M-estimateurs dont l’étude des propriétés fait
l’objet de la théorie de la robustesse. En particulier la moyenne α-tronquée,
appropriée si la distribution est à peu près symétrique, est un M-estimateur
facile à mettre en oeuvre : elle consiste à rejeter un pourcentage d’observations
égal à 100( α2 ) sur chaque extrémité. On peut également renoncer à la moyenne
comme valeur caractéristique de position centrale de la distribution et préférer
la médiane qui ne présente pas les mêmes inconvénients.

8.2.2

Estimation de la variance σ2

On privilégiera l’estimateur S 2 dont on sait qu’il est sans biais et convergent
(voir section 6.5.3). Pour ce qui concerne un intervalle de conﬁance la procédure
classique obtenue dans le cas gaussien (voir section 7.4.2) ne peut être utilisée
car (n − 1)S 2 /σ 2 ne suit plus une loi χ2 (n − 1) dès que l’on s’écarte de cette
hypothèse, y compris quand n tend vers l’inﬁni. On a vu toutefois (note 5.5) que
le théorème central limite s’applique à S 2 , moyennant l’existence du moment
d’ordre 4, et l’on peut donc établir la loi asymptotique de S 2 dans le cas général
sachant que (voir exercice 5.4) :
V (S 2 ) =

1 
n−3 4
(μ −
σ ),
n 4 n−1

Statistique − La théorie et ses applications

170

où μ4 est le moment centré d’ordre 4 de la loi mère. Comme V (S 2 ) est asymptotiquement équivalent à n1 (μ4 − σ 4 ) on a, après centrage et réduction, le résultat
suivant :
√
n(S 2 − σ 2 )
% 
; N (0 ; 1) .
μ4 − σ 4 approx
En écrivant cette statistique centrée et réduite sous la forme :
√
n(S 2 − σ 2 )
%
σ 2 μ4 /σ 4 − 1
on fait apparaı̂tre le coeﬃcient de curtose μ4 /σ 4 qui vaut 3 pour la loi de
Gauss, est supérieur à 3 pour une loi à pic plus prononcé au mode et queues
plus allongées, est inférieur à 3 pour une loi à pic plus plat et
% queues courtes.
Dans le cas gaussien cette expression est bien, au facteur (n − 1)/n près,
la version centrée et réduite de (n − 1)S 2 /σ 2 puisque la loi χ2 (n − 1) est de
moyenne n − 1 et de variance 2(n − 1). Par ailleurs on a vu (voir la remarque
en section 5.8) que la loi du khi-deux tend à devenir gaussienne quand n tend
vers l’inﬁni.
Pour construire un intervalle de conﬁance asymptotique, on peut envisager
d’estimer μ4 − σ 4 par sa version empirique M4 − S 4 (ou le coeﬃcient de curtose
par la curtose empirique M4 /S 4 ), mais la convergence est lente et un nombre
important d’observations sera nécessaire pour espérer une bonne approximation. On peut recourir à une approche dite par rééchantillonnage dont l’intérêt
est général et c’est pourquoi nous y consacrons une section spéciﬁque. Cette
approche sera également appropriée pour l’estimation de l’écart-type.

8.3

Estimation d’un quantile

Nous supposerons que la v.a. soit continue pour que tout quantile existe.
Pour qu’il y ait unicité nous supposerons aussi que le support de la densité f
soit un intervalle [a, b], où éventuellement a = −∞ et/ou b = +∞, de façon
que F soit strictement croissante sur l’ensemble des valeurs de x telles que
0 < F (x) < 1 (voir section 1.5). Nous considérerons essentiellement la médiane,
les développements étant similaires pour un quantile quelconque.
' la médiane empirique déﬁnie par :
Notons μ
' la médiane de la loi mère et X
"
si n = 2m − 1
X(m)
'
,
X=
1
(X(m) + X(m+1) ) si n = 2m
2
' est l’estimateur naoù X(m) est la statistique d’ordre m (voir section 5.6). X
turel de μ
'. Dans la proposition 5.12 nous avons donné la loi d’une statistique
d’ordre quelconque, ce qui s’applique directement au cas de n impair. Pour n

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

171

' n’est pas nécessairement sans biais et sa variance n’a pas d’expression
ﬁni X
simple. On doit se contenter de la propriété asymptotique suivante, que nous
admettrons.
Proposition 8.1 Soit une loi continue de densité f et de médiane μ
', et soit
'n la médiane d’un n-échantillon X1 , X2 , · · · , Xn issu de cette loi. On a :
X
' −→
nV (X)

n→∞

et

1
4[f ('
μ)]2

√
L
' −μ
2f ('
μ) n(X
') −→ N (0 ; 1) .
n→∞

La médiane empirique est donc asymptotiquement sans biais et converge en
moyenne quadratique vers μ
' puisque sa variance tend vers 0 (on peut aussi
montrer qu’elle converge presque sûrement). Étant donné que f ('
μ) est inconnu, il faudrait une estimation de cette valeur pour construire un intervalle
de conﬁance approximatif. On dispose cependant d’une approche directe pour
n ﬁni.
' le nombre d’observations inférieures ou égales à μ
Soit N
'. Pour chaque Xi
' ; B(n, 1 ). On a donc :
la probabilité d’être inférieur ou égal à μ
' vaut 12 , et N
2
' ≤ l2 ) =
P (l1 ≤ N

l2  

n 1
.
k 2n

k=l1

Choisissons l1 et l2 tels que, d’une part, la probabilité ci-dessus soit supérieure
ou égale à 0,95 et au plus proche de cette valeur et, d’autre part, que l2
soit égal à n− l1 ou le plus proche possible pour avoir l’intervalle [l1 , l2 ] le
plus symétrique possible par rapport à n/2 et donc le plus étroit. Notons que
') signiﬁe qu’il y a au moins l1 observations inférieures
l’événement (X(l1 ) ≤ μ
' ) et, de même, l’événement
ou égales à μ
' et il est donc identique à (l1 ≤ N
'
('
μ < X(l2 +1) ) est identique à l’événement (N ≤ l2 ). La probabilité ci-dessus
est donc égale à P (X(l1 ) ≤ μ
' < X(l2 +1) ) et ceci quel que soit μ
'. Pour un
échantillon réalisé x1 , x2 , . . . , xn , ceci fournit donc un intervalle de conﬁance à
95 % pour μ
' :
IC0,95 ('
μ) = [x(l1 ) , x(l2 +1) [ .
Pratiquement on peut fermer l’intervalle à droite, les statistiques d’ordre étant
des v.a. continues.
' ; B(20;0,5) : P (N
' ≤ 5) = 0,021
Exemple 8.1 Soit n = 20. On a pour N
'
'
(mais P (N ≤ 6) = 0,58) et par symétrie P (N ≥ 15) = 0,021 d’où
' ≤ 14) = 0,958 et IC0,95 ('
μ) = [x(6) , x(15) ] .
P (6 ≤ N
On pourra utiliser assez vite l’approximation gaussienne du fait que p = 1/2
(le critère np ≥ 5 et n(1 − p) ≥ 5 de la section 5.8.3 revenant à n ≥ 10). Avec
' ≤ 5).
correction de continuité on obtient ici 0,022 pour P (N


172

Statistique − La théorie et ses applications

Pour le quantile xq d ordre q considérons X[nq]+1 où [nq] est la partie entière
de nq. La proposition s’applique à X[nq]+1 avec :
nV (X[nq]+1 ) −→

n→∞

et

%

f (xq )
q(1 − q)

q(1 − q)
[f (xq )]2

√
L
n(X[nq]+1 − xq ) −→ N (0 ; 1) .
n→∞

L’intervalle de conﬁance pour xq est obtenu de façon analogue à partir de la
loi B(n, q).
Note 8.1 De la proposition 8.1 on peut déduire la précision relative asymptotique de la médiane empirique par rapport à la moyenne√empirique dans le
' est
cas gaussien, par exemple. Pour cette loi f ('
μ) = f (μ) = 1/ 2πσ et V (X)
π σ2
donc équivalent à 2 n quand n tend vers l’inﬁni. La variance asymptotique de
la médiane est donc π/2 fois plus grande.

8.4
8.4.1

Les méthodes de rééchantillonnage
Introduction

Ces méthodes ont pour principe de simuler la variabilité des estimateurs
en tirant des échantillons à l’intérieur de l’échantillon recueilli. La méthode
du jackknife (littéralement couteau de poche ou canif) est la plus ancienne
et eﬀectue des tirages déterministes. La méthode du bootstrap (chausse-pied)
constitue une généralisation du jackknife qui n’a pu être conçue que dès lors que
de puissants moyens informatiques étaient disponibles. Du fait qu’elle eﬀectue
des tirages au hasard elle est de portée beaucoup plus générale. Le jackknife a
été développé initialement par M. Quenouille et J. Tukey dans les années 1950
pour réduire le biais d’un estimateur donné, puis a été envisagé pour obtenir des
intervalles de conﬁance. Le bootstrap a été proposé par Efron (1979). Notons
que ces méthodes s’appliquent aussi bien dans le cas discret que dans le cas
continu.
Les estimateurs non paramétriques du maximum de vraisemblance
Ces estimateurs sont les estimateurs de référence du jackknife comme du
bootstrap et il est utile de les expliciter. Pour une caractéristique ω de la loi
déﬁnie par sa fonction de répartition F (en bref nous dirons la loi F ) son estimateur du maximum de vraisemblance ω
 est obtenu par sa version empirique,
c’est-à-dire la caractéristique de même nature calculée sur l’échantillon : la
moyenne de la loi est estimée par la moyenne de l’échantillon X, la variance
par S'2 , la médiane de la loi par la médiane de l’échantillon, etc. Ceci découle
du fait que la fonction de répartition empirique Fn (déﬁnie en section 5.7)
est l’estimateur fonctionnel (i.e. une fonction pour estimer une fonction) du
maximum de vraisemblance pour F (voir plus loin, section 8.5.3).

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

173

Notons que le passage d’une caractéristique théorique à la caractéristique
empirique correspondante est immédiat dans le cas d’une loi discrète (et en
particulier dans le cas d’un tirage au hasard dans une population ﬁnie) car il
suﬃt d’appliquer à l’échantillon la même formule de déﬁnition que celle de la
caractéristique. Si la loi est continue et que la caractéristique est propre à ce
type de loi, le passage peut être plus délicat du fait que Fn n’est pas continue.
C’est par exemple le cas d’un mode de la loi (maximum de la dérivée de F ) qui
n’a pas d’équivalent direct sur l’échantillon. On pourra alors utiliser un lissage
de Fn en continu du type de celui présenté en section 8.5.3.
Note 8.2 Puisque la loi est parfaitement déﬁnie à partir de F, une caractéristique ω
de la loi F peut s’exprimer comme une application ω(F )qui à une fonction fait correspondre un réel. On dira que ω est un opérateur fonctionnel (en bref une fonctionnelle).
μ est égale à l’intégrale de Riemann-Stieltjes déﬁnie dans la
Par exemple la moyenne

note 2.1 : μ(F ) = R xdF (x) et la moyenne empirique est la statistique fonctionnelle
correspondante
obtenue en remplaçant F par la fonction de répartition empirique

Fn : x = R xdFn (x) (voir note 5.4). Plus généralement, ce que nous avons appelé la
version empirique de ω(F ) est ω(Fn ). Comme la fonction de répartition empirique
Fn est l’estimation du maximum de vraisemblance de F (voir section 8.5.3), par voie
de conséquence ω(Fn ), en tant que fonction de Fn , est l’estimation du maximum de
vraisemblance de ω(F ).

8.4.2

La méthode du jackknife

Nous illustrons d’abord le principe du jackknife pour l’estimation de l’écarttype σ de la loi. Soit l’estimateur de référence :

S=

√

1
2
2
2
S =3

1 
(Xi − X)2
n − 1 i=1
n

qui est généralement biaisé (voir exercice 6.4 pour le cas de la loi de Gauss)
et soit s l’estimation correspondante pour une réalisation x1 , x2 , · · · , xn de
l’échantillon. L’estimation du jackknife est obtenue de la façon suivante.
On calcule la valeur, notée s−1 , de l’écart-type du sous-échantillon obtenu
en omettant la valeur x1 :

s−1

1
2
2
=3

1 
(xi − x)2 ,
n − 2 i=2
n

puis la valeur s∗1 = ns − (n − 1)s−1 . On répète cette opération en omettant à tour de rôle chacune des observations pour obtenir n pseudo-valeurs

174

Statistique − La théorie et ses applications

s∗1 , s∗2 , · · · , s∗n avec, donc :
s−i

1
2
2
=3

1
n−2

n


(xj − x)2

j=1,j =i

s∗i = ns − (n − 1)s−i .
L’estimation du jackknife est alors la moyenne des pseudo-valeurs, notée s∗ .
Un intervalle de conﬁance approché peut être obtenu en appliquant à la série
des n pseudo-valeurs le résultat de la section 6.4.1 concernant la moyenne d’un
échantillon aléatoire gaussien. Ainsi on calcule la variance des pseudo-valeurs :
1 
(s∗i − s∗ )2 ,
n − 1 i=1
n

s2JK =
d’où :

(n−1) sJK
(n−1) sJK
IC0,95 (σ)  [s∗ − t0,975 √ , s∗ + t0,975 √ ] .
n
n

De façon générale soit ω une caractéristique de la loi et Tn un estimateur
convergent de ω, typiquement l’estimateur du maximum de vraisemblance. Soit
Tn−i l’estimateur calculé en omettant Xi . On déﬁnit les pseudo-valeurs :
Tn∗i = nTn − (n − 1)Tn−i , i = 1, . . . , n.

L’estimateur du jackknife fondé sur Tn est alors Tn∗ = n1 ni=1 Tn∗i .
Comme il a été dit en introduction cet estimateur a été proposé à l’origine
pour réduire le biais éventuel de Tn , en vertu du résultat suivant.
Proposition 8.2 Si le biais de Tn est de la forme nc , où c est une constante,
alors Tn∗ , l’estimateur du jackknife fondé sur Tn , est sans biais.
c
En eﬀet, comme E(Tn ) = ω + nc , on a, pour tout i, E(Tn−i ) = ω + n−1
−i
puisque Tn est le même estimateur appliqué au (n − 1)−échantillon aléatoire
X1 , . . . , Xi−1 , Xi+1 , . . . , Xn . Ainsi :

E(Tn∗i ) = nE(Tn ) − (n − 1)E(Tn−i )
c
]
= nω + c − (n − 1)[ω +
n−1
=ω,
d’où E(Tn∗ ) =

1
n

n
i=1

E(Tn∗i ) = ω.

Si le biais est de la forme cn1 + nc22 + nc33 + · · · on montre aisément de la même
façon que le premier terme disparaı̂t dans le biais de Tn∗ . Par conséquent, au

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

175

moins pour des situations de ce type, il y a réduction de biais. Si l’on applique,
par exemple, la procédure du jackknife à la variance empirique S'n2 dont le biais
pour estimer σ 2 est − n1 σ 2 (voir proposition 5.2), on trouve que l’estimateur du
jackknife est la variance de l’échantillon Sn2 qui est sans biais (voir exercices).
Notons incidemment que pour l’estimateur X n de la moyenne μ qui est sans
biais, l’estimateur du jackknife est X n lui-même.
En général, la loi étant totalement inconnue, on ne connaı̂t pas la forme
du biais (comme par exemple pour l’écart-type S dans l’illustration ci-dessus),
mais on s’attend à ce qu’il soit de toute façon réduit par la procédure décrite.
Outre la réduction du biais, l’intérêt du jackknife, primordial ici, est de
permettre l’estimation de l’écart-type de Tn et la possibilité de construire un
intervalle de conﬁance approché. La proposition qui suit va nous y conduire.
Proposition 8.3 Soit Tn∗ l’estimateur du jackknife de la caractéristique ω,
2
la variance des pseudoreposant sur un estimateur convergent Tn , et soit Sn,JK
valeurs. Alors, sous certaines conditions concernant la forme de la statistique :
Tn∗ − ω
L
√
−→ N (0 ; 1) .
Sn,JK / n n→∞
Nous admettrons cette proposition. Elle résulte du fait que les pseudovaleurs tendent à être indépendantes et gaussiennes pour une grande variété de
statistiques. En appliquant l’intervalle de conﬁance de la section 7.4.1 pour la
moyenne de v.a. i.i.d. gaussiennes, on déduit l’intervalle de conﬁance approché
pour ω :
(n−1) sn,JK
(n−1) sn,JK
IC0,975 (ω)  [t∗n − t0,975 √
, t∗n + t0,975 √ ] ,
n
n
où t∗n et sn,JK sont les réalisations respectives de Tn∗ et de Sn,JK .
Ceci s’applique, en particulier, à l’écart-type comme nous l’avons vu plus
haut (voir une application dans les exercices) et également pour estimer la
variance σ 2 . Dans ce dernier cas, en prenant l’estimateur du jackknife reposant
sur la variance empirique S'2 , on établit que la variance des pseudo-valeurs
2
Sn,JK
est égale à :
n3
(M4 − S'4 ) ,
(n − 1)(n − 2)2
ce qui conduit à une procédure d’intervalle de conﬁance très proche (et asymptotiquement équivalente) de l’approche asymptotique proposée en section 8.2.2.
En eﬀet, dans cette approche, on trouvait simplement M4 − S 4 en lieu et place
de l’expression ci-dessus.
Les conditions de validité de laproposition ne sont pas simples à expliciter.
n
Si la statistique est de la forme n1 i=1 g(Xi ) où g est une fonction quelconque,

176

Statistique − La théorie et ses applications

alors la proposition est vériﬁée. C’est le cas de tous les moments simples. Si
la forme est proche cela reste vrai, comme par exemple pour les moments
centrés, en particulier pour la variance empirique, et aussi pour l’écart-type.
En revanche, la médiane qui, dans sa version théorique, s’exprime par F −1 ( 12 )
a une forme très éloignée. Le jackknife est alors inadapté car Sn,JK ne converge
pas vers la valeur de l’écart-type de la médiane. Il en va de même pour d’autres
statistiques fonctions des statistiques d’ordres : quantiles, étendue X(n) − X(1) ,
distance interquartiles.
Note 8.3 Pour préciser quelque peu le domaine de validité du jackknife exprimons
une caractéristique ω comme une expression fonctionnelle ω(F ). Une fonctionnelle
est dite linéaire si ω(a1 F1 + a2 F2 ) = a1 ω(F1 ) + a2 ω(F2 ). Dans ce cas on montre
que ω(F ) est de la forme :

ω(F ) =
g(x)dF (x) = EF (g(X)).
R

Pour la statistique
n correspondante ω(Fn ) du maximum de vraisemblance cela se
traduit par n1
i=1 g(Xi ). Ceci est évidemment le cas de la moyenne empirique et
de tout autre moment empirique non centré.
Un moment centré n’est pas strictement de cette forme. Par exemple la variance

EF ([X − EF (X)]2 ) est l’espérance d’une fonction qui dépend elle-même de F. La
condition pour que le jackknife soit opérant au niveau de la convergence de Sn,JK
est que la caractéristique, et donc la statistique du MV, soit une fonctionnelle linéaire
ou pouvant être raisonnablement approchée par une fonctionnelle linéaire. Ceci est
réalisable pour la variance empirique (qui est une fonctionnelle quadratique) mais pas
dans le cas de la médiane qui est trop fortement non linéaire.

Le jackknife peut être utilisé pour des couples (et des n-uplets) de v.a., par
exemple pour la corrélation entre deux variables, pour la moyenne du ratio de
deux variables. Il s’étend également à des situations autres que des échantillons
aléatoires simples. Par ailleurs, diﬀérentes variantes du jackknife initial ont été
proposées. En particulier, pour de très grands échantillons, il est pratiquement
aussi eﬃcace de l’appliquer en omettant non pas chaque observation mais des
groupes de k observations, ceci aﬁn d’accélérer les calculs. Dans le
√ cas de la
médiane le fait de grouper les observations avec k de l’ordre de n permet
même d’assurer la convergence selon la proposition 8.3 et donc d’appliquer
l’intervalle de conﬁance qui en découle.
Rien ne s’oppose à ce qu’on utilise cette méthode dans un cadre paramétrique pour des fonctions du paramètre complexes. Par exemple on pourra
estimer e−λ , la probabilité qu’il n’y ait aucune occurrence dans une unité de
temps pour une loi de Poisson, en se fondant sur l’estimateur du maximum de
vraisemblance e−X (ceci est à rapprocher de l’exemple 6.13).
En ce qui concerne l’approximation asymptotique de l’intervalle de conﬁance
issu du jackknife il est diﬃcile de savoir à partir de quelle taille d’échantillon

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

177

elle devient satisfaisante. Pour les petits échantillons le bootstrap oﬀre une
alternative plus sûre.

8.4.3

La méthode du bootstrap

Le bootstrap est une approche très générale pour des situations les plus
variées. Il est certes plus coûteux en calcul que le jackknife mais donne en
général des estimateurs de variance plus faible. On a pu le voir comme une
généralisation du jackknife ou, plus exactement, le jackknife a pu être considéré
comme une forme appauvrie du bootstrap. Au lieu de rééchantillonner au hasard un grand nombre de fois, comme la théorie montre qu’il convient de le faire
et comme le fait le bootstrap, le jackknife se contente de choisir des échantillons
bien déterminés en nombre limité à n.
Nous indiquerons sans démonstration1 comment le bootstrap permet d’estimer la variance (donc la précision) d’un estimateur, ce qui débouche sur la
construction d’un intervalle de conﬁance approché. Pour ne pas alourdir les
notations on passera indiﬀéremment d’une variable aléatoire à sa réalisation,
le contexte permettant de reconnaı̂tre l’une ou l’autre de ces entités.
Soit ω
 un estimateur d’une caractéristique ω de la loi mère, cette loi étant
quelconque, continue ou discrète, de fonction de répartition inconnue F. On
ω ) de ω
 ou, plutôt, à une estimation de cette vas’intéresse à la variance VF (
riance puisque F est inconnue. Pour aboutir à cette estimation on opère selon
les étapes suivantes :
1. soit x1 , x2 , . . . , xn l’échantillon réalisé. On eﬀectue n tirages au hasard
avec remise parmi les valeurs x1 , x2 , . . . , xn (on s’attend à des répétitions
car il est très improbable de tirer les n valeurs distinctes initiales). On
calcule l’estimation ω
 ∗ obtenue sur la base de ce nouvel échantillon.
2. on répète l’opération précédente M fois pour obtenir une série d’estima∗
2∗ , . . . , ω
M
.
tions ω
1∗ , ω
3. l’estimation de la variance propre à ω
 est fournie par la variance descriptive de cette série d’observations, i.e. :
1  ∗
∗
(
ωk − ω
 )2
M −1
M

ω) =
s2∗ (

k=1

∗

où ω
 désigne la moyenne de la série.
On montre que lorsque M tend vers l’inﬁni, l’estimateur issu de cette
procédure tend presque sûrement vers l’estimateur du maximum de vraisemω ). En pratique M = 100 fournit une approximation suﬃsante de
blance de VF (
cet EMV car l’écart sera alors négligeable par rapport à l’erreur d’estimation
du maximum de vraisemblance lui-même.
1 Pour de plus amples développements on pourra consulter les ouvrages de référence indiqués en ﬁn de section.

178

Statistique − La théorie et ses applications

Note 8.4 Dénotons VF (
ω ) comme une forme fonctionnelle σω2 (F ) pour faire apparaı̂tre le lien avec la loi mère de l’échantillon. Sauf dans les cas simples cette
fonctionnelle ne peut être explicitée ce qui n’a aucune importance ici. Prenons tou est X, l’estimateur «moyenne de
tefois comme illustration élémentaire le cas où ω
l’échantillon». On sait que sa variance est la variance de la loi mère divisée par n et
l’on peut donc expliciter le lien :
2
σX
(F ) =

1
n


R

(x − μ)2 dF (x)



où μ est en fait μ(F ) = R xdF (x). Si, comme dans ce cas, la forme fonctionnelle
σω2 (F ) est connue on peut l’estimer par sa version empirique σω2 (Fn ) obtenue en
2
remplaçant F par Fn , qui est l’estimateur du maximum de vraisemblance de σω
 (F )

'2 /n. Mais si ω
(voir note 8.2). Ainsi pour la moyenne, σ 2 (Fn ) devient S
 est, par
X
exemple, l’estimateur «médiane de l’échantillon» on ne connaı̂t pas l’expression de sa
variance. C’est alors que la procédure bootstrap devient particulièrement précieuse.

2
Si l’on connaissait F on pourrait estimer σω
 (F ) avec toute la précision souhaitée
par simulation : on sait générer des échantillons de taille n issus de la loi F (méthode
 et, pour un
de Monte-Carlo), sur chaque échantillon on calculerait la statistique ω
grand nombre d’échantillons ainsi générés, on approcherait la distribution réelle de
ω
 . En calculant, par exemple, la variance empirique des valeurs de ω
 ainsi générées
on obtiendrait une estimation de la vraie variance de ω
 , d’autant plus précise que le
nombre d’échantillons générés serait grand, en vertu de la loi des grands nombres. Le
2
processus est identique lorsqu’il s’agit d’estimer σω
 (Fn ) en considérant maintenant
l’échantillon réalisé x1 , x2 , . . . , xn comme une population en soi, la distribution de
celle-ci étant caractérisée par Fn . Si l’on parcourt l’univers de tous les échantillons
possibles (M → ∞) et que l’on examine comment varie la statistique ω
 en calculant
2
sa variance on obtient σω
 (Fn ). Par exemple la variance des moyennes des échantillons

'2 /n. Pour cette raison σ 2 (Fn ) est appelé estimation du bootsconvergera vers S
ω

 puisque la procédure de rééchantillonnage permet de se
trap de la variance de ω
rapprocher autant que l’on veut de l’EMV. Notons au passage que le jackknife ne
parcourt que n échantillons particuliers ce qui explique ses performances moindres.

Ayant estimé la variance de ω
 on peut obtenir un intervalle de conﬁance
approché pour ω en supposant que ω
 soit asymptotiquement sans biais et gaussien :
ω − 1, 96 s∗ (
ω) , ω
 + 1, 96 s∗ (
ω) ] .
IC0,95 (ω)  [
L’approximation gaussienne est fréquemment légitime, en particulier si ω
 est
lui-même une estimation du maximum de vraisemblance de ω, la normalité
asymptotique de l’EMV énoncée en proposition 6.11 dépassant le strict cadre
paramétrique. Mais celle-ci n’est pas assurée pour tous les types de statistiques
ou bien elle peut être trop lente pour fournir une approximation satisfaisante
au vu de la taille n de l’échantillon. En fait on peut contrôler l’hypothèse de

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

179

∗
normalité en examinant l’histogramme des valeurs ω
1∗ , ω
2∗ , . . . , ω
M
car il reﬂète
la distribution de ω
 (voir section 8.5.2).

On peut améliorer cette méthode «basique» par la méthode studentisée.
Cette méthode s’applique toutefois si ω
 tend à être gaussien quand n → ∞
avec une variance de forme équivalente à σω2 /n où σω2 est une constante (par
exemple si ω
 est EMV ce peut être 1/I(ω) ) et si l’on dispose d’un estimateur
convergent de σω2 que nous noterons simplement s2 . A l’étape 1 on calcule, en
plus de ω
 ∗ , la valeur s2∗ pour les n valeurs rééchantillonnées. L’étape 2 produit,
en plus de la série des ω
k∗ précédents, les s2∗
k correspondants. On calcule alors
∗
∗
la série des valeurs T1 , T2∗ , . . . , TM
déﬁnie par :
Tk∗ =

ω
k∗ − ω

√
s∗k / n

et l’on détermine les quantiles empiriques t∗0,025 et t∗0,975 de cette série (moyennant M grand, disons M  1000, pour évaluer avec suﬃsamment de précision
de petites probabilités sur les extrémités de la distribution). Alors l’IC approché
est :
s
s
IC0,95 (ω)  [
ω + t∗0,025 √ , ω
 + t∗0,975 √ ] .
n
n

La méthode des percentiles constitue une autre approche simple à mettre
en oeuvre et, de ce fait, assez répandue. Elle s’applique en prenant directement
∗
∗
et ω
0,975
d’ordres
pour bornes, avec M  1000, les quantiles empiriques ω
0,025
∗
2∗ , . . . , ω
M
, soit :
respectifs 0,025 et 0,975 de la série ω
1∗ , ω
∗
∗
IC0,95 (ω)  [
ω0,025
,ω
0,975
].

Cette méthode présentée à l’origine comme méthode de référence des IC bootstrap est moins précise que la précédente et ne donne de bons résultats qu’à
condition qu’il existe une fonction croissante h telle que h(
ω ) ait une loi symétrique autour de h(ω). Cette condition est forte et évidemment invériﬁable
dans les situations pratiques complexes où le bootstrap est le principal recours.
Cette méthode n’est donc pas sans risque.
Nous avons concentré notre attention sur l’obtention d’un intervalle de
conﬁance dans le cadre non paramétrique. Cependant l’intérêt des valeurs
∗
bootstrap ω
1∗ , ω
2∗ , . . . , ω
M
ne se limite pas à cela. En eﬀet pour des types très
divers de statistiques, il a été démontré que la distribution générée par les
∗
2∗ , . . . , ω
M
en faisant tendre M vers l’inﬁni, appelée distribution
valeurs ω
1∗ , ω
bootstrap de la statistique ω
 , converge vers la vraie distribution de ω
 quand n
tend vers l’inﬁni et ceci de façon rapide. En d’autres termes, pour une valeur de
∗
2∗ , . . . , ω
M
reﬂète bien la distribution
M raisonnable, la série des valeurs ω
1∗ , ω
d’échantillonnage de ω
 . Il faut bien distinguer la convergence pour M → ∞ qui,

180

Statistique − La théorie et ses applications

à un premier niveau, assure une bonne approche de l’estimation du maximum
de vraisemblance de toute statistique propre à l’échantillon réalisé à partir
de sa valeur dans la distribution bootstrap, de la convergence pour n → ∞ qui,
à un deuxième niveau, concerne la convergence de l’estimateur du maximum
de vraisemblance vers la vraie valeur ω, même si, en vérité, les deux niveaux
sont intimement liés, et ceci de façon optimale, ce qui fait la grande force du
bootstrap. Notons ici que la méthode des percentiles donne en fait un intervalle
de probabilité approché pour la statistique ω
.
Ainsi, au-delà de l’écart-type, toute caractéristique de la distribution
d’échantillonnage de ω
 peut être estimée par sa version empirique dans la série
∗
 est une estimation
des valeurs bootstrap. Par exemple la moyenne de la série ω
de la moyenne EF (
ω ) de la loi de ω
 . Ceci peut déboucher sur l’estimation du
biais d’une statistique. Illustrons ceci pour la moyenne α-tronquée.
En présence de valeurs extrêmes il peut être préférable (au sens de l’e.q.m.)
d’utiliser une moyenne α-tronquée (voir section 8.2.1), que nous notons toujours
ω
 , pour estimer la moyenne μ de la loi étudiée. On peut, par le bootstrap,
ω ) de cet
estimer le gain en variance en comparant la variance bootstrap s2∗ (
estimateur à la variance estimée de la moyenne simple, à savoir s2 /n. Mais
cette moyenne α-tronquée peut être plus ou moins fortement biaisée. On peut
∗
 −x (signalons
obtenir une estimation raisonnable de ce biais par la diﬀérence ω
toutefois qu’il n’est pas pour autant judicieux de rectiﬁer de ce biais l’estimation
ω
 ). Il est clair que ceci ne s’applique qu’en présence de valeurs extrêmes et non
de valeurs aberrantes qui ne proviendraient pas de la loi F, car la théorie repose
sur une série d’observations toutes issues de cette même loi F.
L’approche bootstrap est appropriée dans des situations très complexes, où
il n’y aura généralement pas d’alternative, et pas uniquement dans le cadre de
l’échantillonnage aléatoire simple. C’est donc un outil extrêmement précieux
qui est devenu viable avec les capacités de calcul actuelles.
Signalons en particulier qu’alors que le jackknife échoue pour obtenir un
intervalle de conﬁance pour la médiane (ou un quantile) de la loi mère, le
bootstrap donne un résultat très proche de l’intervalle proposé par approche
directe en section 8.3. Il est intéressant aussi de voir qu’il peut s’appliquer
dans un cadre paramétrique si l’on estime une fonction du paramètre avec un
estimateur dont on ignore la loi ou l’expression de la variance. On aura alors
avantage à eﬀectuer les tirages de l’étape 2 à partir de la fonction de répartition
estimée F (x; θM V ) obtenue en remplaçant θ par son estimation du maximum de
vraisemblance dans l’expression F (x; θ). On montre que pour M tendant vers
l’inﬁni et sous certaines conditions de régularité, l’intervalle de conﬁance de la
méthode des percentiles tend vers celui obtenu par l’approximation normale
asymptotique du MV vue en section 7.3.
Enﬁn il existe des variantes visant à améliorer encore le bootstrap basique :
méthode des percentiles à biais corrigé, bootstrap lissé.

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

181

Pour approfondir la méthode du bootstrap on pourra consulter l’ouvrage de
Davison et Hinkley (1997) pour les aspects méthodologiques et celui de Shao
et Tu (1995) pour les démonstrations mathématiques.

8.5
8.5.1

Estimation fonctionnelle
Introduction

Dans le cadre non paramétrique il est pertinent de vouloir estimer, dans
sa globalité, la fonction de répartition F, ou la fonction de densité f, ou la
fonction de probabilité p, alors que, dans le cadre paramétrique, ces fonctions
découlent du seul choix du paramètre inconnu θ. Pour les v.a. continues nous
ferons l’hypothèse que F et f sont quelconques mais dérivables, donc lisses en
pratique, au moins jusqu’à l’ordre 2 et parfois plus. Pour les v.a. discrètes on
pourrait envisager de travailler avec de telles hypothèses pour F et p étendues
à tout R (comme c’est le cas pour la plupart des lois discrètes usuelles), mais
ceci n’est pratiquement jamais eﬀectué car on se contente, l’ensemble des valeurs possibles {xi } étant connu, d’estimer p(xi ) (respectivement F (xi )) par
les fréquences relatives (respectivement les fréquences relatives cumulées) observées. Cette section ne concerne donc que les variables aléatoires
continues. Au sens strict on exclut donc l’étude d’une population réelle qui
ne prend qu’un nombre ﬁni de valeurs, mais on peut supposer qu’en amont
d’une telle population il existe un modèle virtuel, parfois appelé modèle de
superpopulation, qui est celui que l’on cherche à estimer.
On peut se poser la question de savoir quel est l’intérêt réel de l’estimation de f ou de F. L’intérêt le plus immédiat est de visualiser la distribution
des valeurs, ce qui est propre à la fonction de densité plutôt qu’à la fonction de répartition. En eﬀet cette dernière met mal en évidence les zones de
fortes ou faibles probabilités. L’estimation de la densité peut aussi viser à une
première prise de connaissance du phénomène de façon à orienter la recherche
d’un modèle adéquat dans la panoplie des modèles paramétriques disponibles.
Dans certaines applications techniques la densité ou la fonction de probabilité peut avoir un intérêt en soi, par exemple pour eﬀectuer des simulations
ﬁnes de processus ou pour montrer des caractéristiques très spéciﬁques (par
exemple des points d’inﬂexion) ayant une interprétation physique. Quant aux
caractéristiques usuelles (moyenne, variance, moments, quantiles, etc.) il arrive
qu’en les estimant par les caractéristiques correspondantes de l’estimation de la
fonction F ou f on améliore les méthodes d’estimation directes, mais nous n’envisagerons pas ces possibilités encore peu explorées, considérant que globalement les estimations directes vues aux sections précédentes restent préférables.
Nous n’avons pas abordé plus haut l’estimation du ou des modes de la distribution (i.e. les positions des maxima de la densité) : l’estimation de la densité
fournira une façon pertinente d’estimer ces caractéristiques.

182

Statistique − La théorie et ses applications

Le développement de l’estimation fonctionnelle est relativement récent, notamment parce que les méthodes mobilisent de gros moyens de calcul et qu’elles
s’appliquent à des échantillons de taille plutôt grande. Dans les années 1950
on s’est d’abord intéressé à l’estimation de la densité qui présente, comme on
vient de le voir, un intérêt dominant.

8.5.2

L’estimation de la densité

L’histogramme comme estimation de densité
L’histogramme dont l’origine est attribuée à John Graunt au XVIIe siècle
répondait à l’objectif d’une représentation de la distribution de valeurs et, à ce
titre, peut être considéré comme une estimation de densité avant l’heure. C’est
sous cet angle que nous allons l’étudier même si son intérêt réside souvent
plutôt dans l’estimation de pourcentages dans des intervalles (ou «classes»)
bien déterminés (par exemple les classes d’âge des statistiques oﬃcielles).
Dans sa plus grande généralité un histogramme se déﬁnit à partir d’une
suite double de valeurs croissantes {. . . , a−i , . . . , a−1 , a0 , a1 , . . . , ai , . . .} constituant un découpage en intervalles de la droite réelle. Soit nk la fréquence des
observations situées dans l’intervalle ]ak , ak+1 ] pour un échantillon de taille n,
alors l’histogramme est la fonction constante par morceaux fn telle que, pour
tout k ∈ Z :
nk
n
pour x ∈ ]ak , ak+1 ],
fn (x) =
(ak+1 − ak )
conduisant à la représentation graphique classique en rectangles (obtenue en
délimitant verticalement les intervalles). La fréquence relative nk /n estimant
la probabilité (ou la proportion, dans une population) pk associée à l’intervalle
]ak , ak+1 ] y est divisée par la largeur ak+1 − ak de cet intervalle ce qui a bien
valeur de densité de probabilité au sens explicité en section 1.4.
Sauf exception on choisit une «grille» de découpage {ak } régulière et soit,
alors, h la largeur de chaque intervalle. On a :
nk
fn (x) =
nh

pour x ∈ ]ak , ak+1 ] .

Plaçons-nous maintenant dans le cadre d’un n−échantillon aléatoire de loi
mère de densité f continue sur tout R et étudions les propriétés d’échantillonnage de la v.a. (notée simplement comme précédemment) fn (x) = Nk /nh où
Nk est le nombre aléatoire de valeurs tombant dans ]ak , ak+1 ], x étant ﬁxé dans
cet intervalle. On a Nk ; B(n, pk ), d’où :
pk
npk
=
nh
h
np
pk (1 − pk )
(1
−
pk )
k
=
.
V (fn (x)) =
n2 h2
nh2
E(fn (x)) =

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

183

a
Comme pk = akk+1 f (x)dx, phk est la valeur moyenne de f sur [ak , ak+1 ] et fn (x)
n’est donc sans biais que pour la ou les valeurs de x dans ]ak , ak+1 ] où f prend
cette valeurmoyenne. Nous désignons par x∗k l’une de ces valeurs, i.e. telle que
a
f (x∗k ) = h1 akk+1 f (x)dx = phk . Pour toute valeur x où f (x) diﬀère de f (x∗k ),
fn (x) est un estimateur biaisé de f (x) dont le biais est égal à f (x∗ ) − f (x).
k

Considérons le comportement asymptotique de fn (x). Quand n → ∞ le
biais qui ne dépend pas de n ne peut tendre vers zéro. Sauf à faire tendre simultanément la largeur d’intervalle h vers zéro, auquel cas x tend nécessairement
vers x∗k et, par continuité de f , f (x∗k ) − f (x) tend donc vers zéro. Quant à la
variance, du fait que pk (1 − pk )/h = (1 − pk )f (x∗k ), elle ne peut tendre vers
zéro que si, simultanément, nh → ∞. En d’autres termes la largeur d’intervalle doit tendre vers 0 mais de√façon inﬁniment moins « rapide » que 1/n, par
exemple en choisissant h = c/ n. Puisque E(Nk ) = npk = nhf (x∗k ) la condition nh → ∞ assure que le nombre attendu de valeurs dans ]ak , ak+1 ] tende
vers l’inﬁni. Concrètement cela se traduit de la façon suivante : plus n est grand
plus il y a avantage à resserrer les intervalles mais pas trop, aﬁn de garder de
grandes valeurs de nk .
Ces conditions qui assurent la convergence en moyenne quadratique - soit
n → ∞, h → 0, nh → ∞ - restent nécessaires pour assurer d’autres modes
de convergence, notamment en probabilité ou presque sûrement. De plus elles
se retrouvent pour tous les types d’estimateurs fonctionnels comme nous en
verrons des exemples plus loin. La proposition suivante, que nous admettrons,
vient préciser la forme asymptotique de l’erreur quadratique moyenne de fn (x)
pour x ﬁxé.
Proposition 8.4 (Friedman et Diaconis, 1981) En tout x où f est deux fois
dérivable on a :
eqm(fn (x)) =

h2 
f (x)
1
[f (x)]2 +
+ o(h2 ) + o( ) .
12
nh
nh

Ainsi avec les conditions n → ∞, h → 0, nh → ∞, l’e.q.m. est asymptoti2
(x)
, le premier terme étant dû au biais
quement équivalente à h12 [f  (x)]2 + fnh
(au carré) et le deuxième à la variance. L’intérêt de ce résultat est d’établir
la vitesse de convergence de l’e.q.m. vers zéro, dans le cas où h est choisi de
façon optimale, pour la comparer plus loin avec un estimateur plus élaboré. En
annulant sa dérivée par rapport à h on trouve que cette dernière expression est
minimale pour :
h={

6f (x) 1/3 −1/3
} n
[f  (x)]2

et l’e.q.m. est asymptotiquement équivalente, avec cet h optimal, à k(x)n−2/3
où k(x) dépend de f (x) et de f  (x). On dira que la vitesse de convergence de
l’e.q.m. est en n−2/3 .

184

Statistique − La théorie et ses applications

Remarques
1. La construction de l’histogramme dépend de deux paramètres : h, la
largeur des intervalles, et a0 , la position de l’origine de la grille. En
fait le choix de a0 n’est pas crucial (et ne subsiste pas dans l’approche
plus élaborée qui va suivre) alors que celui de h est déterminant et incontournable. Notons que le résultat précédent concernant le h optimal
n’est d’aucun secours en pratique car cette valeur dépend de f (x) et
de f  (x) qui sont inconnus. Diﬀérentes règles empiriques, étrangères aux
considérations asymptotiques ci-dessus, ont été proposées en statistique
descriptive, par exemple√ : choisir un nombre d’intervalles sur l’étendue
des observations égal à n.
2. Si f est discontinue aux bornes de son support (voir la loi uniforme, la
loi exponentielle ou la loi de Pareto) les résultats développés ne sont pas
valables en ces bornes. De plus si celles-ci sont inconnues l’histogramme
pose problème.
3. L’histogramme, c’est-à-dire la fonction fn , est discontinu alors même
que f est continue. On peut donc songer à le rendre continu pour, sans
doute, améliorer son eﬃcacité. C’est l’idée qui prévaut pour le polygone
des fréquences (ligne brisée reliant les milieux des «plateaux» de l’histogramme) qui reste cependant peu usité. La méthode de la section suivante
va proposer une solution plus performante.
Les estimateurs à noyaux
Déﬁnition L’origine de la méthode des noyaux est due à Rosenblatt (1956).
Celui-ci a proposé une sorte d’histogramme mobile où la fenêtre de comptage
des observations se déplace avec la valeur de x. La densité en x est estimée
par la fréquence relative des observations dans l’intervalle [x − h, x + h], donc
centré sur x, divisée naturellement par la largeur de l’intervalle 2h. On appelle
h la largeur de fenêtre (bien que cette largeur soit en fait égale à 2h). Pour des
raisons qui apparaı̂tront plus loin nous écrivons l’estimation ainsi obtenue à
partir des observations x1 , x2 , · · · , xn sous la forme suivante (conservant encore
la même notation fn ) :
1 
K
fn (x) =
nh
n

i=1

où



x − xi
h



1
K(u) = si u ∈ [−1, +1] et 0 sinon.
2

i
En eﬀet xi ∈ [x − h, x +
si et seulement
si x−x
∈ [−1, +1] et xi est alors
h

h]
n
x−xi
comptabilisé 1/2. Ainsi i=1 K h
est égal au nombre d’observations tombant dans [x − h, x + h] divisé par 2 pour obtenir la division de la fréquence
relative par 2h. Comme K est discontinue en ±1, fn (x) présente des petits
sauts de discontinuité aux points x1 ± h, x2 ± h, . . . , xn ± h. Parzen (1962) a

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

185

proposé une généralisation de l’idée de Rosenblatt permettant, entre autres,
de lisser davantage l’estimation. A la fonction K ci-dessus on substitue une
fonction que l’on pourra choisir continue ou dérivable partout, propriété qui
se transfère à la fonction fn . En d’autres termes on fera entrer ou sortir les
points xi «en douceur» quand on déplace la fenêtre. Toutefois la fonction K
est soumise aux conditions suivantes :
– K est positive (ou nulle)
– K
 est paire
– R K(u)du = 1.
Une telle fonction
 est alors appelée noyau. La première condition garantit que

i
de chaque observation xi reste positif ou nul, la deuxième
le poids K x−x
h
que ce poids soit identique de part et d’autre de x. La troisième condition est
une normalisation des poids de façon que fn soit bien une densité. En eﬀet,
i
on obtient :
avec le changement de variable u = x−x
h






1 
fn (x)dx =
nh i=1
R
n

K
R

x − xi
h



h 
nh i=1
n

dx =



1
1 = 1.
n i=1
n

K (u) du =
R

Notons qu’une fonction noyau est, en fait, une fonction de densité symétrique
autour de zéro, donc de moyenne nulle (si elle existe).
Comme :
1
nh




K
R

x − xi
h


dx =

1
n


K(u)du =
R

1
,
n

on peut donner une interprétation concrète de fn . Supposons, pour ﬁxer les
idées, que K ait pour support [−1, +1]. Alors fn est obtenue en remplaçant
chaque observation xi par une même petite «densité» (son aire étant réduite à
1
) de support [xi −h, xi +h] , puis en sommant ces petites densités. Cette vision
n
correspond à un principe général de lissage de données discrètes qui consiste à
faire «bouger» chaque donnée pour lui substituer un élément continu.
En pratique on impose comme condition supplémentaire que K décroisse
de part et d’autre de zéro, dans l’idée naturelle de donner un poids plus faible
aux observations au fur et à mesure qu’elles s’éloignent du centre de la fenêtre
x. Ainsi les noyaux les plus usuels sont :
1
2
K(u) = 1 − |u|
3
K(u) = (1 − u2 )
4
15
(1 − u2 )2
K(u) =
16
1
1
K(u) = √ exp(− u2 )
2
2π
K(u) =

si u ∈ [−1, 1]

noyau de Rosenblatt

si u ∈ [−1, 1]

noyau triangulaire

si u ∈ [−1, 1]

noyau d’Epanechnikov

si u ∈ [−1, 1]

noyau de Tukey ou biweight

u∈R

noyau gaussien.

186

Statistique − La théorie et ses applications

Les deux premiers ont l’avantage d’être simples, le noyau triangulaire étant
continu partout et conduisant à une estimation fn continue. Le troisième doit
sa notoriété à une propriété d’optimalité théorique mais sans grand intérêt pratique (voir plus loin le paragraphe «choix pratiques»). Le quatrième est, à notre
sens, le plus intéressant car donnant une estimation dérivable partout, tout en
étant simple à mettre en oeuvre. En fait il s’agit du noyau le plus simple parmi
les noyaux de forme polynomiale dérivables partout. Ainsi il assure le lissage
local de la fonction fn . Ce noyau est d’une forme très proche du noyau gaussien et est donc préférable, ce dernier ayant un coût de calcul plus élevé du fait
de son support inﬁni (la «largeur de fenêtre» h devenant conventionnellement
l’écart-type de la loi de Gauss). Notons que plus la valeur de h est élevée plus
on élargit la fenêtre, ce qui a un eﬀet de lissage global de fn plus important.
Ceci est à rapprocher du choix de la largeur des intervalles pour l’histogramme.

Note 8.5 Le choix du type de noyau étant ﬁxé, seul reste à eﬀectuer le choix de
h que nous envisagerons plus loin. Le problème du positionnement de la grille
de l’histogramme (choix de a0 ) n’existe pas ici. Pour ce dernier on peut aussi
s’aﬀranchir de ce choix en prenant la moyenne, en continu, des estimations
obtenues par glissement continu de la grille de a0 quelconque à a0 + h. On
obtient alors l’estimateur à noyau triangulaire (voir exercices).

Propriétés asymptotiques des estimateurs à noyaux Il existe peu de
résultats à n ﬁni et l’on doit se satisfaire de résultats asymptotiques. Reprenons
l’expression générale d’un estimateur à noyau pour un échantillon aléatoire
X1 , X2 , . . . , Xn :
1 
fn (x) =
K
nh i=1
n



x − Xi
h


.

Pour calculer le biais et la variance en un point x ﬁxé posons :

Zi =

1
K
h



x − Xi
h


.

Ainsi la variable aléatoire fn (x) est la moyenne des Zi qui, en tant que fonctions

respectives des Xi , sont des variables aléatoires i.i.d.. Soit Z = h1 K x−X
la
h
v.a. symbolisant la loi commune aux Zi comme X symbolise la loi mère des Xi
de densité f.

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

187

Calcul du biais
On a :

Comme

 

n
1
1
x−X

Zi ) = E(Z) = E K
E(fn (x)) = E(
n i=1
h
h



x−t
1
f (t)dt
K
=
h R
h

x−t
.
=
K(u)f (x + uh)du en posant u =
h
R


R

K(u)du = 1 le biais peut s’écrire :

E(fn (x)) − f (x) =
K(u)[f (x + uh) − f (x)]du .
R

On voit que le biais résulte de l’écart de la valeur de la densité dans la fenêtre
centrée sur x par rapport à sa valeur en x même. Si f était constante dans la
fenêtre le biais serait nul, et de même si f était parfaitement linéaire en raison
de la parité du noyau K. Comme pour l’histogramme le biais ne dépend pas
de la taille de l’échantillon et ne peut être réduit à zéro qu’en faisant tendre h
vers zéro. Prenons un développement de Taylor de f au voisinage de x :
f (x + uh) = f (x) + uhf  (x) +
Le biais s’écrit :



u2 h2 
f (x) + o(h2 ) .
2

h2
uK(u)du + f  (x)
2
R

h2 
f (x) u2 K(u)du + o(h2 )
=
2
R

E(fn (x)) − f (x) = hf  (x)


u2 K(u)du + o(h2 )
R

puisque K est paire. Pour h petit le biais dépend donc de f  (x) et du moment
d’ordre 2 du noyau. Le biais est du signe de f ”(x) : si f est concave en x le
biais est négatif, si elle est convexe le biais est positif. En particulier si x est
un point où f est à un maximum le biais sera négatif. On sous-estime donc (en
moyenne) la hauteur du maximum, ce que l’on peut comprendre intuitivement :
la densité au voisinage de x étant plus faible il y a nécessairement un déﬁcit de
points dans la fenêtre. A l’inverse les minima éventuels seront surestimés. Par
conséquent la méthode tend à écrêter les creux et les pics de la densité ce qui
est un inconvénient majeur.
Calcul de la variance
On a :
V (fn (x) = V (

n
5
14
1
1
E(Z 2 ) − [E(Z)]2
Zi ) = V (Z) =
n i=1
n
n

188

Statistique − La théorie et ses applications

avec :
2
  
1
1
x−t
E(Z 2 ) =
f (t)dt
K
n
nh2 R
h

x−t
1
2
)
[K(u)] f (x + uh)du (en posant u =
=
nh R
h
et :
1
1
[E(Z)]2 =
n
n



2
K(u)f (x + uh)du

.

R

Alors que le terme n1 [E(Z)]2 tend bien vers zéro quand n → ∞ on voit que
le terme n1 E(Z 2 ) ne tend vers zéro que si nh → ∞. Par conséquent, pour que
fn (x) converge vers f (x) en moyenne quadratique les mêmes conditions sont
nécessaires que pour l’histogramme : n → ∞, h → 0, nh → ∞.
Le terme n1 [E(Z)]2 est d’ordre n1 , ce que l’on note O( n1 ). En utilisant le
développement de Taylor : f (x + uh) = f (x) + uhf  (x) + o(h), on obtient :

1
1
1
2
E(Z 2 ) =
f (x) [K(u)] du + O( ) ,
n
nh
n
R
d’où :
V (fn (x) =

1
f (x)
nh



1
2
[K(u)] du + O( ).
n
R

Finalement l’erreur quadratique moyenne en x ﬁxé est :
h4 
[f (x)]2
eqm(fn (x)) =
4

2


2

u K(u)du

+

R

f (x)
nh



2

[K(u)] du
R

1
+ o(h4 ) + O( ) .
n
Faisant abstraction des termes o(h4 ) + O( n1 ) négligeables dans les conditions
de convergence, on voit que plus la largeur de fenêtre h est faible plus le biais
diminue mais plus la variance augmente et, inversement, l’élargissement de la
fenêtre augmente le biais et diminue la variance. Il existe un optimum (mais
valable uniquement pour le point x) qui, comme pour l’histogramme, est obtenu
en dérivant par rapport à h, soit :


hopt

f (x)



1/5

2

[K(u)] du
R
=


2
[f (x)]
u2 K(u)du
R

2

n−1/5

et, en substituant hopt dans la formule de l’expression asymptotique de l’e.q.m.,
celle-ci prend la forme k(x) ν(K) n−4/5 où ν(K) est une expression qui ne
dépend que du choix du noyau et k(x) est fonction de f (x) et de f  (x). Ainsi

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

189

la convergence est plus rapide que pour l’histogramme, étant d’ordre n−4/5 au
lieu de n−2/3 .
Jusqu’à présent nous avons raisonné à x ﬁxé. Il est clair que ce qui nous
intéresse est de connaı̂tre le comportement de l’estimateur fn de la fonction f
globalement sur tout R. Pour cela on considère, pour une réalisation donnée, son
écart à f intégré sur tout R ce qui conduit, en prenant l’espérance mathématique
de cet écart intégré, au critère d’erreur quadratique intégrée moyenne (e.q.i.m.
ou MISE en anglais : mean integrated square error ) :
 +
,2 


fn (x) − f (x) dx .
eqim(fn ) = E
R

Celle-ci se calcule aisément à partir des résultats précédents car, étant donné
les conditions de régularité imposées à f et à K, il est licite d’intervertir
les intégrations (l’une explicite, l’autre implicite dans le calcul de l’espérance
mathématique) ce qui conduit à intégrer l’expression de l’e.q.m. en x ﬁxé :
+


,2 
eqim(fn ) =
dx =
E fn (x) − f (x)
eqm(fn (x))dx.
R

R

D’où :
h4
eqim(fn ) =
4






2

[f (x)] dx
R

2
2

u K(u)du
R

1
+
nh



2

[K(u)] du
R

1
+ o(h ) + O( ).
n
4

Comme précédemment on trouve un h optimal qui est en n−1/5 et une e.q.i.m.
de la forme g(f  ) ν(K) n−4/5 . Le même critère aurait pu être appliqué à l’histogramme, la vitesse de convergence étant également conservée en n−2/3 .
Ainsi, en tant qu’estimateur fonctionnel un estimateur à noyau converge
plus vite vers la vraie densité f que l’histogramme. Mais ce résultat repose sur
un choix optimal très théorique (puisque dépendant de l’inconnue f  ) et de
conditions de convergence artiﬁcielles. C’est pourquoi nous considérons maintenant les aspects pratiques.
Choix pratiques Le praticien doit eﬀectuer deux choix : celui du noyau K
et celui de la fenêtre h.
Le choix de K s’avère être relativement indiﬀérent pour ce qui concerne le
critère de l’e.q.i.m. Ceci a pu être constaté par calcul direct ou par simulation
sur une grande variété de lois mères et est d’ailleurs conﬁrmé sur l’expression
asymptotique ci-dessus. En eﬀet la valeur minimale de ν(K) est atteinte avec le
noyau d’Epanechnikov. Cette valeur est 0,349 alors qu’elle est égale à 0,351 pour
le biweight et 0,369 pour le noyau de Rosenblatt (voir exercices). Par conséquent

190

Statistique − La théorie et ses applications

le biweight (noyau de Tukey) doit être recommandé pour son avantage de lissage
local évoqué plus haut.
Reste le choix diﬃcile de h pour lequel diverses méthodes ont été proposées,
aucune ne donnant satisfaction de façon universelle. Sans aborder dans le détail
ce vaste sujet mentionnons trois approches.
Deheuvels (1977) a suggéré de prendre la valeur optimale vue ci-dessus :
m(f  ) ν(K) n−4/5 , en calculant m(f  ) sur f gaussienne. Toutefois on n’échappe
pas à l’estimation de la variance σ de la loi de Gauss que l’on eﬀectue naturellement par la variance empirique.
Une deuxième approche dérivée d’une procédure de type rééchantillonnage
dite de validation croisée a été étudiée initialement par Marron (1987) et est
souvent adoptée par les logiciels, car de portée plus générale. Elle consiste à
choisir la valeur de h qui maximise l’expression :
n
$

−i
(xi )
fn,h

i=1
−i
où fn,h
est l’estimation de densité eﬀectuée avec une valeur h en omettant
la i-ème observation. On maximise ainsi globalement les densités attribuées
aux observations xi à la manière du maximum de vraisemblance. Pour que
l’évaluation en xi ne soit pas inﬂuencée par la valeur de xi elle-même, on
élimine celle-ci du calcul.

Pour notre part nous proposons de choisir, avec le noyau biweight :
h = 0,75 min[x(i+[ n2 ]) − x(i) ]
i

n
100

−1/5

.

Ceci résulte du fait que, pour une diversité de lois et pour n = 100, la valeur
optimale reste proche de 0,75 fois la largeur de l’intervalle de probabilité 0,5
autour du mode. Cette méthode est simple à mettre en oeuvre et eﬀectue
généralement un lissage adéquat (voir Lejeune, 1982).
La plupart des propositions de choix de h reposent sur l’optimisation de
l’e.q.i.m. (ou sur l’un des critères mentionnés plus loin, mais aucun n’est la
panacée) et, par expérience, on constate souvent qu’elles ne fournissent pas
nécessairement une estimation graphiquement satisfaisante, laissant subsister
des variations locales (tendance à sous-lisser). La méthode la plus sûre reste
donc celle des essais et erreurs où, partant d’une valeur de toute évidence
trop faible de h donnant des ﬂuctuations locales indésirables, on augmente
progressivement cette valeur jusqu’au seuil de disparition de telles ﬂuctuations.
Par calcul direct ou par simulation on constate que, même pour d’assez
grands échantillons, la valeur de h eﬀectivement optimale (au sens de l’e.q.i.m.
mais aussi d’autres critères d’erreur) reste étonnamment élevée. Ainsi en prenant, simplement à titre indicatif, une loi mère N (0 ; 1) la valeur de h optimale

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

191

est de 1,11 pour n = 100 et vaut encore 0,70 pour n = 1 000. Ceci restreint
fortement la validité des expressions asymptotiques établies avec h tendant vers
zéro. De plus avec un tel choix il faut s’attendre à un écrêtement non négligeable
des extrema de la densité.
Même si les expressions asymptotiques sont à prendre avec précaution, elles
permettent de vériﬁer sur diverses lois que la méthode des noyaux, outre le
fait qu’elle peut donner une estimation lisse, est nettement plus eﬃcace, au
sens de l’e.q.i.m., que l’histogramme. En se plaçant, par exemple, aux valeurs
respectives optimales de h avec n = 100, pour une loi mère N (0 ; 1), l’e.q.i.m.
asymptotique de l’histogramme est 2,5 fois plus élevée que celle obtenue par
noyau biweight. Lorsque n s’accroı̂t ce rapport augmente, ce qui correspond
aux diﬀérences de vitesses de convergence : pour n = 1 000 il vaut 3,4 (voir
exercices).
Remarques diverses On a vu que le biais était de nature à écrêter les
extrema de f ce qui est particulièrement fâcheux s’agissant de points caractéristiques de la densité (dont son ou ses modes). On peut réduire le biais, et
notamment ce phénomène, en relâchant la contrainte de positivité du noyau.

En eﬀet en choisissant un noyau K tel que R u2 K(u)du = 0 on élimine, dans

2
l’expression asymptotique du biais le terme en h2 : h2 f  (x) R u2 K(u)du. En
poursuivant le développement de Taylor jusqu’à l’ordre 4 l’expression asymp
4
totique du biais devient h4! f (4) (x) R u4 K(u)du + o(h4 ) où f (4) est la dérivée à
l’ordre 4 de f. Ainsi, si f reste proche d’un polynôme du deuxième ou troisième
degré au voisinage de x, le biais sera pratiquement réduit
 2 à zéro, ce qui peru K(u)du = 0 (et par
met de mieux coller aux
extrema.
Un
noyau
vériﬁant
R

la parité, forcément R u3 K(u)du = 0) est appelé noyau d’ordre 4. Le noyau
d’ordre 4 dérivable partout de type polynomial le plus simple est :
K(u) =

105
(1 − u2 )2 (1 − 3u2 )
64

si u ∈ [−1, 1]

(0 sinon) ,

qui est une modiﬁcation du biweight et dont la ﬁgure 8.1 montre qu’il a des
plages négatives sur les extrémités. Ce faisant la variance est accrue par rapport
au biweight, mais cet accroissement est largement compensé par la diminution
du biais pour le critère d’e.q.i.m. (à titre d’exemple, pour une loi N (0 ; 1) le gain
global est de l’ordre de 25% avec des largeurs de fenêtre autour de l’optimum).
Toutefois le prix à payer pour réduire le biais est le fait que fn n’est
plus nécessairement positive ou nulle. Étant donné le faible poids des plages
négatives dans le noyau (voir ﬁgure 8.1) il s’agira d’eﬀets de bord : les zones
négatives de fn seront limitées aux extrémités, où les observations se font rares,
et seront de très faible ampleur. Néanmoins on sera contraint de « rectiﬁer » fn
sur ces bords. En raison de cet inconvénient, même marginal, le noyau d’ordre 4
n’a pas le succès qu’il mériterait pourtant. La ﬁgure 8.2 est un exemple obtenu
avec le noyau proposé ci-dessus.

192

Statistique − La théorie et ses applications

K (u ) =

105
(1 − u 2 ) 2 (1 − 3u 2 )
64

1,5
1
0,5

u
0
-1

-0,5

0

0,5

1

-0,5
Figure 8.1 - Noyau d’ordre 4 dérivé du biweight

Comme nous l’avons déjà indiqué d’autres critères d’erreur que l’erreur
quadratique intégrée ont été étudiés, notamment :
 



fn (x) − f (x) dx
R




sup fn (x) − f (x)
x

dont on a montré la convergence en probabilité vers zéro avec le même type
de conditions que précédemment (citons à ce propos les travaux de Devroye et
Györﬁ, 1985).
D’autres approches que la méthode des noyaux ont été proposées : séries
orthogonales, splines, maximum de vraisemblance pénalisé, plus proches voisins (nearest neighbour ), ondelettes, etc. Globalement on peut dire qu’elles
ne donnent pas des résultats signiﬁcativement meilleurs que la méthode des
noyaux. Aucune ne peut se soustraire à l’incontournable problème du choix
d’un paramètre de lissage, explicite ou non.

8.5.3

L’estimation de la fonction de répartition

La démarche que nous allons suivre est calquée sur celle de la densité.
Partant de la solution classique de la statistique descriptive, à savoir la fonction
de répartition empirique, nous étudierons les possibilités de lissage en vue d’une
amélioration.

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

193

Figure 8.2 - Estimation par noyau : données de taux de cholestérol de 3 200
hommes (source : projet FNRS-Suisse sur la prévention des maladies cardiovasculaires)

La fonction de répartition empirique
Rappelons sa déﬁnition donnée en section 5.7 :
1
I(−∞,x] (Xi ) pour tout x ∈ R,
Fn (x) =
n
n

i=1

où I(−∞,x] est la fonction indicatrice de l’intervalle (−∞, x]. Pour une réalisation
x1 , x2 , . . . , xn , c’est une fonction en escalier s’élevant de 1/n à chaque rencontre d’une valeur xi . Pour x ﬁxé on a vu que la statistique nFn (x) suit
une loi binomiale B(n, F (x)). Pour n grand, par l’approximation gaussienne
d’une binomiale (voir section 5.8.3), nFn (x) suit approximativement une loi
N (nF (x), nF (x)(1 − F (x))) et on a donc :


F (x)(1 − F (x))
.
Fn (x) ; N F (x) ,
approx
n
Les deux paramètres de cette loi de Gauss correspondent à la moyenne et la
variance de Fn (x). Pour x ﬁxé, Fn (x) est donc un estimateur sans biais et
convergent de F (x). Notons que Fn (x) est la moyenne d’une suite de variables

194

Statistique − La théorie et ses applications

aléatoires i.i.d. I(−∞,x] (Xi ) (elles sont indépendantes comme fonctions respectives des Xi ) toutes issues de la loi de Bernoulli de paramètre p = F (x). Ainsi
la loi des grands nombres (théorème 5.3) s’applique et Fn (x) converge aussi
presque sûrement vers F (x). L’approximation gaussienne ci-dessus permet de
construire un intervalle de conﬁance approché sur le modèle de celui de la
section 7.4.5 pour le paramètre p d’une loi de Bernoulli.
Dans cette section nous nous intéressons aux propriétés de Fn en tant qu’estimateur fonctionnel de F. La première proposition qui suit a déjà été mentionnée plus haut dans le cadre du rééchantillonnage (section 8.4).
Proposition 8.5 La fonction de répartition Fn est l’estimateur fonctionnel du
maximum de vraisemblance pour F.
Cette proposition démontrée par Kiefer et Wolfowitz (1956) mérite d’être
commentée dans la mesure où, jusqu’à présent, l’estimateur du maximum de
vraisemblance (EMV) n’a été déﬁni que dans le cadre paramétrique (déﬁnition
6.11). Le principe reste le même : il s’agit de donner aux valeurs observées
x1 , x2 , · · · , xn la plus forte densité ou fonction de probabilité. Dans le cas
continu on doit rechercher pour
*nquelle fonction F l’expression de la fonction de
vraisemblance de F : L(F ) = i=1 F  (xi ), est maximisée. Pour une loi discrète
F  (xi ) doit être remplacé par F (xi ) − F (x−
i ) correspondant à la fonction de
probabilité en xi (voir section 1.3). En fait on a avantage à rester dans la plus
grande généralité, n’ayant aucun a priori sur la nature de la loi et considérant
une maximisation sur l’ensemble F des fonctions de répartition englobant le cas
discret, continu ou mixte. F est donc l’ensemble des fonctions répondant aux
conditions nécessaires et suﬃsantes d’une fonction de répartition (croissance
sur R de 0 à 1, continuité à droite en chaque point). Cette approche générale
nécessite des connaissances au-delà du niveau de cet ouvrage et nous admettrons donc que la solution du problème de maximisation sur F est Fn . L’intérêt
d’en rester à une approche générale tient au fait qu’une solution simple existe
(si l’on met une contrainte de continuité pour F le problème devient diﬃcile)
et qu’elle est naturelle dans la mesure où il s’ensuit que l’EMV de toute caractéristique de la loi mère s’exprimant
ncomme une espérance mathématique
d’une fonction g(X) devient alors n1 i=1 g(Xi), le cas le plus simple étant
celui de la moyenne μ dont l’EMV est X (voir note 8.2).
Les propriétés de Fn (x) pour x ﬁxé ont été établies, notamment la convergence vers F (x). Le théorème suivant (que nous admettrons) est essentiel car
il montre la convergence uniforme, pour tout x ∈ R, de Fn vers F.
Théorème 8.1 (Glivenko-Cantelli) Soit un échantillon aléatoire X1 , X2 , . . . ,
Xn issu de la loi de fonction de répartition F et Fn sa fonction de répartition
empirique. Alors, quand n −→ ∞ :
p.s.

sup |Fn (x) − F (x)| −→ 0 .

x∈R

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

195

Pour voir les choses concrètement, ce théorème nous dit que l’on peut être
assuré que l’écart maximal entre Fn et F va tendre vers 0 si l’on augmente la
taille de l’échantillon à l’inﬁni ou encore que partout, simultanément, la fonction
de répartition empirique va se rapprocher de la vraie fonction de répartition.
De plus, le théorème suivant donne le comportement asymptotique de l’écart
maximal entre Fn et F.
Théorème 8.2 (Kolmogorov-Smirnov) Soit la variable aléatoire :
Dn = sup |Fn (x) − F (x)|.
x∈R

Alors, pour x > 0, on a :
∞

√
2
(−1)k−1 e−2(kx) .
P ( nDn < x) −→ 1 − 2
n→∞

k=1

√
En d’autres termes nDn tend en loi vers une v.a. à valeurs positives (car
Dn est nécessairement positive) de fonction de répartition G(x) égale à l’expression limite ci-dessus, laquelle ne dépend pas de F. En fait même pour n ﬁni
la loi de Dn ne dépend pas de F et, de ce fait, elle a été tabulée. A partir
de n = 40 l’approximation par G(x) est correcte à 10−2 près.
Ce résultat permet de donner une bande de conﬁance approchée pour F.
En eﬀet, soit g0,95 le quantile d’ordre 0,95 de G(x), on a :
√
P ( n sup |Fn (x) − F (x)| < g0,95 )  0, 95 .
x∈R

√
Mais l’événement ( n supx∈R |Fn (x) − F (x)| < g0,95 ) équivaut à l’événement
√
( n[Fn (x) − F (x)| < g0,95 ] , pour tout x)
ou :
g0,95
g0,95
(Fn (x) − √
< F (x) < Fn (x) + √
, pour tout x).
n
n
On a donc une procédure qui garantit, a priori avec une probabilité 0,95, que
F (x), pour tout x, soit compris dans l’intervalle :
g0,95
Fn (x) ± √
.
n
Pour une réalisation, la bande autour de Fn ainsi dessinée sera une région de
conﬁance à 95% approchée pour F. On peut même établir une bande exacte
en lisant dans une table le quantile 0,95 de la loi exacte de Dn . A partir de
n = 40 on peut utiliser l’expression asymptotique qui peut se réduire, disons
2
si x >0,8, pratiquement à 1 − 2e−2x en ne gardant que le premier terme de

196

Statistique − La théorie et ses applications

la somme. Ainsi g0,95 est approximativement déﬁni par la valeur de x telle que
2
1−2e−2x = 0,95 soit x = 1,36. Pour n assez grand la bande de conﬁance à 95
√ .
% est donc Fn (x) ± 1,36
n
Le théorème 8.2 trouvera plus loin une application très répandue pour tester
un modèle de loi (voir le test de Kolmogorov-Smirnov en section 10.4.1).
Lissage de Fn
Nous envisageons maintenant le lissage de Fn pour le cas où l’on sait pouvoir
se restreindre à une fonction de répartition dérivable jusqu’à un certain ordre. Il
existe, comme pour la densité, une série de solutions, mais nous ne présenterons
que celle qui fait le pendant de l’estimateur à noyau de la densité.
Considérons l’estimation de F obtenue en intégrant l’estimation par noyau
de la densité f :


n
t − xi
1 
K
dt
h
−∞
−∞ nh i=1


n 
t − xi
1  x
K
=
dt
nh i=1 −∞
h
n  x−xi
h
t − xi
1
K (v) dv en posant v =
.
=
n i=1 −∞
h

Fn (x) =



x

fn (t)dt =



x



u

Déﬁnissons le noyau intégré :
K (u) du ,

H(u) =
−∞

alors on a :
1
H
Fn (x) =
n
n

i=1



x − xi
h


.

Comme K(u) était de la forme d’une fonction de densité, H(u) est de la forme
d’une fonction de répartition.
Pour voir l’analogie avec la fonction de répartition empirique Fn , rappelons
que Fn (x) est la moyenne des indicatrices I(−∞,x] (xi ). Or :
I(−∞,x] (xi ) = I(−∞,0] (xi − x) = I(0,+∞,] (

x − xi
)=
h

0 si x < xi
.
1 si x ≥ xi

Fn est donc de la forme de Fn (x), avec une fonction H(u) très particulière,
donnant le saut brutal de 0 à 1 en u = 0. L’apport d’une fonction plus souple
répond au même principe général de lissage que celui évoqué pour la densité,

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

197

à savoir qu’on eﬀectue un passage de 0 à 1 en douceur, étalé entre xi − h et
xi + h autour de x. On remarquera que du fait que H est une primitive de K
elle est continue et Fn est donc également continue.
Par intégration du noyau de Rosenblatt, le plus simple, et de celui de Tukey
préconisé pour la densité, on obtient :
⎧
si u ≤ −1
⎨ 0
1
(u
+
1)
si
− 1 < u < +1
(Rosenblatt intégré)
H1 (u) =
⎩ 2
1
si + 1 ≤ u
⎧
⎪
⎨ 0
H2 (u) =

⎪
⎩

1
(8
16

1

si u ≤ −1
+ 15u − 10u3 + 3u5 )

si − 1 < u < +1
si + 1 ≤ u

(Tukey intégré) .

Lorsqu’on examine le graphe obtenu avec diﬀérents noyaux on constate que la
diﬀérence est imperceptible. Ceci s’explique par le fait que l’estimation d’une
fonction de répartition est fortement contrainte par la condition de croissance
de 0 à 1, et par sa continuité. De ce fait le problème est beaucoup plus simple
que pour la densité. En particulier la croissance implique de faibles courbures
et donc peu ou pas de problème de biais, contrairement à la densité. Il n’y a
donc pas d’avantage tangible à utiliser des noyaux ou autres instruments de
lissage sophistiqués et nous préconisons donc l’emploi du noyau H2 . Notons
bien, toutefois, que les estimations de densités obtenues par dérivation seront,
quant à elles, très sensibles aux variations jugées mineures pour la fonction
de répartition. Malgré ce constat il n’est pas inutile d’examiner le biais et la
variance, de façon asymptotique, comme cela a été fait pour la densité.
Biais et variance On démontre les résultats suivants (voir Lejeune et Sarda,
1992) par des développements similaires à ceux de la densité. Pour un noyau
K symétrique de support [-1,+1] on a, en x ﬁxé :

h2 
f (x) u2 K(u)du + o(h2 ) ,
E(Fn (x)) − F (x) =
2
R

!
 +1
1
H 2 (u)du − 1 + o(h) .
V (Fn (x)) =
F (x)[1 − F (x)] + hf (x)
n
−1
Alors que la fonction de répartition empirique Fn est sans biais, le lissage
ne peut éviter d’introduire un certain biais. Les simulations à n ﬁni montrent
toutefois que ce biais reste très faible. En particulier on voit sur l’expression
asymptotique qu’à o(h2 ) près il s’annule aux extrema de la densité (donc aux
modes) qui correspondent à des points d’inﬂexion pour F. Pour la variance
on retrouve dans le premier terme de son expression asymptotique la variance
de Fn . Par conséquent
on gagne sur la variance de Fn si le deuxième terme
 +1
est négatif, soit −1 H 2 (u)du < 1, ce qui est vériﬁé pour les noyaux intégrés

198

Statistique − La théorie et ses applications

courants. Dans le cas du noyau de Rosenblatt
h
3n f (x)

2

h
6



 +1
−1

H 2 (u)du =

2
3,

la variance

2

et le biais vaut
f (x) (à o(h ) près). Pour ce qui
décroı̂t donc de
concerne l’erreur quadratique moyenne elle sera améliorée si la diminution de
variance compense le biais au carré. Pour estimer, par exemple, le mode d’une
loi qui serait une loi de Gauss, il n’y a pas de biais en raison de la symétrie en
ce point et la variance diminue de 15 %.
Ici comme pour la densité se pose le problème de la largeur de fenêtre
optimale. Il est toutefois moins crucial en raison de la plus faible sensibilité de
l’estimation à ce paramètre de lissage.
Note 8.6 On peut penser que l’estimation d’une caractéristique ω(F ) par ω(Fn )
puisse être meilleure que la simple version empirique ω(Fn ). Encore faut-il choisir une valeur de h appropriée (la valeur optimale pour ce problème étant alors
généralement plus faible qu’avec l’objectif de minimisation de l’erreur quadratique

intégrée moyenne). Par ailleurs on a avantage à utiliser une version «rétrécie»
( de Fn

qui conserve la variance empirique s2 . Elle s’obtient en remplaçant x par

1+

h2
s2

x

dans l’expression de Fn (x). Silverman (1987) montre que pour la plupart des lois, le
moment simple d’ordre 6 est mieux estimé par une version lisse, alors que l’amélioration
n’est pas systématique pour les moments d’ordres inférieurs.

Pour approfondir l’estimation fonctionnelle on pourra consulter l’ouvrage
méthodologique très complet de Simonoﬀ (1996) ou, pour les aspects mathématiques, celui de Bosq et Lecoutre (1987).

8.6

Exercices

Exercice 8.1 Générer 200 observations de loi lognormale LN (0 ; 1) (aide :
dans un tableur du type EXCEL générer 200 observations de loi N (0 ; 1) et les
transformer par ex ).
Donner une estimation ponctuelle et par intervalle pour la médiane de la loi.
L’intervalle contient-il la vraie valeur ? Recommencer pour estimer le quantile
d’ordre 0,90.
Exercice 8.2 (Adapté de Mosteller et Tukey, 1977) Soit les valeurs 0,1 0,1 0,1
0,1 0,4 0,5 1,0 1,1 1,3 1,9 1,9 4,7. Donner une estimation du jackknife de l’écarttype de la loi mère ayant généré ces observations, fondée sur la statistique S.
Donner un intervalle de conﬁance pour cet écart-type.
Exercice 8.3 Montrer que les pseudo-valeurs du jackknife fondées sur la van
(Xi − X)2 , i = 1, . . . , n.
riance empirique S'n2 sont n−1

Chapitre 8. Estimation non paramétrique et estimation fonctionnelle

199

Exercice 8.4 Vériﬁer que l’estimateur du jackknife appliqué à la moyenne
empirique redonne la moyenne empirique. Quelles sont les pseudo-valeurs ?
Exercice 8.5 ∗ Dans les notations de la section 8.5.2 concernant l’histogramme,
écrire l’estimation fn (x) sous la forme d’une somme d’indicatrices. Dans le cas
d’une grille régulière {ak } de largeur d’intervalle h, déterminer fn (x) pour un
déplacement de la grille {ak + t} où t > 0 et t < h. Calculer la valeur moyenne
de fn (x) quand t varie de 0 à h. Montrer qu’on obtient ainsi une estimation
par noyau triangulaire (voir note 8.5).
Exercice 8.6 Dans un tableur du type EXCEL générer 50 observations de loi
N (0 ; 1). Estimer f (0) par un noyau biweight avec h = 1. Comparer à la vraie
valeur. Recommencer la procédure plusieurs fois pour conﬁrmer le type de biais
en présence.
Exercice 8.7 Dans un tableur du type EXCEL générer 50 observations de loi
N (0 ; 1). Estimer f (0) par un noyau biweight avec h = 0,5 ; 0,75 ; 1 ; 1,25 ; 1,5
pour apprécier la variabilité des estimations ainsi obtenues.
Quelle est la valeur de h asymptotiquement optimale ?
Exercice 8.8 Dans l’expression asymptotique avec h optimal de l’erreur quadratique intégrée moyenne de l’estimateur à noyau de la densité, déterminer
l’expression ν(K) qui ne dépend que du noyau. Calculer et comparer ν(K)
pour le noyau d’Epanechnikov, le noyau de Rosenblatt et le noyau de Tukey.
Exercice 8.9 A partir de l’expression asymptotique de l’erreur quadratique
intégrée moyenne de l’estimateur à noyau de la densité établir, dans le cas du
noyau de Tukey et pour une loi mère N (μ, σ2 ), que la valeur de h optimale est
hopt 2,78 σ n−1/5 et que l’e.q.i.m. correspondante est environ 0,321 σ −1 n−4/5
(aide : on utilisera l’expression de hopt établie à l’exercice précédent et les
valeurs numériques utiles concernant le noyau biweight. Par ailleurs on peut

−5
√
pour la loi de Gauss).
établir que R [f  (x)]2 dx = 3σ
8 π
Montrer de même pour l’histogramme que la valeur optimale de h donnée
par la formule asymptotique est d’environ 3,49 σ n−1/3 avec une e.q.i.m. correspondante de 0,430 σ −1 n−2/3 (aide : on prendra comme e.q.i.m. l’expression
2 
1
. On peut
intégrée de l’e.q.m. de la proposition 8.4, soit h12 R [f  (x)]2 dx + nh
 
−3
σ√
2
établir que R [f (x)] dx = 4 π pour la loi de Gauss).
Exercice 8.10 *Établir la formule du biais pour Fn (x), estimateur à noyau
intégré de F :

2
E(Fn (x)) − F (x) = h2 f  (x) R u2 K(u)du + o(h2 )
Aide : utiliser une intégration par partie pour introduire K.

Chapitre 9

Tests d’hypothèses
paramétriques
9.1

Introduction

Les tests statistiques constituent une approche décisionnelle de la statistique
inférentielle. Un tel test a pour objet de décider sur la base d’un échantillon si
une caractéristique de la loi mère (ou de la population) répond ou non à une
certaine spéciﬁcation que l’on appelle hypothèse, par exemple : la moyenne de la
loi est supérieure à 10. Ces spéciﬁcations peuvent avoir diverses provenances :
normes imposées, aﬃrmations faites par un tiers (par exemple le fabricant d’un
produit), valeurs cruciales de paramètres de modèles, etc.
Dans le cadre paramétrique où nous nous situerons initialement, les hypothèses portent sur le paramètre inconnu θ ou sur une fonction de ce paramètre
h(θ) correspondant à une caractéristique d’intérêt de la loi. Dans le cas simple
d’un espace paramétrique Θ ⊆ R, l’hypothèse spéciﬁera une valeur ou un intervalle de valeur pour θ (ou pour h(θ)). Alors qu’un intervalle de conﬁance
indique l’ensemble des valeurs plausibles, un test décidera si tel ensemble de
valeurs spéciﬁées est plausible ou non. Bien que conceptuellement distinctes
ces deux démarches reposent sur les mêmes bases mathématiques et de ce fait
nous reprendrons de nombreux éléments du chapitre 7. Nous verrons d’ailleurs
en ﬁn de chapitre que, pour tout test, on peut établir une équivalence avec
un intervalle de conﬁance. D’une façon générale il est plus facile de construire
un test que de construire un intervalle de conﬁance. Aussi la multiplicité des
tests justiﬁera-t-elle que nous approfondissions les problèmes d’optimalité. En
outre on pourra utiliser la propriété d’équivalence pour déﬁnir des procédures
d’intervalles de conﬁance dérivées de tests.
Les tests statistiques permettent d’aborder une grande variété d’hypothèses
au-delà du test d’une hypothèse portant sur un paramètre. Par exemple : la

202

Statistique − La théorie et ses applications

comparaison de plusieurs lois (ou populations), l’existence de liens entre plusieurs variables aléatoires, l’adéquation d’un modèle, etc. C’est au chapitre 10
que nous aborderons plus particulièrement des hypothèses de nature plus complexe, notamment dans le cadre non paramétrique.
Redéﬁnissons le cadre paramétrique. La loi observée est réputée appartenir
à une famille de lois décrite par la famille de densités de probabilité (respectivement de fonctions de probabilité) {f (x; θ); θ ∈ Θ}, la forme fonctionnelle f
étant connue et seul le paramètre θ étant inconnu. Θ est l’espace paramétrique
et il est inclus dans Rk où k est la dimension du paramètre θ. La fonction de
répartition est notée F (x; θ), l’échantillon est X1 , X2 , · · · , Xn et X désignera
la v.a. symbolisant la loi mère de l’échantillon.
Dans l’approche paramétrique la plus générale un test statistique consiste
à décider d’accepter ou de rejeter une hypothèse spéciﬁant que θ appartient à
un ensemble de valeurs Θ0 . Cette hypothèse de référence est appelée hypothèse
nulle et est notée H0 . A contrario on déﬁnit l’hypothèse alternative, notée H1 ,
pour laquelle θ appartient à Θ1 = Θ − Θ0 où Θ − Θ0 dénote le complémentaire
de Θ0 par rapport à Θ. En bref on identiﬁera cette situation en écrivant que
l’on teste :
H0 : θ ∈ Θ0
vs.
H1 : θ ∈ Θ1 ,
le mot vs. étant l’abréviation du latin versus. Suivant la nature de Θ0 et de Θ1
on distinguera trois cas :
– hypothèse nulle simple et alternative simple où Θ = {θ0 , θ1 } :
vs.
H1 : θ = θ 1
H0 : θ = θ0
– hypothèse nulle simple et alternative multiple :
vs.
H1 : θ = θ 0
H0 : θ = θ0
– hypothèse multiple et alternative multiple :
H0 : θ ∈ Θ0
vs.
H1 : θ ∈ Θ1 .
Pour une hypothèse nulle ou une hypothèse alternative multiple il est sousentendu qu’il y a plusieurs valeurs possibles de θ. Nous commencerons par le
premier cas qui, s’il est en réalité peu fréquent, permet de poser simplement
les notions essentielles et d’établir des résultats qui pourront être étendus aux
autres situations. Dans ce chapitre, comme pour les intervalles de conﬁance au
chapitre 7, nous introduirons tout d’abord la théorie générale pour présenter
ensuite les tests paramétriques classiques.

9.2

Test d’une hypothèse simple avec alternative simple

L’espace paramétrique Θ ne comprend donc que deux valeurs θ0 et θ1 , la
valeur θ0 étant la valeur spéciﬁée à tester, i.e. :
H0 : θ = θ0

vs.

H1 : θ = θ1 .

Chapitre 9. Tests d’hypothèses paramétriques

203

Un test pour H0 est une règle de décision fondée sur la valeur réalisée t
d’une statistique T appelée statistique de test. Sauf exception la statistique T
sera à valeurs dans R, nous le supposerons implicitement. La règle est comme
suit :
– si t ∈ A (une partie de R) on accepte H0 ,
– si t ∈ A (partie complémentaire) on rejette H0 .
La région A, qui est généralement un intervalle, sera appelée région d’acceptation et A région de rejet.
Une telle règle de décision recèle deux types d’erreur possibles du fait que
la vraie valeur du paramètre est inconnue :
– rejeter H0 alors qu’elle est vraie (i.e. θ = θ0 ) : erreur de première espèce,
– accepter H0 alors qu’elle est fausse (i.e. θ = θ1 ) : erreur de deuxième
espèce.
Étant donné que la décision se fonde sur un résultat d’origine aléatoire on
caractérisera chaque erreur par sa probabilité. En théorie de la décision une
probabilité d’erreur est appelée risque, d’où les déﬁnitions suivantes.
Déﬁnition 9.1 On appelle risque de première espèce la valeur α telle que :
α = Pθ0 (T ∈ A),
c’est-à-dire la probabilité de rejeter H0 alors qu’elle est vraie.
Il est usuel de noter cette probabilité P (T ∈ A | H0 ) même s’il ne s’agit
pas là d’une probabilité conditionnelle et, dorénavant, nous adopterons cette
notation commode. Le risque de première espèce est aussi appelé en bref risque
α.
Déﬁnition 9.2 On appelle risque de deuxième espèce la valeur β telle
que :
β = Pθ1 (T ∈ A),
c’est-à-dire la probabilité d’accepter H0 alors que H1 est vraie.
Ici également on notera cette probabilité P (T ∈ A | H1 ) et on parlera de
risque β.
Les deux risques sont interdépendants puisque l’un repose sur A et l’autre
sur son complémentaire A. Par le choix de A ou de A on peut donc vouloir
contrôler l’un ou l’autre, mais pas les deux. Dans un test statistique on privilégie
en fait le risque α que l’on se ﬁxe a priori et le plus souvent on prend α = 0,05.
C’est pourquoi la valeur α est aussi appelée le niveau ou niveau de signiﬁcation1
du test. Ce niveau ayant été choisi il s’agit de déterminer une région de rejet A
1 Ce terme de signiﬁcation est à rapprocher de la notion de test de signiﬁcativité présentée
en ﬁn de section 9.4.2.

204

Statistique − La théorie et ses applications

telle que, «sous H0 », la probabilité que T «tombe» dans A soit eﬀectivement
égale à α. On voit ainsi que la loi de la statistique de test doit être
parfaitement connue sous H0 . La construction d’un test consiste donc à
rechercher une statistique pertinente (nous expliciterons plus loin ce que nous
entendons par là) dont on connaı̂t la loi sous H0 . La région de rejet étant ainsi
déterminée, la région d’acceptation l’est aussi et donc également le risque de
deuxième espèce β. Il est essentiel de garder à l’esprit que dans une procédure
de test on contrôle le risque α mais pas le risque β. En d’autres termes, dans
un test, on souhaite avant tout limiter à un faible niveau le risque de rejeter à
tort la spéciﬁcation H0 , se souciant moins d’accepter à tort, quand H1 est vraie,
cette même spéciﬁcation. On peut encore dire que le rejet d’une hypothèse nulle
est une véritable décision alors que son acceptation est plutôt un défaut de rejet.
Face, par exemple, à une spéciﬁcation sur une caractéristique d’un produit, le
fait de rejeter cette spéciﬁcation est une preuve quasi irréfutable qu’elle n’est
pas correcte, alors que le fait de l’accepter ne signiﬁe pas qu’elle soit correcte
mais simplement que, sur la base des observations eﬀectuées, rien ne permet de
conclure qu’elle soit fausse. Notons que H0 et H1 ne sont pas interchangeables
car la construction du test, via le choix de α, repose sur H0 et non pas sur H1 .
En particulier il n’est pas nécessaire de connaı̂tre la loi de T sous H1 , ce qui
est d’ailleurs le cas pour la plupart des tests, y compris parmi les plus usuels.
Nous en venons maintenant à préciser cette idée de «pertinence» de la
statistique, tant il est vrai qu’il ne suﬃt évidemment pas de choisir n’importe
quelle statistique de loi connue sous H0 . Il est naturel de poser comme exigence
que la statistique ait une plus forte propension à tomber dans la région de rejet
quand H1 est la bonne hypothèse, ce que nous transcrivons mathématiquement
par la condition que la probabilité de rejeter H0 soit plus élevée sous H1 que
sous H0 , et si possible nettement plus élevée. Toute la recherche, intuitive ou
non, d’une bonne statistique de test repose sur ce principe que nous allons
maintenant formaliser avec la notion de puissance.
Déﬁnition 9.3 On appelle puissance d’un test la probabilité de rejeter H0
alors qu’elle est eﬀectivement fausse soit, dans les notations précédentes :
P (T ∈ A | H1 ) .
La puissance, qui est la capacité à détecter qu’une hypothèse nulle est fausse,
n’est rien d’autre que 1 − β puisque β, le risque de première espèce, est la
probabilité de l’événement complémentaire également sous H1. Nous pouvons
maintenant clairement exprimer notre exigence.
Déﬁnition 9.4 On dit qu’un test est sans biais si sa puissance est supérieure
ou égale à son risque α, soit :
P (T ∈ A | H1 ) ≥ P (T ∈ A | H0 ).

Chapitre 9. Tests d’hypothèses paramétriques

205

En conclusion une condition naturelle pour qu’une statistique soit éligible
pour tester une hypothèse est qu’elle induise un test sans biais. Incidemment
ce terme de «sans biais» n’a pas de rapport direct avec la notion de biais d’un
estimateur.
On entrevoit dès lors que le choix entre plusieurs tests potentiels, pour
une hypothèse nulle donnée, se jouera sur la puissance. Avant de préciser cela
notons qu’un test, tel que nous avons présenté les choses, est parfaitement déﬁni
par le couple : statistique de test et région d’acceptation (T, A), puisque α, β
et la puissance 1 − β en découlent (même si conceptuellement le choix de α
précède celui de A, mais pour α ﬁxé il y a diﬀérentes façons de choisir une
région - généralement un intervalle - de probabilité α sur la loi de T sous H0 ).
En vérité il n’est pas nécessaire de se référer à une statistique de test. En
eﬀet mettons en évidence la fonction de l’échantillon déﬁnissant la statistique :
T = h(X1 , X2 , · · · , Xn ) et soit A l’ensemble des points de Rn , réalisations de
(X1 , X2 , · · · , Xn ), déﬁni par :
A = {(x1 , x2 , · · · , xn ) | h(x1 , x2 , · · · , xn ) ∈ A} .
L’événement (T ∈ A), que ce soit sous H0 ou sous H1 , est identique à l’événement
((X1 , X2 , · · · , Xn ) ∈ A). Le test est donc parfaitement déﬁni par la région d’acceptation A dans Rn . D’une façon générale un test s’identiﬁe à une région
d’acceptation dans l’espace des réalisations. Cette vision plus fondamentale sera parfois utile dans les développements à venir, bien que ce ne soit
qu’une vue de l’esprit dans la mesure où une règle de décision fondée sur une
région dans un espace à n dimensions n’est pas praticable et que tout test,
ou presque, passe par une statistique à valeurs dans R avec une région d’acceptation sous forme d’un intervalle. Ayant dégagé la déﬁnition d’un test nous
pouvons aborder la comparaison de divers tests.
Déﬁnition 9.5 On dit que le test τ1 est plus puissant que le test τ2 au niveau
α s’il est de niveau α, si τ2 est de niveau égal (ou inférieur) à α et si la puissance
de τ1 est supérieure à celle de τ2 .
Il est évident que toute comparaison de puissance doit s’opérer à un même
niveau. En eﬀet pour tout test il y a un lien entre risque α et puissance : en
prenant un risque α plus élevé on agrandit la région de rejet A et, par voie
de conséquence, on augmente également la puissance. Notons aussi que le fait
de comparer τ1 à τ2 qui serait à un risque α plus faible est pénalisant pour ce
dernier, mais cette éventualité aura sa raison d’être, notamment dans le cas
discret.
L’objectif sera ﬁnalement de rechercher le test le plus puissant parmi
tous. Dans le cas où H0 et H1 sont des hypothèses simples il existe un tel test,
mais cela n’est pas nécessairement vrai dans le cas où l’hypothèse alternative

206

Statistique − La théorie et ses applications

est multiple. Par ailleurs, en général, quand une statistique de test donne le
test le plus puissant à un niveau donné elle reste optimale à tout autre niveau.
Remarques
1. Dans le cas d’une loi discrète la statistique sera elle-même discrète et
le niveau α choisi ne pourra être exactement atteint. Comme pour les
intervalles de conﬁance, si l’on souhaite un risque de première espèce de
0,05, par exemple, on recherchera une région A de probabilité, sous H0 ,
la plus proche possible mais inférieure à 0,05. On dira alors que l’on a un
test conservateur. Ceci justiﬁe, au demeurant, la comparaison de τ2 à τ1
selon la déﬁnition 9.5 à un niveau de τ2 éventuellement inférieur à celui
de τ1 .
2. Les déﬁnitions ci-dessus s’appliquent à des situations d’hypothèses multiples (et même non paramétriques) moyennant quelques précisions que
nous donnerons en temps utile.
3. Nous développons ici la théorie des tests telle qu’elle a été formalisée par
J. Neyman et E.S. Pearson autour de 1930. La pratique s’est aujourd’hui
éloignée de la théorie. En particulier, le choix a priori d’un niveau α ne
correspond pas à l’usage, sauf dans des protocoles de tests déﬁnis par
exemple par une réglementation (notamment les tests pharmaceutiques).
Il n’empêche que le cadre théorique classique reste indispensable pour
élaborer les bonnes méthodes.
4. Il est une autre exigence, outre celle de «sans biais», que l’on doit avoir
pour une bonne procédure de test, à savoir que lorsque la taille de l’échantillon tend vers l’inﬁni, la suite de tests correspondante {τn } soit telle que
la puissance βn s’accroisse et tende vers 1. En d’autres termes on doit
avoir la garantie que l’on gagne à observer de très grands échantillons,
étant pratiquement sûr, à la limite, de détecter une hypothèse nulle qui
serait fausse. On dit alors que la procédure de test est convergente.
Donnons deux exemples, certes quelque peu artiﬁciels, mais illustrant les
notions introduites, l’un dans le cas continu, l’autre dans le cas discret.
Exemple 9.1 Supposons que deux machines A et B produisent le même type
de produit, mais la machine A fournit un produit plus cher de qualité supérieure.
La qualité d’un produit se mesure à une entité aléatoire qui est de loi N (5 ; 1)
pour la machine A et N (4 ; 1) pour la machine B, et ne diﬀère donc que par
la moyenne. Un client achète le produit le plus cher par lots de 10 et désire
développer un test pour contrôler qu’un lot donné provient bien de la machine
A. Comme accuser le producteur à tort peut avoir de graves conséquences, il
doit limiter le risque correspondant et tester H0 : μ = 5 vs. H1 : μ = 4, à un
niveau 0,05 par exemple. Il semble naturel d’utiliser comme statistique de test
la moyenne X du lot. Sous H0 sa loi est N (5 ; 1/10) et l’on a alors l’intervalle

Chapitre 9. Tests d’hypothèses paramétriques

207

√
√
de probabilité 0,95 : [5−1,96/ 10 ; 5+1,96/ 10], soit [4,38 ; 5,62]. D’où une
règle de décision simple :
- accepter H0 si la réalisation x (moyenne du lot considéré) de X est dans
[4,38 ; 5,62],
- rejeter sinon.
Il est possible de calculer la puissance de ce test puisque la loi de X est
connue sous H1 : c’est la loi N (4 ; 1/10). Le risque de deuxième espèce vaut :
β = P (4,38 < X < 5,62 | H1 )
5,62 − 4
4,38 − 4
√ ) avec Z ; N (0 ; 1)
<Z<
= P( √
1/ 10
1/ 10
= P (1,20 < Z < 5,12)  0,115.
D’où une puissance d’environ 0,885. Notons que l’on peut obtenir un test
plus puissant en prenant
√ comme région d’acceptation l’intervalle de probabilité 0,95 : [5−1,645/ 10 ; +∞[ où -1,645 est le quantile d’ordre 0,05 de la
loi N (0 ; 1), soit [4,48 ; +∞[ . En eﬀet :
β = P (4,48 < X | H1 ) = P (

4,48 − 4
√
< Z) = P (1,52 < Z)  0,064 ,
1/ 10

ce qui donne une puissance de 0,936. Intuitivement on sent bien que, dans le
premier test, il est peu pertinent de borner la zone d’acceptation vers le haut
car cela conduit à rejeter l’hypothèse nulle pour de très grandes valeurs de x,
au-delà de 5,62.

Exemple 9.2 On sait que le nombre de particules émises par une source radioactive par unité de temps suit une loi de Poisson. Observant l’émission d’un
corps durant 20 unités de temps on doit décider s’il s’agit d’une source de type
A versus une source de type B. La source A émet en moyenne 0,6 particules
par unité de temps et la source B en émet 0,8. On teste donc
20H0 : λ = 0,6 vs.
H1 : λ = 0,8 . On peut construire un test sur la statistique i=1 Xi , le nombre
total de particules émises au cours des 20 unités de temps, qui suit une loi P(12)

sous H0 . Intuitivement on choisit une région de rejet de la forme 20
i=1 xi ≥ k ,
puisqu’un nombre plutôt élevé de comptages va à l’encontre de l’hypothèse
nulle. Choisissant a priori α = 0,05, on lit dans une table (ou dans un logiciel)
que pour une v.a. T ; P(12) on a P (T ≥ 18) = 0,0630 et P (T ≥ 19) = 0,0374.
20
On optera pour un test conservateur en rejetant H0 si i=1 xi ≥ 19.
20
La puissance du test est égale à P ( i=1 Xi ≥ 19 | H1 ) soit, dans une table,
P (S ≥ 19) où S ; P(16). On trouve 0,258 qui montre que le test est sans
biais.

Nous mettons maintenant en évidence le test le plus puissant.

208

9.3
9.3.1

Statistique − La théorie et ses applications

Test du rapport de vraisemblance simple
Propriété d’optimalité

Reprenons la fonction de vraisemblance du paramètre inconnu θ (voir déﬁnition 6.11) pour une réalisation de l’échantillon (x1 , x2 , · · · , xn ) :
L(θ; x1 , x2 , ..., xn ) = f (x1 , x2 , ..., xn ; θ) =

n
$

f (xi ; θ).

i=1

Nous restons dans le cadre d’une hypothèse nulle et d’une hypothèse alternative
simples, soit Θ = {θ0 , θ1 }. On supposera dans cette section que le support de
la densité f (x; θ) ne dépend pas de θ.
Déﬁnition 9.6 On appelle test du rapport de vraisemblance (RV) de l’hypothèse H0 : θ = θ0 vs. H1 : θ = θ1 au niveau α, le test déﬁni par la région de
rejet de la forme :
L(θ0 ; x1 , x2 , ..., xn )
< kα
L(θ1 ; x1 , x2 , ..., xn )
où kα est une valeur (positive) déterminée en fonction du risque de première
espèce α.
Ce test a une certaine logique intuitive puisqu’il conduit à rejeter la valeur
spéciﬁée θ0 lorsqu’elle est moins vraisemblable que la valeur alternative θ1 , car
kα (dont on admettra l’existence) se trouvera, en fait, être plus petit que 1
pour garantir un risque α faible (voir exemple 9.3). Notons que le rapport des
deux vraisemblances (que l’on nomme rapport de vraisemblance) est bien une
statistique puisque θ0 et θ1 sont donnés.
Théorème 9.1 (historiquement : lemme de Neyman-Pearson)
Le test du RV est le plus puissant quel que soit le choix de α ∈]0, 1[.
Démonstration : soit A∗ ⊂ Rn la région d’acceptation associée au test du RV
de niveau α, i.e. :
!
L(θ0 ; x1 , x2 , ..., xn )
∗
A = (x1 , x2 , ..., xn ) |
≥ kα
L(θ1 ; x1 , x2 , ..., xn )
et A celle d’un quelconque autre test. Les risques de première espèce du test du
∗
RV et de l’autre test s’écrivent2 donc, respectivement, P (A | H0 ) et P (A | H0 )
2 Il n’est pas inutile de rappeler ici la convention initiale de la section 1.1, à savoir que
pour une v.a. X quelconque P (X ∈ A) est la probabilité P (A) associée à la partie A de R.
∗
∗
Ici la probabilité P (A ), par exemple, aurait pu être notée P ((X1 , X2 , · · · , Xn ) ∈ A ) pour
n
se référer aux réalisations de l’échantillon dans R . Même si cette notation est plus explicite
nous y renonçons pour simpliﬁer les écritures.

Chapitre 9. Tests d’hypothèses paramétriques

209

et, pour eﬀectuer la comparaison, on doit avoir, en accord avec la déﬁnition
∗
9.5, P (A | H0 ) − P (A | H0 ) ≥ 0 .
Partitionnons A∗ selon A∗ ∩ A et A∗ ∩ A, et de même A selon A ∩ A∗ et
∗
A ∩ A . La diﬀérence entre les risques de deuxième espèce des deux tests est :
∗

P (A | H1 ) − P (A∗ | H1 ) = P (A ∩ A | H1 ) − P (A∗ ∩ A | H1 )
puisque les probabilités sur la partie commune A∗ ∩ A s’éliminent. Or en tout
point de A∗ , et donc de A∗ ∩ A, on a :
1
L(θ0 ; x1 , x2 , ..., xn ),
kα
1
i.e. f (x1 , x2 , ..., xn ; θ1 ) ≤
f (x1 , x2 , ..., xn ; θ0 )
kα
L(θ1 ; x1 , x2 , ..., xn ) ≤

et, par intégration (ou sommation dans le cas discret) sur le domaine A∗ ∩ A,
on a donc :
1
P (A∗ ∩ A | H1 ) ≤
P (A∗ ∩ A | H0 ).
kα
∗

∗

Pour tout point de A , et donc de A ∩ A , l’inégalité s’inverse et on obtient :
∗

P (A ∩ A | H1 ) >

1
∗
P (A ∩ A | H0 ).
kα

En revenant à l’équation de départ, il s’ensuit que :
,
1 +
∗
P (A | H1 ) − P (A∗ | H1 ) >
P (A ∩ A | H0 ) − P (A∗ ∩ A | H0 ) .
kα
Considérant les partitions indiquées plus haut, le terme de droite est aussi égal
à k1α [P (A | H0 ) − P (A∗ | H0 )], lequel est lui-même égal à :
,
,
1 +
1 +
∗
∗
1 − P (A | H0 ) − 1 + P (A | H0 ) =
P (A | H0 ) − P (A | H0 )
kα
kα
qui, par hypothèse, est positif ou nul. Ainsi le risque β d’un test quelconque
est strictement supérieur à celui du test du RV et, de façon équivalente, il est
donc moins puissant.

Note 9.1 Dans le cas discret la démonstration du théorème peut poser problème.
∗
En eﬀet on n’a pas nécessairement P (A | H0 ) − P (A | H0 ) ≥ 0 dans la mesure où le
risque de première espèce réel du test du RV peut, par conservatisme, être inférieur
au niveau nominal α, alors que celui de l’autre test peut être plus proche de α. En
fait le même résultat d’optimalité peut être démontré à condition de «randomiser»
la règle de décision (test randomisé ou test mixte).
Soit ka la valeur qui donne une probabilité de rejet sous H0 aussi proche que possible du niveau nominal α mais inférieure (exceptionnellement égale) à α, ka + 1

210

Statistique − La théorie et ses applications

donnant alors une probabilité supérieure à α. La randomisation consiste à choisir la limite kα avec une probabilité p et ka + 1 avec probabilité 1 − p. On doit
déterminer p pour que ﬁnalement le risque résultant soit exactement α. Appliqué à
20
l’exemple 9.2 ce procédé conduirait à rejeter selon la règle
i=1 xi ≥ 19 avec proba20
bilité p et selon
i=1 xi ≥ 18 avec probabilité 1 − p, la valeur de p étant telle que
p×0,0374+(1 − p)×0,0630=0,05 soit p =0,51. A peu de choses près on doit jouer à
pile ou face le choix de la règle avec 19 ou celui de la règle avec 18.
probabilité exactement
On peut aussi ajuster son choix de α en prenant une 
20
atteinte (dans l’exemple ci-dessus on pourra prendre la règle
i=1 xi ≥ 19 en ﬁxant
α = 0, 0374). Alors la démonstration ci-dessus est valide.

Note 9.2 On a dû supposer que la densité ait un support indépendant de
θ pour éviter que L(θ1 ; x1 , x2 , ..., xn ) s’annule alors que L(θ0 ; x1 , x2 , ..., xn )
ne s’annule pas. On peut toutefois contourner ce problème en déﬁnissant la
région de rejet par L(θ0 ; x1 , x2 , ..., xn ) < k L(θ1 ; x1 , x2 , ..., xn ). Si
L(θ1 ; x1 , x2 , ..., xn ) s’annule on a une réalisation impossible sous H1 et l’on
peut choisir θ0 sans aucun risque d’erreur.
Proposition 9.1 Le test du RV est sans biais.
Démonstration : pour rester très général ne supposons pas que kα soit inférieur
à 1. Si kα ≥ 1 on a, pour tout point de la région d’acceptation A∗ :
f (x1 , x2 , ..., xn ; θ0 ) ≥ f (x1 , x2 , ..., xn ; θ1 )
∗

∗

et, par conséquent, P (A∗ | H0 ) ≥ P (A∗ | H1 ) d’où P (A | H0 ) ≤ P (A | H1 ) qui
est la condition requise. Inversement, si kα < 1, on a, pour tout point de la
∗
région de rejet A :
f (x1 , x2 , ..., xn ; θ0 ) < f (x1 , x2 , ..., xn ; θ1 )
∗

∗

et, par conséquent, il est vrai aussi que P (A | H0 ) < P (A | H1 ).



Moyennant des conditions mineures on peut également démontrer que la
procédure du RV est convergente.
Ayant mis en évidence le test le plus puissant se pose la question de la
faisabilité de ce test. En eﬀet pour qu’il puisse être mis en oeuvre il faut que la
statistique du rapport de vraisemblance prenne une forme telle que sa loi soit
connue sous H0 . Montrons sur l’exemple 9.1 que le test du RV peut se ramener
à une forme simple ce que nous démontrerons ensuite pour les lois de la classe
exponentielle.

Chapitre 9. Tests d’hypothèses paramétriques

211

Exemple 9.3 Dans le contexte de l’exemple 9.1 la fonction de vraisemblance
pour μ est :
n
$

1
1
√ exp{− (xi − μ)2 }
2
2π
i=1
n

n
1
1
√
=
exp{−
(xi − μ)2 }.
2 i=1
2π

L(μ; x1 , x2 , · · · , xn ) =

Pour être plus général considérons H0 : μ = μ0 vs. H1 : μ = μ1 d’où le
rapport de vraisemblance :
"
 n
#
n

1 
L(μ0 ; x1 , x2 , · · · , xn )
2
2
= exp −
(xi − μ0 ) −
(xi − μ1 )
L(μ1 ; x1 , x2 , · · · , xn )
2 i=1
i=1

"
#
n

1
2
2
= exp − −2(μ0 − μ1 )
xi + n(μ0 − μ1 )
.
2
i=1
Le
ntravers
nrapport de vraisemblance ne dépend donc des observations qu’à
x
.
De
plus
comme
c’est
une
fonction
croissante
de
(μ
−
μ
)
0
1
i=1 i
i=1 xi il
n
est inférieur à kα si et seulement si (μ0 − μ1 ) i=1 xi < kα où kα est une autre
constante que l’on
n déduit aisément de kα . Si μ0 > μ1 alors la région de rejet
est de la forme i=1 xi < kα ou, de façon équivalente, x < kα . Si μ0 < μ1 les
inégalités doivent être inversées.
Dans l’exemple 9.1 on a vu que, pour α = 0,05 , μ0 = 5 et μ1 = 4, la région
de rejet était déﬁnie par x <4,48 pour le deuxième test envisagé, lequel s’avère
être le test le plus puissant.
Ainsi, bien que la statistique du rapport de vraisemblance
n elle-même ne soit
pas simple, le fait qu’elle soit fonction monotone de i=1 Xi dont la loi est
connue suﬃt pour mettre au point le test. Par curiosité calculons la constante
kα propre au rapport de vraisemblance. Comme n = 10 on a kα = 44,8 = kα
puisque μ0 − μ1 = 1. Alors :
1
kα = exp − [−2 × 44,8 + 10(25 − 16)]
2

!
= 0,82.

Ceci signiﬁe que le test du RV consiste à rejeter H0 : μ = 5 vs. H1 : μ = 4
lorsque la vraisemblance de la valeur 5 du paramètre inconnu μ est 0,82 fois
celle de la valeur 4.
Notons encore que si l’on avait eu μ0 < μ1 la région de rejet aurait été de

la forme x > c, ce qui correspond à l’intuition.

212

9.3.2

Statistique − La théorie et ses applications

Cas d’un paramètre de dimension 1

Nous montrons ici que le test du RV se ramène à un test simple, comme
dans l’exemple ci-dessus, dans une grande variété de situations.
Proposition 9.2 S’il existe une statistique T = t(X1 , X2 , · · · , Xn ) exhaustive
minimale à valeurs dans R alors le rapport de vraisemblance ne dépend de la
réalisation (x1 , x2 , ..., xn ) qu’à travers la valeur t(x1 , x2 , ..., xn ). De plus si ce
rapport de vraisemblance est une fonction monotone de t(x1 , x2 , ..., xn ) alors
le test du RV se ramène à un test dont la région de rejet est de la forme
t(x1 , x2 , ..., xn ) < c si c’est une fonction croissante ou t(x1 , x2 , ..., xn ) > c si la
fonction est décroissante.
En eﬀet, par le théorème de factorisation 6.1, si T = t(X1 , X2 , · · · , Xn )
est une statistique exhaustive minimale alors l’expression de la vraisemblance
L(θ; x1 , x2 , · · · , xn ) qui est identique à l’expression de la densité conjointe est
de la forme g(t(x1 , x2 , ..., xn ); θ) h(x1 , x2 , ..., xn ) et le RV ne dépend plus que du
rapport g(t(x1 , x2 , ..., xn ); θ0 ) / g(t(x1 , x2 , ..., xn ); θ1 ). Si g(u; θ0 ) / g(u; θ1 ) est
une fonction monotone de u alors l’inégalité g(u; θ0 ) / g(u; θ1 ) < k est équivalente à une inégalité sur u.
Proposition 9.3 Si la loi mère est dans une famille appartenant à la classe
exponentielle, i.e. f (x; θ) = a(θ) b(x) exp{c(θ) d(x)} (voir section 6.3), alors le
test du RV a une région de rejet de la forme :
n


ou

i=1
n


d(xi ) < k

si c(θ0 ) − c(θ1 ) > 0

d(xi ) > k

si c(θ0 ) − c(θ1 ) < 0.

i=1

nCette proposition est un corollaire de la précédente puisque l’on a vu que
i=1 d(Xi ) est exhaustive minimale
 (voir proposition 6.5). Le RV étant égal à
[a(θ0 )/a(θ1 )]n exp{[c(θ0 ) − c(θ1 )] ni=1 d(xi )} on voit que le sens del’inégalité
n
dépendra du signe de c(θ0 )−c(θ1 ). Notons que très souvent la loi de i=1 d(Xi )
est de type connu et k sera donc le quantile d’ordre α de cette loi sous H0 ou
d’ordre 1 − α, selon que le signe est positif ou négatif. On pourra également calculer la puissance de ce test optimal. Les exemples 9.1 et 9.2 (loi de Gauss de variance connue et loi de Poisson) correspondaient à cette situation. Considérons
encore le cas d’une loi de Bernoulli.
Exemple 9.4 Soit le test H0 : p = p0 vs. H1 : p = p1 pour le paramètre p
d’une loi de Bernoulli. Cette famille de lois appartient à la classe exponentielle
p
et l’on a : f (x; p) = px (1−p)1−x = exp{ln( 1−p
) x} pour x ∈ {0, 1}. Donc d(x) =
p0
p1
> ln 1−p
et la région
x et, supposant par exemple que p0 > p1 , on a ln 1−p
0
1
n
de rejet est de la forme i=1 xi < k ou préférablement, comme nous sommes

Chapitre 9. Tests d’hypothèses paramétriques

213

n
n
dans le cas discret, i=1 xi ≤ k . La statistique de test i=1 Xi suit une loi
B(n, p) et pour α = 0,05 , k  est la valeur égale ou immédiatement inférieure au
quantile d’ordre 0,05 sur la loi B(n, p0 ). La puissance est la probabilité d’être
inférieur ou égal à k  pour la loi B(n, p1 ).
Soit par exemple H0 : p = 0,5 vs. H1 : p = 0,3 à tester au niveau 0,05
avec un échantillon
ntable binomiale pour la loi
n de taille 30. On lit dans une
=
0,0494
et
P
(
B(30 ; 0,5) : P ( i=1 xi ≤ 10)
i=1 xi ≤ 11) = 0,1003. On

choisit donc la
règle de rejet ni=1 xi ≤ 10. Pour la puissance on lit sur la loi
n

B(30 ;0,3) : P ( i=1 xi ≤ 10) = 0,730 .
Le cas de deux hypothèses simples, nous l’avons dit, est peu réaliste et il
nous faut maintenant envisager des situations plus générales.

9.4

Tests d’hypothèses multiples

9.4.1

Risques, puissance et optimalité

Lorsque l’une des hypothèses H0 ou H1 est multiple les déﬁnitions de la
section 9.2 doivent être revues. En eﬀet si dans une hypothèse plusieurs valeurs
du paramètre sont possibles il n’y a plus de risque unique. Ainsi une expression
telle que P (T ∈ A | H0 ) n’a pas de sens si H0 est multiple.
Plaçons-nous dans le cas le plus général où l’on souhaite tester :
H0 : θ ∈ Θ0

vs.

H1 : θ ∈ Θ1 ,

où Θ1 = Θ − Θ0 est le complémentaire de Θ0 par rapport à Θ. Comme
précédemment, de la façon la plus générale, un test est déﬁni par une région
A ⊂ Rn d’acceptation de l’hypothèse nulle H0 . Nous supposerons ici, comme
cela se trouve en pratique, que cette région se réduit à un intervalle A de R
pour une statistique de test T. Alors la règle de décision consiste à accepter
H0 si la valeur réalisée t de T appartient à A et à rejeter H0 sinon. Si H0 est
multiple le risque de première espèce Pθ (T ∈ A) dépend de θ appartenant à Θ0 .
Le niveau du test est alors déﬁni comme le risque maximal que l’on encourt à
rejeter H0 alors qu’elle serait fausse.
Déﬁnition 9.7 Soit H0 : θ ∈ Θ0 une hypothèse nulle multiple et α(θ) le risque
de première espèce pour la valeur θ ∈ Θ0 . On appelle niveau du test (ou seuil
du test) la valeur α telle que :
α = sup α(θ) .
θ∈Θ0

De même si l’hypothèse alternative H1 : θ ∈ Θ1 est multiple le risque de
deuxième espèce est une fonction β(θ) ainsi que la puissance. On déﬁnit alors
la fonction puissance du test :
h(θ) = 1 − β(θ) = Pθ (T ∈ A) déﬁnie pour tout θ ∈ Θ1 .

Statistique − La théorie et ses applications

214

Déﬁnition 9.8 On dit qu’un test est sans biais si sa fonction puissance reste
supérieure ou égale à son niveau α, soit :
Pθ (T ∈ A) ≥ α pour tout θ ∈ Θ1 .
En d’autres termes, la probabilité de rejeter H0 si elle est fausse, quelle
que soit la valeur de θ dans Θ1 , est toujours plus élevée que la probabilité de
la rejeter si elle est vraie, quelle que soit alors la valeur de θ dans Θ0 .
Déﬁnition 9.9 On dit que le test τ1 de niveau α est uniformément plus
puissant que le test τ2 au niveau α s’il est de niveau α, si τ2 est de niveau
égal (ou inférieur) à α et si la fonction puissance de τ1 reste toujours supérieure
ou égale à celle de τ2 , mais strictement supérieure pour au moins une valeur
de θ ∈ Θ1 , i.e. pour tout θ ∈ Θ1 , h1 (θ) ≥ h2 (θ) et il existe θ ∗ ∈ Θ1 tel que
h1 (θ∗ ) > h2 (θ∗ ), où h1 (θ) et h2 (θ) sont les fonctions puissance respectives des
tests τ1 et τ2 .
Le terme «uniformément» se rapporte au fait que la puissance de τ1 est
supérieure quelle que soit θ ∈ Θ1 .
Déﬁnition 9.10 On dit que le test τ ∗ est uniformément le plus puissant
(UPP) au niveau α s’il est uniformément plus puissant que tout autre test au
niveau α.
Rien ne dit qu’un tel test existe. En eﬀet il se peut, par exemple, qu’un
premier test domine tous les autres pour certaines valeurs de θ dans Θ1 , qu’un
deuxième soit le meilleur pour d’autres valeurs, etc. Signalons que certains ouvrages en français parlent de test UMP (de l’anglais uniformly most powerful ).
Dans la situation la plus générale il n’existera généralement pas de test UPP.
Néanmoins le résultat de Neyman-Pearson obtenu dans la situation simple de la
section 9.3 s’étend assez naturellement à des situations d’hypothèses multiples
dites unilatérales, très fréquentes en pratique.

9.4.2

Tests d’hypothèses multiples unilatérales

Nous considérons dans cette section des situations de test du type :
H0 : θ ≤ θ0
ou H0 : θ ≥ θ0

vs.
vs.

H1 : θ > θ 0
H1 : θ < θ 0

où θ est donc un paramètre de dimension 1 (Θ ⊆ R). Hypothèse nulle et
hypothèse alternative sont multiples. De telles situations se rencontrent lorsque
l’on s’intéresse uniquement à juger si le paramètre θ dépasse un certain seuil
(par exemple une norme de qualité, un seuil de pollution, un niveau antérieur,
etc.). L’hypothèse nulle est dite unilatérale et par extension on parle aussi de
test unilatéral du fait que la région de rejet est usuellement de la forme T > c
ou T < c, T étant la statistique de test.

Chapitre 9. Tests d’hypothèses paramétriques

215

Proposition 9.4 S’il existe une statistique T = t(X1 , X2 , · · · , Xn ) exhaustive
minimale à valeurs dans R et si, pour tout couple (θ, θ ) tel que
θ < θ  , le rapport de vraisemblance L(θ; x1 , x2 , ..., xn )/L(θ  ; x1 , x2 , ..., xn ) est
une fonction monotone de t(x1 , x2 , · · · , xn ), alors il existe un test uniformément
le plus puissant pour les situations d’hypothèses unilatérales et la région de rejet
est soit de la forme t(x1 , x2 , · · · , xn ) < k, soit de la forme t(x1 , x2 , · · · , xn ) > k.
En proposition 9.2 on a vu que le RV ne pouvait dépendre que de
t(x1 , x2 , · · · , xn ) et que, dans le cas où l’hypothèse nulle et l’hypothèse alternative sont simples, il suﬃsait, pour se ramener à une région de rejet de la forme
t(x1 , x2 , · · · , xn ) < k ou t(x1 , x2 , · · · , xn ) > k, que le RV soit monotone pour
les deux valeurs de θ concernées, l’existence d’un test le plus puissant étant
quoi qu’il en soit assurée. Ici la monotonicité du RV pour tout couple (θ, θ ) est
requise aﬁn, d’une part, de garantir l’existence d’un test UPP et, d’autre part,
de ramener ce test à une simple inégalité sur t(x1 , x2 , · · · , xn ).
Démonstration de la proposition : prenons le cas où le RV est une fonction
croissante de t(x1 , x2 , · · · , xn ) et pour H0 : θ ≤ θ0 vs. H1 : θ > θ0 .
Montrons tout d’abord que Pθ (T < k) croı̂t avec θ. Considérons le test
simple ﬁctif : H0 : θ = θ vs. H1 : θ = θ avec θ  < θ  . Le test à région de rejet
t(x1 , x2 , · · · , xn ) < k est équivalent au test du RV avec
L(θ ; x1 , x2 , ..., xn )/L(θ ; x1 , x2 , ..., xn ) < k 
et est donc sans biais (voir proposition 9.1), d’où Pθ (T < k) ≤ Pθ (T < k)
quels que soient θ et θ  tels que θ < θ  .
Supposons maintenant que nous choisissions t(x1 , x2 , · · · , xn ) < k comme
région de rejet pour tester H0 : θ ≤ θ0 vs. H1 : θ > θ0 . Alors Pθ (T < k)
correspond à la probabilité de rejet pour une valeur quelconque θ. En particulier
le risque de première espèce croı̂t pour θ ∈] − ∞ , θ0 ] et le risque maximal
est donc atteint en θ0 . Par conséquent, pour obtenir un niveau α il suﬃt de
choisir k tel que Pθ0 (T < k) = α. On notera au passage que Pθ (T < k), pour
θ ∈ ]θ0 , +∞ [, déﬁnit la fonction puissance laquelle est également croissante, et
ceci au fur et à mesure que l’on s’éloigne de θ0 .
Du fait que le risque de première espèce est maximal en θ0 , on peut se
' 0 : θ = θ0 vs. H1 : θ > θ0 .
contenter d’étudier la situation de test restreinte H

Soit θ une valeur dans H1 , alors t(x1 , x2 , · · · , xn ) < k équivaut à :
L(θ0 ; x1 , x2 , ..., xn )/L(θ ; x1 , x2 , ..., xn ) < k ,
qui correspond au test du RV simple de puissance h(θ  ) supérieure à celle de
tout autre test en vertu du théorème 9.1. Ceci restant vrai quel que soit θ dans

H1 , le test t(x1 , x2 , · · · , xn ) < k est bien uniformément le plus puissant.
La démonstration a été faite dans un cas particulier, mais elle est analogue
pour les trois autres cas possibles. Pour la situation H0 : θ ≥ θ0 vs. H1 : θ < θ0

Statistique − La théorie et ses applications

216

l’inégalité sur t(x1 , x2 , · · · , xn ) doit être inversée par rapport à la situation
précédente car la région de rejet L(θ0 ; x1 , x2 , ..., xn )/L(θ  ; x1 , x2 , ..., xn ) < k
est alors déﬁnie avec une valeur θ0 supérieure à θ et le RV change donc de sens
de variation par rapport à t(x1 , x2 , · · · , xn ). En résumé on a les régions de rejet
suivantes :
1.
2.
3.
4.

H0
H0
H0
H0

:θ
:θ
:θ
:θ

≤ θ0
≤ θ0
≥ θ0
≥ θ0

et
et
et
et

RV
RV
RV
RV

fonction
fonction
fonction
fonction

croissante de t(x1 , · · · , xn ) : t(x1 , · · · , xn ) < k
décroissante de t(x1 , · · · , xn ) : t(x1 , · · · , xn ) > k
croissante de t(x1 , · · · , xn ) : t(x1 , · · · , xn ) > k
décroissante de t(x1 , · · · , xn ) : t(x1 , · · · , xn ) < k .

Notons que la propriété établie en préambule de la démonstration s’étend
aux trois autres cas : la probabilité de rejet est une fonction monotone
du paramètre θ, le risque de première espèce est maximal en θ0 , la
fonction puissance croı̂t quand on s’éloigne de θ0 (voir illustration de
l’exemple 9.5 et ﬁgure 9.1).
Certains auteurs considèrent la situation H0 : θ = θ0 vs. H1 : θ > θ0 . On
a vu au cours de la démonstration que celle-ci est équivalente à la situation
H0 : θ ≤ θ0 vs. H1 : θ > θ0 pour le test UPP.
Une famille de densité qui vériﬁe les conditions de la proposition 9.4 est dite
être à rapport de vraisemblance monotone. Si elle appartient
à la classe
n
exponentielle son RV est égal à [a(θ)/a(θ )]n exp{[c(θ)−c(θ )] i=1 d(xi )} (voir
démonstration de la proposition 9.3). Dans cette classe une condition nécessaire
et suﬃsante pour remplir ces conditions est donc que c(θ)−c(θ ) garde le même
signe quel que soit le couple (θ, θ  ) tel que θ < θ , c’est-à-dire que la fonction
c(θ) soit monotone.
De plus la fonction des observations t(x1 , x2 , · · · , xn ) de la
proposition 9.4 est ni=1 d(xi ), d’où la proposition suivante.
Proposition 9.5 Si la loi mère est dans une famille appartenant à la classe
exponentielle, i.e. f (x; θ) = a(θ)b(x)exp{c(θ)d(x)}, et si la fonction c(θ) est
monotone, alors il existe un test uniformément le plus puissant pour les situations d’hypothèses unilatéraleset la région de rejet est soit de la forme

n
n
i=1 d(xi ) < k, soit de la forme
i=1 d(xi ) > k.

Exemple 9.5 Comme dans les exemples 9.1 et 9.3 considérons une loi mère
N (μ , 1) où μ est inconnu. Testons H0 : μ ≤ μ0 vs. H1 : μ > μ0 . La densité de
la loi mère est :
1
1
1
1
1
f (x; μ) = √ exp{− (x − μ)2 } = √ exp{− (x2 )} exp{− μ2 } exp{μx}.
2
2
2
2π
2π
Ici c(μ) = μ et d(x)

n = x. Le test UPP a donc une région de rejet de la forme
n
x
>
k
ou
i=1 i
i=1 xi < k. Pour trouver le sens correct de l’inégalité il faut
repartir du rapport de vraisemblance L(μ; x1 , x2 , ..., xn )/L(μ ; x1 , x2 , ..., xn ) qui,

Chapitre 9. Tests d’hypothèses paramétriques

217

n
comme il a été établi dans l’exemple 9.3, varie comme exp{(μ
− μ ) i=1 xi }

n
fonction décroissante de i=1 xi . Ainsi un RV
et, pour μ < μ , est donc une 
n

inférieur à k est équivalent à i=1 xi > k ou encore à x > k . Ceci déﬁnit
la région de rejet puisque nous sommes dans le cas 2 ci-dessus. Notons qu’on
pouvait trouver intuitivement le sens de l’inégalité du fait qu’une moyenne
empirique élevée abonde dans le sens de H1 à l’encontre de H0 .
Dans l’hypothèse nulle on sait que le risque de première espèce est le √
plus
élevé en μ0 . Pour cette valeur du paramètre μ, X est de loi N (μ0 , 1/ n).

Pour un niveau 0,05 la constante k  est déﬁnie par
√ Pμ0 (X > k ) = 0,05
√ et
est donc égale au quantile 0,95 de la loi N (μ0 , 1/ n), soit μ0 +1,645(1/
n).
√
(X
>
μ
+1,645(1/
n))
pour
La fonction puissance
est
déﬁnie
par
h(μ)
=
P
μ √
0
√
X ; N (μ, 1/ n) et μ ∈]μ0 , +∞[. En posant Z = n(X − μ) on obtient :
√
h(μ) = P (Z > 1, 645 − n(μ − μ0 ) )
√
où Z ; N (0 ; 1). Comme μ − μ0 > 0 la valeur 1,645− n(μ − μ0 ) tend vers −∞
quand n tend vers l’inﬁni, donc h(μ) tend vers 1, ce qui démontre la convergence
du test.
Pour illustrer cela considérons un produit dont une certaine mesure de qualité est, selon le producteur, inférieure ou égale à 5 en moyenne (par exemple la
teneur en lipides d’un aliment allégé) et soit un test s’appuyant sur échantillon
de taille 10. On considère toujours que l’écart-type de la variable qualité est
connu et égal à 1. Pour un niveau de test 0,05 on rejettera H0 si la teneur
moyenne observée
√ sur les 10 produits (tirés au hasard) est supérieure ou égale
à 5+1,645(1/ 10) = 5,52. La fonction puissance est obtenue en calculant
1
) pour μ ∈]5, +∞[, c’est-à-dire en calcuPμ (X >5,52) avec X ; N (μ , 10
√
√
lant P (Z > 10(5,52−μ) ) où Z ; N (0 ; 1), soit 1 − Φ( 10(5,52−μ)) où Φ
est la fonction de répartition de cette loi. Cette valeur croı̂t avec μ comme le
montre la ﬁgure 9.1. A gauche de μ = 5 il s’agit du risque de première espèce.
Cette croissance est caractéristique des familles à rapport de vraisemblance
monotone.

Exemple 9.6 Soit la famille de lois exponentielles {f (x; λ) = λe−λx ; x ≥ 0,
λ > 0}. On a c(λ) =
−λ et d(x) = x. Pour H0 : λ ≥ λ0 vs. H1 : λ < λ0 le
n
test UPP repose sur i=1 xi ou sur x. Pour trouver le sens de l’inégalité on
rappelle que la moyenne de la loi est 1/λ. Ainsi une forte valeur observée pour
x doit reﬂéter une forte valeur de 1/λ, soit une faible valeur de λ, ce qui est du
côté de H1 . La région de rejet est donc de la forme x > k.
La valeur de k 
doit être telle que Pλ0 (X > k) = α pour obtenir le niveau
n
α. Or, si λ = λ0 , i=1 Xi suit une loi Γ(n, λ0 ) (voir section 4.2.3). Comme
Pλ0 (X > k) = Pλ0 ( ni=1 Xi > nk), nk est le quantile d’ordre 1 − α sur la loi

Γ(n, λ0 ), d’où l’on déduit k.
Pour la loi de Bernoulli B(p) qui régit notamment le test sur une proportion
p
qui est une fonction croissante de p et
dans une population, on a c(p) = ln 1−p

Statistique − La théorie et ses applications

218

1

H0 : μ ≤ 5

H1 : μ > 5

α=0,05

0
4,5

5

5,5

6

6,5

Figure 9.1 - Fonction puissance pour H0 : μ = 5 sur une loi N (μ , 1).

d(x) = x (voir
n exemple 9.4). Le test UPP de H0 : p ≤ p0 vs. H1 : p > p0 repose
donc sur i=1 xi qui est le nombre de «succès» observé. On rejette H0 si ce
nombre est trop élevé (car cela fait pencher vers H1 ) et la valeur critique k se
lit sur la loi BN (n , p0 ) comme dans l’exemple 9.4.
Pour la loi de Poisson P(λ) on a c(λ) = ln λ qui est une fonction croissante
de λ et d(x) = x 
(voir exemple 6.2). Pour déterminer la valeur critique k on
utilise le fait que ni=1 Xi suit une loi P(nλ). L’examen de la loi de Pareto est
proposé dans les exercices.
L’existence d’un test UPP n’est pas réservée à la classe exponentielle. Pour
la famille des lois uniformes U[0, θ] on a vu que le maximum X(n) est statistique
exhaustive minimale. On peut montrer qu’elle est à RV monotone et il existe
donc un test UPP de la forme X(n) < k ou X(n) > k (voir exercices).
Remarque 9.1

Choix de H0

Face à une situation pratique le choix du sens de H0 (θ ≤ θ0 ou θ ≥ θ0 ) n’est
pas toujours évident. Il devra se faire en considérant les deux erreurs possibles.
On doit faire en sorte que celle qui est jugée la plus grave soit une erreur de
première espèce : aﬃrmer que H0 est fausse (donc que l’on choisit H1 ) alors

Chapitre 9. Tests d’hypothèses paramétriques

219

qu’elle est vraie. En d’autres termes l’aﬃrmation la plus sensible doit correspondre à H1 . Dans l’illustration de l’exemple 9.5 on peut commettre une erreur
soit en déclarant que la teneur n’est pas respectée alors qu’elle l’est pourtant,
soit en déclarant qu’elle est respectée alors qu’elle ne l’est pas. La première erreur est beaucoup plus préjudiciable pour la personne eﬀectuant le test (on ne
veut pas accuser à tort). Aussi H1 doit-elle exprimer le fait que la teneur n’est
pas respectée, soit H1 : μ > 5 d’où H0 : μ ≤ 5. Supposons qu’un médicament
soit considéré eﬃcace si un paramètre θ dépasse un seuil θ0 . On peut déclarer
le médicament eﬃcace alors qu’il ne l’est pas ou le déclarer ineﬃcace alors
qu’il est eﬃcace. La première de ces erreurs est plus critique car elle aura pour
conséquence de mettre sur le marché un médicament inutile, alors que pour la
deuxième le médicament ne sera pas diﬀusé par mesure conservatoire. H1 doit
donc correspondre au fait que le médicament est eﬃcace, soit θ > θ0 d’où H0 :
θ ≤ θ0 .
Notons que dans la théorie classique développée ici la notion de risque de
première ou de deuxième espèce suppose que l’hypothèse nulle et, donc, l’hypothèse alternative aient été posées avant d’observer les données. En pratique il
est fréquent que l’on décide du sens du rejet sur la base même des observations.
Cette façon de faire est à rapprocher de l’usage de la P-valeur décrit en section
9.6 ou encore de ce qu’on appelle parfois un test de signiﬁcativité.

9.4.3

Tests d’hypothèses bilatérales

Nous considérons deux situations du type bilatéral :
H0 : θ = θ 0

vs.

ou H0 : θ1 ≤ θ ≤ θ2

H1 : θ = θ0
vs.

H1 : θ < θ1 ou θ > θ2

où θ est un paramètre de dimension 1 (Θ ⊆ R). La première situation est
fréquente lorsque θ représente en fait un écart entre paramètres de deux populations, par exemple entre leurs moyennes (voir section 9.7.3). La seconde teste
si le paramètre est situé dans un intervalle de tolérance acceptable. L’appellation de bilatéral se réfère au fait que l’alternative est située de part et d’autre
de l’hypothèse nulle.
On ne peut s’attendre dans ces situations à obtenir un test UPP du fait
qu’il faut faire face à des alternatives à la fois du type θ < θ0 et du type
θ > θ0 , par exemple pour le premier cas. Toutefois il pourra y avoir un test
uniformément plus puissant dans la classe restreinte des tests sans biais, en
bref UPP-sans biais. Ceci est notamment vrai pour les familles de lois de la
classe exponentielle. D’une façon générale la région d’acceptation aura la forme
, où c1 < c2 , pour la réalisation d’une statistique de test T approc1 < t < c2
n
priée (soit i=1 d(Xi ) pour la classe exponentielle). Ceci justiﬁe l’appellation
fréquente de test bilatéral puisqu’on est amené à un rejet sur les deux extrémités.
Nous rencontrerons de telles situations dans la section sur les tests usuels.

Statistique − La théorie et ses applications

220

Note 9.3 L’usage veut que l’on détermine les valeurs critiques c1 et c2 en
répartissant α/2 sur chaque extrémité. Ainsi, pour le cas H0 : θ = θ0 , ces
valeurs seront telles que Pθ0 (T < c1 ) = Pθ0 (T > c2 ) = α/2. Mais cette règle ne
conduit pas au test UPP-sans biais si la loi de T n’est pas symétrique (le test
peut même ne plus être sans biais). Dans la classe exponentielle la répartition
doit être telle que la dérivée par rapport à θ de Pθ (T < c1 ) + Pθ (T > c2 )
s’annule en θ0 .
Pour le cas H0 : θ1 ≤ θ ≤ θ2 la condition est que le seuil α soit atteint à la
fois en θ1 et en θ2 . La résolution de tels problèmes n’est pas simple et, dès lors,
la règle de répartition égale apparaı̂t bien commode (une situation de test de
ce type sera envisagée dans l’exemple 9.8).
Nous n’approfondirons pas la recherche de tests optimaux dans les cas bilatéraux et nous nous contenterons de présenter un test de portée générale,
inspiré du rapport de vraisemblance simple, s’appliquant aux situations les
plus complexes et, en particulier, aux hypothèses bilatérales ci-dessus.

9.5

Test du rapport de vraisemblance
généralisé

Considérons maintenant les hypothèses paramétriques les plus générales.
Déﬁnition 9.11 Soit la famille paramétrique {f (x; θ), θ ∈ Θ}, où Θ ⊆ Rk ,
et les hypothèses H0 : θ ∈ Θ0 vs. H1 : θ ∈ Θ1 où Θ1 = Θ − Θ0 est le
complémentaire de Θ0 par rapport à Θ. On appelle rapport de vraisemblance
généralisé (RVG), la fonction λ(x1 , x2 , ..., xn ) telle que :
sup L(θ; x1 , x2 , ..., xn )

λ(x1 , x2 , ..., xn ) =

θ∈Θ0

supL(θ; x1 , x2 , ..., xn )

θ∈Θ

et test du RVG, le test déﬁni par une région de rejet de la forme :
λ(x1 , x2 , ..., xn ) < k ≤ 1.
Il est évident que λ(x1 , x2 , ..., xn ) est inférieur ou égal à 1 pour toute
réalisation (x1 , x2 , ..., xn ). De plus, s’il existe une estimation du maximum de
vraisemblance θM V , et c’est pour ainsi dire toujours le cas (voir section 6.7.1),
alors le dénominateur est la valeur de la fonction de vraisemblance en θM V ,
soit : L(θM V ; x1 , x2 , ..., xn ).
Le RVG relève de la même rationalité que le RV simple. Si, pour une
réalisation donnée, la vraisemblance atteint un maximum dans H0 qui reste
bien inférieur à son maximum absolu dans tout l’espace paramétrique Θ, alors
il y a lieu de douter de cette hypothèse.

Chapitre 9. Tests d’hypothèses paramétriques

221

Vériﬁons que dans le cas d’hypothèses nulle et alternative simples, le test
du RVG est équivalent au test du RV simple. Le dénominateur du RV est
L(θ1 ; x1 , x2 , ..., xn ) alors que celui du RVG est la valeur maximale entre
L(θ0 ; x1 , x2 , ..., xn ) et L(θ1 ; x1 , x2 , ..., xn ). Pour les réalisations (x1 , x2 , ..., xn )
telles que le RV simple est strictement inférieur à 1, cette valeur maximale est
donc atteinte pour θ1 et le RV simple est égal au RVG et, réciproquement, si le
RVG est strictement inférieur à 1 alors il en va de même pour le RV. Les régions
de test λ(x1 , x2 , ..., xn ) < k et L(θ0 ; x1 , x2 , ..., xn )/L(θ1 ; x1 , x2 , ..., xn ) < k sont
donc identiques. Toutefois si le RV simple est supérieur à 1 alors la valeur
maximale est atteinte pour θ0 , le RVG reste égal à 1 et il n’y a donc pas
équivalence sur un plan strictement mathématique. Cependant il est clair qu’en
règle générale le test du RV simple n’a de sens que s’il se fonde sur une valeur
de k inférieure à 1, ceci pour garantir un risque de première espèce faible (voir
le commentaire à la suite de la déﬁnition 9.6), et on peut considérer qu’il y a
équivalence du point de vue pratique.
Le test du RVG n’a pas de propriétés d’optimalité notables mais on constate
dans des situations usuelles qu’il donne le test UPP-sans biais (voir l’exemple
9.7). Cependant il possède des propriétés asymptotiques intéressantes, notamment sa convergence moyennant des conditions de régularité analogues à celles
de l’estimateur du maximum de vraisemblance.
Le problème est toutefois de connaı̂tre la loi de la statistique du RVG
λ(X1 , X2 , · · · , Xn ) pour toute valeur de θ dans Θ0 aﬁn de déﬁnir la valeur de
k permettant de garantir le niveau α choisi. En eﬀet cette valeur doit être telle
que :
sup Pθ (λ(X1 , X2 , · · · , Xn ) < k) = α .
θ∈Θ0

Il arrive, comme dans l’exemple qui suit, que la région de rejet se ramène à
une forme simple. Mais cela reste l’exception et nous verrons plus loin qu’on
dispose, sinon, d’une approximation asymptotique très utile.
Exemple 9.7 Soit la famille de loi N (μ, σ 2 ) avec (μ, σ 2 ) inconnu et l’hypothèse
H0 : σ 2 = σ02 vs. H1 : σ 2 = σ02 . Comme μ est inconnu H0 et H1 sont toutes
deux multiples. Pour (μ, σ 2 ) quelconque la fonction de vraisemblance est :
L(μ, σ2 ; x1 , x2 , · · · , xn ) =

n
$

√

i=1


=

√

1
2πσ 2

1
2πσ 2

exp{−

1
(xi − μ)2 }
2σ 2

n
exp{−

n
1 
(xi − μ)2 } .
2σ 2 i=1

On a vu (exemple 6.20) quel’estimation du MV pour le paramètre (μ, σ2 )
n
inconnu est (x, s'2 ) où s'2 = n1 i=1 (xi −x)2 . Le dénominateur du RVG est donc
obtenu en remplaçant respectivement μ et σ 2 par x et s'2 . Pour le numérateur
il s’agit de maximiser uniquement sur μ car σ 2 est ﬁxé, égal à σ02 . Cela revient

222

Statistique − La théorie et ses applications

à minimiser
est égal à :

n

i=1 (xi

− μ)2 ce qui s’obtient pour μ = x . Finalement le RVG
n


√1
λ(x1 , x2 , ..., xn ) =

=

exp{− 2σ1 2

2πσ02

√ 1
2π'
s2

s'2
σ02

0

n

exp{− 2'1s2

 n2 exp{− n s'2 }
2 σ2
0

exp{− n2 }

n

i=1 (xi

n

i=1 (xi


=

− x)2 }

− x)2 }
 n2

s'2
s'2
exp{1 − 2 }
2
σ0
σ0

.

Considérons la fonction g(u) = u exp{1 − u}. Elle est nulle pour u = 0 et
tend vers 0 quand u → +∞. Son sens de variation est le même que celui de
ln g(u) = ln u + 1 − u dont la dérivée est u1 − 1, laquelle est positive pour u < 1,
s’annule en u = 1 et est négative pour u > 1. La fonction g(u) admet donc un
unique maximum sur [0, +∞[ en u = 1. La région de rejet λ(x1 , x2 , ..., xn ) < k
2
se traduit ainsi en u1 < σs' 2 < u2 où u1 < 1 < u2 et g(u1 ) = g(u2 ), soit encore,
0
en multipliant par n :
n
nu1 <

i=1 (xi
σ02

− x)2

< nu2 .

n
Or la statistique i=1 (Xi − X)2 /σ02 suit une loi χ2 (n − 1) (voir théorème 5.1)
2 (n−1)
2 (n−1)
ce qui permet de trouver des quantiles appropriés χα1
et χ1−α2 . Ceux-ci,
pour le niveau α choisi, doivent d’une part vériﬁer α1 + α2 = α et d’autre part
2 (n−1)
2 (n−1)
) = g( n1 χ1−α2 ). Ce test est de la même forme que le test classique
g( n1 χα1
que nous verrons en section 9.7.2 et dont on peut montrer qu’il est UPP-sans
biais pour un choix particulier des quantiles ci-dessus.

Exemple 9.8 Nous donnons maintenant un exemple dans la situation :
H0 : θ1 ≤ θ ≤ θ2

vs.

H1 : θ < θ1 ou θ > θ2

qui, bien qu’assez peu envisagée par les praticiens, est souvent plus réaliste que
celle où H0 : θ = θ0 . En eﬀet tester une valeur ponctuelle n’a pas grand sens
tant il est vrai qu’on peut être certain qu’elle ne peut correspondre de façon
exacte à la vérité. D’ailleurs, pour un test convergent, on sera amené à coup
quasi sûr à rejeter cette hypothèse avec de grands échantillons. Dans la mesure
du possible il est préférable de considérer une marge de tolérance [θ1 , θ2 ] pour
le paramètre inconnu.
Nous prenons le cas d’une loi mère N (μ, σ 2 ) où le paramètre (μ, σ 2 ) de
dimension 2 est inconnu et testons donc :
H0 : μ1 ≤ μ ≤ μ2 vs. H1 : μ < μ1 ou μ > μ2 .

Chapitre 9. Tests d’hypothèses paramétriques

223

−n
√
Le dénominateur du RVG vaut
2π'
s2
exp{− n2 } comme dans l’exemple
9.7. Pour trouver le numérateur il faut maximiser :

√

2πσ2

−n

exp{−

n
1 
(xi − μ)2 }
2σ 2
i=1

pour μ ∈ [μ1 , μ2 ] et σ 2 > 0 quelconque. En passant au logarithme cela revient
à minimiser :
 n

1 1
2
2
ln σ + 2
(xi − μ) .
σ n
i=1

2

1
a
σ2

Pour a ﬁxé, ln σ +
minimiser la fonction :

n


est minimal quand σ 2 est égal à a et il faut donc

(xi − μ)2 =

i=1

n


x2i + nμ(μ − 2x)

i=1

ou encore h(μ) = μ(μ − 2x).
Pour les réalisations telles que x soit dans [μ1 , μ2 ] le minimum est atteint
pour μ = x. Mais dans ce cas la solution est identique à celle du dénominateur
et le RVG vaut 1. On est donc nécessairement dans la zone d’acceptation
(l’inégalité pour la zone de rejet donnée dans la déﬁnition 9.11 étant stricte).
Si x < μ1 le minimum cherché pour μ ∈ [μ1 , μ2 ] est obtenu pour μ = μ1 car
la fonction h(μ) est croissante pour μ > x. Le numérateur du RVG vaut alors
−n
%
n
2πs21
exp{− n2 } où s21 = n1 i=1 (xi − μ1 )2 et le RVG vaut (s21 /'
s2 )−n/2 .

L’inégalité λ(x1 , x2 , ..., xn ) < k se traduit donc en s21 /'
s2 > k avec k  > 1. Or :
1
1
2
(xi − μ1 )2 =
[(xi − x) + (x − μ1 )]
n i=1
n i=1
n

n

=

n
n
n

1
2
1
(xi − x)2 + (x − μ1 )
(xi − x) +
(x − μ1 )2
n i=1
n
n
i=1
i=1

= s'2 + (x − μ1 )2 .
s2 = 1 + (x − μ1 )2 /'
s2 et s21 /'
s2 > k équivaut à (x − μ1 )2 /'
s2 > k
Donc s21 /'

où k > 0, soit encore,
√ après multiplication par n − 1 et en prenant la racine
carrée, |x − μ1 |/(s/ n) > k1 avec k1 > 0.√Comme x < μ1 on a ﬁnalement
une région de rejet de la forme (x − μ1 )/(s/ n) < −k1 .
De la même façon on obtient,
pour les réalisations telles que x > μ2 , la
√
région de rejet (x − μ2 )/(s/ n) > k2 . La diﬃculté vient alors du fait qu’il faut
trouver les constantes positives k1 et k2 telles que, pour tout μ ∈ [μ1 , μ2 ] :




X − μ1
X − μ2
√ < −k1 + Pμ
√ > k2 ≤ α
Pμ
S/ n
S/ n

Statistique − La théorie et ses applications

224

X−μ
√ suit une loi de Student à n-1 degrés de liberté. Même en
sachant que S/
n
admettant que le risque maximal est atteint en μ1 ou en μ2 , la solution n’est
pas simple. En eﬀet si, par exemple, μ = μ1 la deuxième probabilité concerne
une v.a. qui ne suit pas une loi de Student classique du fait que μ2 n’est pas
la moyenne de X, mais une loi de Student non centrale de paramètre de noncentralité (μ1 − μ2 )/σ dont les tables sont peu répandues (sans compter que σ
est inconnu). On peut mettre en évidence une procédure conservatrice en
réécrivant la somme des probabilités ci-dessus, dans ce même cas où μ = μ1 ,
selon :




μ2 − μ1
X − μ1
X − μ1
√ < −k1 + Pμ1
√ > k2 +
√
Pμ1
.
S/ n
S/ n
S/ n
(n−1)

En prenant k1 = k2 = t1−α/2 , la première probabilité vaut bien α/2 et la
seconde est certainement inférieure à α/2 puisque μ2 − μ1 est positif. Le raisonnement est identique lorsque μ = μ2 . Notons que la procédure est d’autant
plus conservatrice que les deux moyennes s’éloignent.
La situation H0 : μ = μ0 vs. H1 : μ = μ0 peut être considérée comme
un cas particulier de la situation ci-dessus, avec μ1 = μ2 = μ0. On a alors une
(n−1)
de la loi t(n − 1) et
procédure exacte en choisissant pour −k1 un quantile tα1
(n−1)
pour k2 un quantile t1−α2 de façon que α1 +α2 =α. En raison de la symétrie de la
(n−1)

loi de Student il semble naturel d’opter pour les quantiles symétriques tα/2
(n−1)
−t1−α/2

=

(n−1)
t1−α/2 .

et
Ceci correspond au test classique présenté en section 9.7.1
dont on montre qu’il est UPP-sans biais.
En exercices est proposé le cas plus simple où σ2 est connu.



Paramètre de nuisance
Supposons que la loi mère appartienne à une famille à paramètre de dimension 2, noté pour la circonstance (θ, ρ) où θ et ρ sont ses deux composantes. Si
l’hypothèse nulle ne spéciﬁe que la composante θ, la composante ρ est appelée
paramètre de nuisance. C’est le cas de σ 2 dans l’exemple ci-dessus ou celui de
μ dans l’exemple 9.7. Pour les tests usuels, la présence d’un paramètre de nuisance fera qu’il n’y aura généralement pas de test UPP, mais il peut exister un
test UPP-sans biais. Ceci est vrai pour l’hypothèse H0 : σ 2 = σ02 de l’exemple
9.7 ou pour H0 : μ = μ0 de l’exemple qui précède. Pour une hypothèse nulle de
la forme H0 : θ1 ≤ θ ≤ θ2 on montre qu’il existe un test UPP-sans biais pour
une famille de la classe exponentielle si sa densité (ou fonction de probabilité)
peut s’écrire :
f (x; θ, ρ) = a(θ, ρ) b(x) exp{c1 (θ)d1 (x) + c2 (ρ)d2 (x)}.
On notera que ceci n’est pas vériﬁé par la loi de Gauss qui ne sépare pas ainsi
μ et σ 2 dans la partie exponentielle. De fait, il n’existe pas de test UPP-sans

Chapitre 9. Tests d’hypothèses paramétriques

225

biais pour H0 : μ1 ≤ μ ≤ μ2 considérée ci-dessus. Ces résultats se généralisent
à plusieurs paramètres de nuisance.
Nous donnons maintenant un résultat asymptotique très précieux qui permettra de déterminer une région de rejet approchée dans le cas de grands
échantillons.
Théorème 9.2 Soit la famille paramétrique {f (x; θ), θ ∈ Θ}, où Θ ⊆ Rk , et
l’hypothèse H0 spéciﬁant les valeurs de r composantes de θ (1 ≤ r ≤ k). Supposons que soient remplies les conditions de régularité garantissant que l’estimateur du maximum de vraisemblance soit BAN (voir proposition 6.11). Alors,
sous H0, la statistique du RVG Λn = λ(X1 , X2 , · · · , Xn ) est telle que :
L

−2 ln Λn −→ χ2 (r).
Donnons une esquisse de démonstration dans le cas simple où le paramètre
θ inconnu est de dimension k = 1 et est donc parfaitement spéciﬁé par H0 :
θ = θ0 . On verra ainsi que ce résultat est lié aux propriétés asymptotiques de
l’estimateur du MV. Pour une quelconque réalisation (x1 , x2 , · · · , xn ) notée en
bref xn , développons en série de Taylor la log-vraisemblance de xn en θ0 autour
de l’estimation du maximum de vraisemblance θn :
ln L(θ0 ; xn ) =
∂
1
∂2
ln L(θn ; xn ) + (θn − θ0 ) ln L(θn ; xn )+ (θn − θ0 )2 2 ln L(θ'n ; xn )
∂θ
2
∂θ
où θ'n est une valeur comprise entre θ0 et θn . Comme, par déﬁnition de l’esti∂
ln L(θn ; xn ) = 0, on a, pour le RVG :
mation du MV, ∂θ
−2 ln λn = − 2[ln L(θ0 ; xn )− ln L(θn ; xn )],
d’où :
−2 ln λn = −(θn − θ0 )2

∂2
ln L(θ'n ; xn ).
∂θ2

En passant aux v.a. (avec Xn = (X1 , X2 , · · · , Xn )), sous l’hypothèse nulle, θn
converge en probabilité vers θ0 et il en va donc de même pour θ'n . On peut alors
∂2
montrer, d’une part par continuité de ∂θ
2 ln f (x; θ) par rapport à θ (condition
de régularité), d’autre part par la loi des grands nombres, que :
1 ∂2
1  ∂2
ln L(θ'n ; Xn ) = −
ln f (Xi ; θ'n )
2
n ∂θ
n i=1 ∂θ2
n

−

+
,
∂2
= I(θ0 ). Ainsi
converge en probabilité et donc en loi vers Eθ0 − ∂θ
2 ln f (X; θ0 )
−2 ln Λn a la même convergence en loi que la suite de v.a. nI(θ0 )(θ̂n − θ0 )2 . Or,

226

Statistique − La théorie et ses applications

%
selon la proposition 6.11, nI(θ0 )(θ̂n − θ) converge en loi vers la loi N (0 ; 1)

d’où il découle que nI(θ0 )(θ̂n − θ0 )2 converge en loi vers la loi χ2 (1).
Comme la région de rejet λ < k est équivalente à −2 ln λ > k  on rejettera
à un niveau approximatif α si :
2 (r)

−2 ln λ > χ1−α .
Ce résultat dont la validité s’étend au-delà de l’échantillonnage aléatoire simple
autorise un test approché dans des situations complexes. C’est pourquoi on
trouve le test du RVG de façon omniprésente dans les logiciels. Nous
montrerons plus loin qu’il est à la base des tests classiques dits du khi-deux
portant sur des fréquences (voir sections 10.1 à 10.4).
Note 9.4 Dans les situations de test que nous avons décrites, le cas le plus
fréquent est celui d’une hypothèse nulle ne spéciﬁant qu’une seule dimension
du paramètre, la statistique du RVG suivant alors une loi χ2 (1). Quand le
RVG est un test usuel avec une statistique dont la loi est connue sous H0 on
peut apprécier la validité de l’approximation asymptotique (voir exercice 9.8).
2 (1)
Notons que la région de rejet au niveau 0,05 est −2 ln λ > χ0,95 = 3,84 ce qui
signiﬁe un rapport de vraisemblance inférieur à 0,147.
Note 9.5 Lorsque l’on comparera plusieurs populations on posera l’égalité de
certains paramètres sans vraiment les spéciﬁer tous. On verra par exemple en
section 9.7.3 l’hypothèse nulle d’égalité des moyennes de deux lois gaussiennes
H0 : μ1 = μ2 . En fait pour le théorème ci-dessus on peut considérer que l’on
spéciﬁe un paramètre. Pour le voir il suﬃt de poser μ2 = μ1 + θ et H0 s’écrit
alors θ = 0.

9.6

Remarques diverses

La plupart des tests disponibles ont été développés à partir d’idées intuitives
en imaginant une statistique dont le comportement est très diﬀérencié sous
H0 et sous H1 , et dont la loi exacte ou approchée est accessible sous H0 .
Evidemment de tels tests seront rarement UPP. Pour une situation de test
donnée il existe généralement, comme on peut le voir dans les logiciels, plusieurs
propositions de test. Il est diﬃcile de savoir quel test est préférable car le calcul
formel de la puissance ne peut être conduit, soit parce que la loi de la statistique
est trop complexe pour l’alternative, soit en raison de la multiplicité de formes
de cette alternative (notamment pour les tests de type non paramétrique comme
on en verra au chapitre 10). Suite à des études examinant des hypothèses
alternatives particulières, formellement ou par simulation, on dispose parfois
d’éléments permettant d’eﬀectuer le meilleur choix compte tenu des formes les
plus plausibles de l’alternative dans la situation considérée.

Chapitre 9. Tests d’hypothèses paramétriques

227

Comme pour les intervalles de conﬁance les résultats asymptotiques concernant l’estimateur
%du MV peuvent être utilisés pour construire un test approxiune loi N (0 ; 1). Si
matif. En eﬀet nI(θ) (θ̂nM V − θ) suit approximativement
%
l’hypothèse nulle spéciﬁe parfaitement θ = θ0 alors nI(θ0 ) (θ̂nM V − θ0 ) est
une statistique de loi approximativement connue, ce qui permet de déﬁnir une
région de rejet. Ceci sera utilisé dans la prochaine section pour le cas d’une loi
mère de Bernoulli avec de grands échantillons.
La théorie classique des tests peut être généralisée dans le cadre de la théorie
de la décision. Celle-ci stipule une fonction de perte qui déﬁnit un coût pour
l’erreur de première et pour celle de deuxième espèce, puis la notion de risque
comme espérance mathématique de cette fonction de perte. Le risque étant une
fonction de θ l’objectif est alors de rechercher le test qui minimise le risque,
si possible uniformément en θ. Un tel test n’existant généralement pas on se
satisfera, par exemple, d’un test minimax pour lequel le risque maximum sur
Θ reste inférieur ou égal à celui de tout autre test. On peut conjuguer cette
approche avec une approche bayésienne en considérant une loi a priori sur Θ
et en minimisant le risque a posteriori qui ne dépend plus du ou des paramètres
inconnus. Notons que la théorie classique repose implicitement sur une fonction
de perte attribuant le coût 1 à une erreur de première comme de deuxième
espèce.
L’usage courant de la P-valeur
La décision d’accepter ou de refuser une hypothèse est sujette au choix
du risque de première espèce α. Aﬁn d’éviter ce choix on peut recourir, et
c’est ce que font les logiciels, à la notion de P-valeur pour simplement rendre
compte du résultat d’un test. La P-valeur est la probabilité que, sous
H0 , la statistique de test prenne une valeur au moins aussi extrême
que celle qui a été observée. La notion de position extrême se déﬁnit en
relation avec la déﬁnition du seuil du test. Si la région de rejet est unilatérale
du type t > c, alors pour une valeur t0 observée après expérience la P-valeur
est P (T > t0 |H0 ) si H0 est simple ou bien le maximum de Pθ (T > t0 ) sur Θ0
si elle est multiple. Si la région de rejet est bilatérale, par exemple {t | t < c1
ou t > c2 } alors la P-valeur est déﬁnie par 2P (T < t0 |H0 ) si t0 est plus petit
que la médiane de la loi de T sous H0 ou 2P (T > t0 |H0 ) s’il est plus grand,
ceci aﬁn de tenir compte du rejet sur les deux extrémités.
Reprenons la situation de l’exemple 9.5 avec H0 : μ ≤ 5 vs. H1 : μ > 5 pour
une loi mère N (μ , 1). On a vu que sur la base√d’un échantillon de taille 10
x > 5 + 1,645(1/ 10) au niveau 0,05 ou, plus
on est amené à rejeter H0 si √
généralement, x > 5 + z1−α (1/ 10) au niveau α où z1−α est le quantile d’ordre
1 − α de la loi de Gauss centrée-réduite. Supposons que l’échantillon observé
ait une moyenne égale à 6. Comme le risque de première espèce maximal est
atteint pour
√ μ = 5 la P-valeur est égale à P (X > 6) pour X ; N (5 ; 1/10) soit
P (Z > 10(6 − 5) ) = P (Z > 3, 16) pour Z ; N (0 ; 1), laquelle est égale à

228

Statistique − La théorie et ses applications

0,008 ce qui indique directement que la valeur observée est au-delà de la valeur
critique au niveau 0,05 et même au niveau 0,01. Si le test avait été bilatéral avec
H0 : μ = 5 vs. H1 : μ = 5, la P-valeur correspondant à la même observation 6
aurait été prise égale à 0,016 impliquant un rejet au niveau 0,05 mais pas au
niveau 0,01.
Avec cette déﬁnition, d’une façon générale, la P-valeur permet de déterminer
si l’on rejette à un niveau α donné (à condition toutefois que, dans le cas bilatéral, la zone de rejet soit partagée en risque α/2 équitablement sur chaque
extrémité, ce qui est l’usage courant). Si la P-valeur est inférieure à α on rejette
H0 sinon on l’accepte. Comme autre façon de voir les choses on peut dire que
plus la P-valeur est faible plus l’hypothèse nulle est suspecte. Ainsi l’indication des P-valeurs dans les logiciels a rendu obsolète l’usage des tables pour le
praticien.

9.7

Les tests paramétriques usuels

Certains de ces tests ont, en fait, déjà été vus dans la théorie générale et
nous les indiquerons plus brièvement. La construction des tests usuels découle
de la présence d’une statistique de loi connue sous H0 et souvent sous H1 ,
s’imposant de façon assez naturelle, indépendamment de toute considération
d’optimalité. Dans la plupart des cas il se trouve que le test ainsi construit est
UPP-sans biais ce que nous mentionnerons au passage.
Il y a un parallélisme étroit entre les sections 7.4.1 à 7.4.6 pour la construction des intervalles de conﬁance usuels et les sections qui vont suivre. En eﬀet
le point de départ est identique. Pour un IC on met en évidence une statistique
T telle qu’il existe une fonction g(T, θ) dont la loi est indépendante de θ, ce
qui permet un encadrement à un niveau de probabilité souhaité. Pour peu que
cette fonction puisse pivoter (voir déﬁnition 7.3) on en déduit un encadrement
de θ. Pour un test les choses sont plus simples car on spéciﬁe θ = θ0 sous H0 et
g(T, θ0 ) devient une statistique de loi connue. Soit uα/2 et u1−α/2 les quantiles
d’ordres respectifs α/2 et 1 − α/2 pour cette loi, on peut donner comme région
d’acceptation A = {t | g(t, θ0 ) ∈ [uα/2 , u1−α/2 ]}. Il n’y a donc pas nécessité à
pivoter pour la fonction g(t, θ) mais, en revanche, il faut souhaiter qu’elle soit
monotone vis-à-vis de t pour que A soit un intervalle pour t. Le calcul de la
puissance, par exemple en θ = θ1 : Pθ1 (g(T, θ0 ) ∈
/ [uα/2 , u1−α/2 ]), pourra toutefois poser des diﬃcultés dans la mesure où la statistique est ici g(T, θ0 ) et non
pas g(T, θ1 ) : la loi de g(T, θ0 ) «sous θ1 » n’aura pas souvent une forme simple.
Quoi qu’il en soit tous les tests mis en évidence ci-après seront convergents.
En ﬁn de chapitre nous reviendrons plus précisément sur le lien entre IC
et tests et montrerons l’exploitation que l’on peut en faire. Nous proposons
aussi dans la section des exercices quelques « exercices appliqués » permettant
de voir des situations pratiques pour les tests usuels.

Chapitre 9. Tests d’hypothèses paramétriques

9.7.1

229

Tests sur la moyenne d’une loi N (μ, σ2 )

Cas où σ 2 est connu
Considérons tout d’abord ce cas d’école simple. La statistique T du cas
général évoqué ci-dessus √
est la statistique exhaustive minimale X et la fonction
g(T, θ) est (X − μ)/(σ/ n) de loi connue : N (0 ; 1). Notons que ce point de
départ est celui de la section 7.4.1. Ainsi pour une hypothèse nulle H0 : μ = μ0
vs. H1 : μ = μ0 on a, sous H0 :
X − μ0
√ ; N (0 ; 1).
σ/ n
Comme :



X − μ0
√ < z1−α/2
σ/ n


σ
σ
= Pμ0 μ0 − z1−α/2 √ < X < μ0 + z1−α/2 √
n
n

1 − α = Pμ0

−z1−α/2 <

une région d’acceptation peut donc être déﬁnie, pour un test de risque α, par :
σ
σ
A = {x | μ0 − z1−α/2 √ < x < μ0 + z1−α/2 √ }.
n
n
Ce test est UPP-sans biais, en vertu de la propriété mentionnée en section 9.4.3
pour la classe exponentielle.
√ Ici la fonction puissance h(μ) peut être déterminée
car la loi de (X − μ0 )/(σ/ n) reste gaussienne quand μ est diﬀérent de μ0 . En
eﬀet :


X − μ0
√
< z1−α/2
h(μ) = 1 − Pμ −z1−α/2 <
σ/ n


X − μ μ − μ0
√ +
√ < z1−α/2
= 1 − Pμ −z1−α/2 <
σ/ n
σ/ n


μ −μ
X −μ
μ0 − μ
√ − z1−α/2 <
√ < 0 √ + z1−α/2
= 1 − Pμ
σ/ n
σ/ n
σ/ n




μ0 − μ
μ0 − μ
√ + z1−α/2 + Φ
√ − z1−α/2
=1−Φ
σ/ n
σ/ n
où Φ est la fonction de répartition de la loi de Gauss centrée-réduite. On montre
que la fonction h(μ) s’accroı̂t de part et d’autre de μ0 à partir de la valeur α
(voir une illustration dans l’exercice 9.3). Par ailleurs on vériﬁe aisément que
h(μ) → 1 quand n → ∞ aussi bien lorsque μ > μ0 que lorsque μ < μ0 ce qui
démontre la convergence du test.
Pour des hypothèses unilatérales, par exemple H0 : μ ≤ μ0 vs. H1 : μ > μ0 ,
il est naturel de rejeter H0 lorsque x est trop grand car cela reﬂète une moyenne

Statistique − La théorie et ses applications

230

μ élevée. Pour déterminer la valeur critique on se place en μ = μ0 qui est la
valeur la plus défavorable pour H0 (plus précisément, comme on l’a vu en
section 9.4.2, le risque de première espèce est maximal en μ = μ0 ). Comme :


X − μ0
√
α = Pμ0 z1−α <
,
σ/ n
√
H0 : μ ≥ μ0
on a pour région de rejet A = {x | x > μ0 + z1−α σ/ n}. Pour √
vs. H1 : μ < μ0 la région de rejet sera A = {x | x < μ0 − z1−α σ/ n}.
Ces tests unilatéraux sont UPP comme vu en section 9.4.2 (voir exemple
9.5).
Cas où σ 2 est inconnu
Passons maintenant à ce cas général et plus réaliste. Ici une statistique
exhaustive minimale est nécessairement de dimension 2 et l’on prendra (X, S 2 ).
Mais on dispose d’une
√ fonction à valeur dans R de loi connue quel que soit μ,
à savoir (X − μ)/(S/ n) de loi de Student t(n − 1). Ainsi pour une hypothèse
nulle H0 : μ = μ0 vs. H1 : μ = μ0 on a, sous H0 :
X − μ0
√ ; t(n − 1).
S/ n
Comme :


1 − α = Pμ0

(n−1)
−t1−α/2

X − μ0
√ < t(n−1)
<
1−α/2
S/ n


,

une région de rejet peut être déﬁnie, pour un test de risque α, par :
x − μ0
(n−1)
(n−1)
√ ∈
/ [−t1−α/2 , t1−α/2 ].
s/ n
Pour des hypothèses bilatérales on aura, comme régions de rejet :
x − μ0
√ > t(n−1)
1−α/2
s/ n
x − μ0
√ < −t(n−1)
1−α/2
s/ n

pour H0 : μ ≤ μ0 ,
pour H0 : μ ≥ μ0 .

Le test bilatéral est UPP-sans biais de même que les tests unilatéraux (ces
derniers ne sont qu’UPP-sans biais en raison de la présence du paramètre de
nuisance σ 2 ).
Qu’en est-il de la détermination de la fonction puissance de ce test de
Student ? Par transcription du cas σ2 connu il nous faut calculer, pour μ «dans
H1 » :


X − μ0
(n−1)
√ < t(n−1)
h(μ) = Pμ −t1−α/2 <
1−α/2 .
S/ n

Chapitre 9. Tests d’hypothèses paramétriques

231

√
Mais ici (X − μ0 )/(S/ n) ne suit plus une loi de Student car μ0 n’est plus la
moyenne de X. En écrivant :
X − μ0 + (μ − μ0)
X − μ0
√ =
√
S/ n
S/ n
on met en évidence le fait que cette statistique de test suit alors une loi de
Student non centrale de paramètre de non centralité (μ − μ0 )/σ, déjà rencontrée dans l’exemple 9.8. Les tables des lois de Student non centrales sont
volumineuses et peu répandues. Les logiciels pourraient faciliter la tâche mais
rares sont ceux qui ont intégré ces calculs. Il existe également des abaques pour
ce problème.
Pour les mêmes raisons que celles indiquées lors de l’étude de l’IC sur la
moyenne (voir section 7.4.1), ce test est robuste vis-à-vis de l’hypothèse gaussienne. De fait les praticiens l’utilisent sans se soucier de cette hypothèse.
Pour ce qui concerne l’hypothèse :
H0 : μ1 ≤ μ ≤ μ2 vs. H1 : μ < μ1 ou μ > μ2
on a vu les diﬃcultés de mise en oeuvre dans l’exemple 9.8. On a montré qu’en
√
√
(n−1)
(n−1)
rejetant H0 lorsque (x − μ1 )/(s/ n) < −t1−α/2 ou (x − μ2 )/(s/ n) > t1−α/2
on obtient un test conservateur de niveau α. Il n’existe pas de test UPP-sans
biais dans ce cas.

9.7.2

Test sur la variance σ 2 d’une loi N (μ, σ 2 )

Nous supposons que μ est inconnu et nous nous intéressons à une hypothèse
sur σ 2 , par exemple H0 : σ 2 = σ02 vs. H1 : σ 2 = σ02 . Ce test a déjà été traité dans
l’exemple 9.7 via le rapport de vraisemblance généralisé. Nous allons retrouver
le même test en développant l’approche directe analogue à celle de l’intervalle
de conﬁance vue en section 7.4.2. Le point de départ est identique, à savoir le
fait que :
(n − 1)S 2
; χ2 (n − 1).
σ2
Sous H0 on a donc :


(n − 1)S 2
2 (n−1)
2 (n−1)
P χα/2
<
< χ1−α/2
= 1 − α.
σ02
On dispose donc d’une statistique de test (n − 1)S 2 /σ02 et d’une région d’ac2 (n−1)
2 (n−1)
, χ1−α/2 ].
ceptation [ χα/2
Ce test est UPP-sans biais, mais pour un choix précis de quantiles
2 (n−1)
2 (n−1)
χα1
et χ1−α2 tels que α1 + α2 = α (on montre que la condition à remplir, conformément à la note 9.3, est que la probabilité associée à l’intervalle

Statistique − La théorie et ses applications

232

délimité par ces deux valeurs sur la loi χ2 (n + 1) soit égale à 1-α). Ce choix
est compliqué et l’on s’en tient généralement au choix ci-dessus qui est assez
proche.
Pour l’hypothèse unilatérale H0 : σ 2 ≤ σ02 la région de rejet doit être intuiti2 (n−1)
vement (n − 1)S 2 /σ02 > χ1−α car une valeur élevée de S 2 rend H0 suspecte.
2 (n−1)
Pour H0 : σ 2 ≥ σ02 on rejette si (n − 1)S 2 /σ02 < χα
. Ces tests sont
également UPP-sans biais.
Rappelons que le résultat sur la distribution d’échantillonnage de
(n − 1)S 2 /σ 2 est peu robuste vis-à-vis de l’hypothèse gaussienne et, par conséquent, ces tests ne sont valables que si la loi mère est proche d’une loi de
Gauss.
A titre de curiosité, si la moyenne μ était connue on utiliserait le fait que
n
2
i=1 (Xi − μ)
; χ2 (n),
σ2
les développements étant analogues aux précédents.

9.7.3

Tests de comparaison des moyennes de deux lois de
Gauss

On est en présence de deux échantillons indépendants, l’un de taille
n1 , de moyenne X 1 et variance S12 , issu d’une loi N (μ1 , σ12 ), l’autre de taille
n2 , de moyenne X 2 et variance S22 issu d’une loi N (μ2 , σ22 ). En général les deux
moyennes des lois et les deux variances sont inconnues. On souhaite comparer
les deux moyennes μ1 et μ2 sur la base des échantillons. Essentiellement, les
questions qui se posent sont de savoir si l’on peut décider à un niveau de
risque α donné si elles sont diﬀérentes (cas bilatéral) ou si l’une est supérieure
à l’autre (cas unilatéral). Ce type de situation, bilatérale ou unilatérale, est
très fréquent car on est souvent amené à comparer deux populations réelles ou
virtuelles suivant leurs moyennes. Par exemple dans l’expérimentation clinique
on veut démontrer l’eﬃcacité d’un traitement en comparant un échantillon
traité et un échantillon témoin. Prenant soin de sélectionner ces échantillons de
façon qu’ils puissent, chacun, être considérés comme pris au hasard parmi les
personnes présentant la pathologie à traiter, l’échantillon traité (respectivement
l’échantillon témoin) peut alors être envisagé comme issu d’une population
virtuelle de patients traités (respectivement de patients non traités). On se
place ici dans une situation de test unilatérale, cherchant à voir si l’on peut
décider que le traitement est eﬃcace, en moyenne, selon un critère quantitatif
approprié. Dans la mesure où le traitement ne peut qu’avoir soit aucun eﬀet soit
un eﬀet nécessairement bénéﬁque, on pourrait se restreindre à une hypothèse
nulle ponctuelle, par exemple H0 : μ1 − μ2 = 0 vs. H1 : μ1 − μ2 < 0, comme on
le fait dans certains ouvrages. Cette restriction, comme on l’a vu en particulier
en section 9.4.2, ne modiﬁant pas le niveau du test ni sa puissance par rapport

Chapitre 9. Tests d’hypothèses paramétriques

233

au test H0 : μ1 − μ2 ≤ 0, nous en resterons à cette dernière hypothèse nulle qui
est plus générale.
C’est à ces tests de comparaison des moyennes que l’on doit l’expression
«hypothèse nulle». Ceci est également le cas pour l’expression courante du
praticien qui parle de «test signiﬁcatif au niveau α» lorsque l’hypothèse nulle
peut être rejetée à ce niveau. En eﬀet ceci se dit par extension de l’idée qu’une
diﬀérence de deux moyennes empiriques est ou n’est pas statistiquement
signiﬁcative selon que le test d’égalité des moyennes théoriques est rejeté ou
accepté.
Comme pour la construction d’un intervalle de conﬁance vue en section
7.4.3, il n’existe pas de méthode exacte dans le cas général où σ12 = σ22 , mais
une procédure asymptotique que nous présenterons par la suite. On suppose
donc pour l’heure que les deux lois ont même variance σ2 . Dès lors notre
point de départ est le même que celui de la section 7.4.3 avec le résultat général
suivant :
(X 1 − X 2 ) − (μ1 − μ2 )
(
; t(n1 + n2 − 2)
Sp n11 + n12
où Sp2 =

(n1 − 1)S12 + (n2 − 1)S22
n1 + n2 − 2

est un estimateur sans biais de la variance commune σ 2 .
Référons-nous tout d’abord au cas(bilatéral : H0 : μ1 = μ2 vs. H1 : μ1 = μ2 .
Sous H0 la statistique (X 1 − X 2 )/Sp n11 + n12 suit donc une loi t(n1 + n2 − 2)
ce qui permet de déﬁnir une région de rejet, pour un test de risque α, par :
(n1 − 1)s21 + (n2 − 1)s22
x − x2
(n1 +n2 −2)
(n1 +n2 −2)
(1
∈
/ [−t1−α/2
, t1−α/2
] où s2p =
.
n1 + n2 − 2
sp n11 + n12
Pour la situation unilatérale H0 : μ1 ≤ μ2 vs. H1 : μ1 > μ2 , on voit
(n1 +n2 −2)
intuitivement qu’il faut rejeter uniquement dans ] t1−α
, +∞[. Au-delà de
l’intuition, on peut montrer que les résultats propres au test sur la moyenne
d’un seul échantillon s’étendent à la situation présente. Notamment, avec une
telle région de rejet, le risque α maximal est atteint pour μ1 = μ2 et le test
proposé est donc bien de niveau α. Pour H0 : μ1 ≥ μ2 vs. H1 : μ1 < μ2 le rejet
(n1 +n2 −2)
[.
se fait dans ] − ∞, −t1−α
Ces tests sont UPP-sans biais mais la détermination même de la fonction
puissance n’est pas formellement possible. Par ailleurs, l’approche par le rapport de vraisemblance est équivalente (voir exercices).
Pour ce qui concerne la robustesse de ce test vis-à-vis des conditions de lois
gaussiennes et d’égalité des variances, les considérations de la section 7.4.3 pour

234

Statistique − La théorie et ses applications

l’intervalle de conﬁance restent valables. Rappelons brièvement que, comme
pour un seul échantillon, les conditions de distributions gaussiennes ne sont pas
cruciales et, d’autre part, que celle d’égalité des variances peut être assouplie
dans la mesure où les tailles d’échantillons restent du même ordre. A la lumière
des développements de ce chapitre, il est intéressant de revenir sur la pratique
mentionnée en section 7.4.3, consistant à eﬀectuer le test de la section suivante
pour décider de l’égalité des variances. Cette approche, pour être rassurante
pour le praticien, n’oﬀre pas une garantie absolue de l’applicabilité du test
de Student ci-dessus. En eﬀet nous savons maintenant que l’acceptation d’une
hypothèse ne signiﬁe pas qu’elle soit vraie, le risque d’erreur de deuxième espèce
n’étant pas contrôlé.
Si les deux lois mères n’ont pas même variance on peut utiliser,
comme pour l’intervalle de conﬁance, le fait que, pour de grands échantillons
(n1 et n2 supérieurs à 100) :
(X 1 − X 2 ) − (μ1 − μ2 )
( 2
S1
S2
+ n22
n1

;

approx

N (0 ; 1) .

Ainsi, à un niveau approximativement égal à α, on a, par exemple dans le cas
bilatéral, la règle de rejet :
x1 − x2
( 2
∈
/ [−z1−α/2 , z1−α/2 ] ,
s1
s22
+
n1
n2
ces quantiles étant lus sur la loi N (0 ; 1). Pour des tailles d’échantillons plus
faibles il existe des formules d’approximation dont l’usage n’est pas très répandu.
On peut étendre les résultats ci-dessus au test d’hypothèses nulles du type
H0 : μ1 − μ2 = Δ0 , H0 : μ1 − μ2 ≤ Δ0 ou H0 : μ1 − μ2 ≥ Δ0 . Il suﬃt pour
cela de retrancher la valeur Δ0 à celle de x1 − x2 . En revanche le test bilatéral
H0 : |μ1 − μ2 | ≤ Δ0 vs. |μ1 − μ2 | > Δ0 pose des diﬃcultés majeures. On peut
toutefois recourir à un test conservateur simple d’une façon tout à fait analogue
à celle exposée en ﬁn de section 9.7.1 pour une moyenne.
Cas d’échantillons appariés
Cette situation a été décrite en section 7.4.3. Comme pour l’intervalle de
conﬁance on se ramène au cas d’un seul échantillon en étudiant la série des
diﬀérences entre paires. Soit d et sd respectivement la moyenne et l’écart-type
observés pour ces paires, le test se fonde sur la réalisation :
d
√
sd / n
où n est le nombre de paires. Les quantiles déﬁnissant les valeurs critiques sont
à lire sur une loi de Student à n − 1 degrés de liberté. Toutes les considérations
de la section 9.7.1 restent valables.

Chapitre 9. Tests d’hypothèses paramétriques

9.7.4

235

Tests de comparaison des variances de deux lois de
Gauss

Avec les mêmes notations qu’à la section précédente on a établi, en section
5.5, le résultat général suivant :
S12 /σ12
; F (n1 − 1, n2 − 1).
S22 /σ22
Ceci est particulièrement approprié pour tester l’égalité des variances selon
H0 : σ12 /σ22 = 1 vs. H1 : σ12 /σ22 = 1. En eﬀet, sous H0 la statistique S12 /S22 suit
la loi de Fisher mentionnée ci-dessus. La règle de rejet au niveau α sera donc :
s21
(n −1,n2 −1)
(n1 −1,n2 −1)
∈
/ [Fα/21
, F1−α/2
].
s22
(n −1,n2 −1)

Rappelons pour l’usage des tables que le quantile Fα/21
1/

est égal à

(n2 −1,n1 −1)
F1−α/2
.

Ce résultat étant peu robuste par rapport aux conditions gaussiennes, son
intérêt est limité.

9.7.5

Tests sur le paramètre p d’une loi de Bernoulli (ou
test sur une proportion)

Les applications de ces tests sont multiples dès lors que l’on veut étudier un
caractère binaire dans une population. Citons notamment le contrôle de qualité
où l’on souhaite vériﬁer si le taux de produits défectueux ne dépasse pas une
valeur donnée.
Le test d’une hypothèse unilatérale a déjà été abordé à la suite de l’exemple
9.6. On a vu que le test UPP repose sur le nombre total de succès observé. Pour
H0 : p ≤ p0 vs. H1 : p > p0 , par exemple, on rejette si ce nombre est trop élevé
et la valeur critique se lit sur la loi B(n, p0 ). Étant donné le caractère discret
de cette loi, le test n’est UPP que dans la mesure où l’on introduit une règle de
rejet randomisée aﬁn d’obtenir un risque exactement égal à α (voir note 9.1).
Pour l’hypothèse bilatérale H0 : p = p0 vs. H1 : p = p0 on a un test UPP-sans
biais en choisissant deux quantiles extrêmes sur la loi B(n, p0 ) vériﬁant une
contrainte diﬃcile à mettre en pratique. On préfère donc utiliser des quantiles
correspondant au plus proche, et de façon conservatrice, à une équirépartition
sur chaque extrémité de cette loi. Cette approche a les mêmes fondements que
l’approche des intervalles de conﬁance par la méthode des quantiles présentée
en section 7.5 .
On peut également utiliser une approximation gaussienne comme pour l’intervalle de conﬁance. La condition d’applicabilité que nous avions retenue est

236

Statistique − La théorie et ses applications

n
p(1 − p) > 12. Mais ici, sous H0 , la valeur de p est donnée et nous pouvons prendre la condition np0 ≥ 5 et n(1 − p0 ) ≥ 5 indiquée en section 5.8.3
(toutefois, étant donné que l’on veut une bonne précision sur des quantiles
aux extrémités de la loi ces conditions sont souvent renforcées, par exemple en
exigeant np0 (1 − p0 ) ≥ 10). Alors la statistique P, proportion de succès dans
l’échantillon, est telle que, sous H0 :
)

P − p0
p0 (1 − p0 )
n

;

approx

N (0 ; 1) .

D’où l’on déduit, par exemple pour le cas bilatéral, la règle de rejet :
)

p − p0
p0 (1 − p0 )
n

∈
/ [−z1−α/2 , z1−α/2 ]

où p est la proportion de succès dans l’échantillon réalisé.
Notons que cette approche est en correspondance avec celle de la procédure
d’IC conduisant à la résolution d’une inégalité du second degré (voir section
7.4.5), mais pas avec celle de la formule classique où p(1 − p) est estimé par
p(1 − p), ce qui n’est pas nécessaire ici puisque p est spéciﬁé sous H0 . Cependant il est intéressant de noter qu’en substituant p(1 − p) à p0 (1 − p0 ) la
statistique ci-dessus est quasiment identique à la statistique de Student pour le
test sur une moyenne. En eﬀet grâce au codage 1/0, la moyenne de l’échantillon
n
x1 , x2 , · · · , xn est p et la variance empirique est (en remarquant que i=1 x2i
est le nombre de succès) :
1 2
xi − p2 = p − p2 = p(1 − p).
n
n

s'2 =

i=1

%
Il ne manque qu’un facteur n/(n − 1) pour retrouver l’écart-type de l’échantillon plutôt que l’écart-type empirique.
On a donc avantage, dans un ﬁchier de données, à utiliser ce codage 1/0
pour une variable binaire car certains logiciels rudimentaires ne proposent que
le test de Student (le fait que la P-valeur soit calculée sur la loi de Student
plutôt que sur la loi de Gauss ne posant pas de problème puisque cela va dans
le sens conservateur).
On peut voir aussi le test par approximation gaussienne comme une application du résultat asymptotique de l’estimateur du maximum de vraisemblance
P pour p, mentionnée en section 9.6, car l’information de Fisher I(p0 ) est égale
à [ p0 (1 − p0 )]−1 pour la loi de Bernoulli.

Chapitre 9. Tests d’hypothèses paramétriques

237

Par ailleurs, pour le cas bilatéral, le test du RVG ne donne ni le test exact ni
le test approché ci-dessus mais on verra qu’il est asymptotiquement équivalent
au test du khi-deux, lui-même identique au test approché (voir sections 10.1.3
et 10.1.4).

9.7.6

Tests de comparaison des paramètres de deux lois
de Bernoulli (comparaison de deux proportions)

La comparaison de deux proportions à partir de deux échantillons indépendants est très fréquente, au même titre que la comparaison de deux moyennes.
Elle s’applique également à l’étude de l’eﬃcacité d’un traitement par rapport
à un autre (ou en l’absence de traitement avec un échantillon témoin) lorsque
cette eﬃcacité est évaluée par un critère binaire, par exemple la guérison ou
non guérison d’un patient. Pour les mêmes raisons que celles invoquées en
section 9.7.3, dans la mesure où le traitement ne saurait avoir d’eﬀet négatif
par rapport à une absence de traitement, on pourrait se restreindre à une
hypothèse nulle ponctuelle H0 : p1 − p2 = 0 avec une hypothèse alternative
unilatérale. Ici également on conclura en déclarant que la diﬀérence est ou
n’est pas signiﬁcative à un niveau donné.
Soit S1 et S2 les statistiques exhaustives minimales des nombres de succès
parmi les n1 et n2 observations respectives de chaque échantillon. On obtient un
test UPP-sans biais en utilisant la loi conditionnelle de S1 (ou de S2 ) sachant
S1 + S2 . Montrons tout d’abord que, sous H0 : p1 = p2 , cette loi est une loi
hypergéométrique et notons p = p1 = p2 sous cette hypothèse nulle.
Rappelons (voir section 4.1.3) que si S1 ; B(n1 , p) et S2 ; B(n2 , p), alors
S1 + S2 ; B(n1 + n2 , p). On a donc :
P (S1 = x, S2 = t − x)
P (S1 = x | S1 + S2 = t) =
P (S1 + S2 = t)
 


n1 x
n2
p (1 − p)n1 −x
pt−x (1 − p)n2 −t+x
x
t−x


=
n1 + n2 t
p (1 − p)n1 +n2 −t
t
 

n1
n2
x
t−x

= 
n1 + n2
t
qui est le terme général de la fonction de probabilité de la loi H(n1 + n2 , t, n1 )
selon les notations de la section 4.1.5.

238

Statistique − La théorie et ses applications

Le test UPP-sans biais est déﬁni, par exemple dans le cas bilatéral, par une
/ [cα1 , c1−α2 ] où cα1 et c1−α2 sont des quantiles
région de rejet de la forme s1 ∈
d’ordres α1 et 1 − α2 tels que α1 + α2 = α, issus de la loi H(n1 + n2 , s1 +
s2 , n1 ) où s1 et s2 sont les nombres de succès observés sur les échantillons
réalisés. On montrera en section 10.3.2 (note 10.3) qu’un test déﬁni, comme
celui-ci, conditionnellement à une statistique exhaustive sous H0 (ce qu’est
S1 + S2 ici) est légitime au sens où il s’agit bien d’un test de niveau α non
conditionnellement.
Les calculs à la main sont très fastidieux mais certains logiciels donnent
la P-valeur pour ce test. On peut également utiliser le test exact de Fisher
pour l’indépendance de deux v.a. binaires dont les calculs sont parfaitement
identiques (voir section 10.3.2 et exemple 10.1 pour la mise en oeuvre).
La puissance est diﬃcilement accessible pour une alternative quelconque
(toutefois elle peut être calculée en fonction du rapport des chances - en anglais :
odds ratio - [ p2 /(1 − p2 )]/[ p1 /(1 − p1 )] dont dépend la loi conditionnelle de S1
sachant S1 + S2 sous H1 ).
Dès lors que l’approximation gaussienne vaut pour chaque échantillon on
utilise une formulation approchée simple. Les conditions de validité que nous
retenons sont n1 p1 (1 − p1 ) > 12 et n2 p2 (1 − p2 ) > 12 où p1 et p2 sont les
proportions de succès observées. On a établi pour l’IC correspondant (voir
section 7.4.6) la loi approximative de P1 − P2 :


p1 (1 − p1 ) p2 (1 − p2 )
+
P1 − P2 ; N p1 − p2 ,
,
approx
n1
n2
ce qui donne sous H0 :
P1 − P2


;

approx

N

0 ; p(1 − p)(


1
1
+
) .
n1
n2

Toutefois la valeur de p est inconnue et doit être estimée par la proportion de
succès dans les deux échantillons fusionnés, i.e.
p =

s1 + s2
n1 p1 + n2 p2
=
n1 + n2
n1 + n2

d’où, ﬁnalement, la région de rejet (cas bilatéral) :
)

p1 − p2
∈
/ [−z1−α/2 , z1−α/2 ].
1
1
p(1 − p)(
+
)
n1
n2

On montrera dans le cadre plus général de la comparaison de lois multinomiales (voir section 10.2) que ce test bilatéral est identique au test classique
du khi-deux.

Chapitre 9. Tests d’hypothèses paramétriques

239

Par ailleurs, pour le cas bilatéral, le test du RVG ne donne ni le test exact ni
ce test approché mais on verra (section 10.2) que la forme de la statistique du
rapport de vraisemblance est asymptotiquement équivalente à celle de la statistique du khi-deux, donc de la statistique ci-dessus obtenue par approximation
gaussienne.
Notons bien que ces tests ne sont valables que pour des échantillons indépendants. Pour des échantillons appariés citons simplement le test de
McNemar (la construction de ce test est proposée dans un exercice du chapitre
10). Il s’appliquera dans les enquêtes, par exemple, pour tester s’il y a une
évolution signiﬁcative dans la réponse à une certaine modalité d’une question
pour un même échantillon réinterrogé après un certain laps de temps (situation
de mesures répétées).
Note 9.6 Comparaisons de proportions au sein d’un même
échantillon
On peut vouloir tester l’égalité de deux proportions entre deux sous-échantillons. Par exemple tester que le pourcentage de réponses à une modalité d’une
question dans une enquête est le même pour les femmes et pour les hommes.
Nous ne sommes plus dans le schéma précédent du fait que n1 et n2 (fréquences
des femmes et fréquences des hommes) ne sont plus ﬁxées a priori mais sont des
variables aléatoires. En réalité, il s’agit ici d’un test d’indépendance entre la
variable sexe et le choix ou non de la modalité de réponse. Ce test sera présenté
en section 10.3 où l’on verra que statistique de test et région critique seront,
en fait, identiques à celles des tests présentés ci-dessus, que ce soit pour le test
exact ou pour le test par approximation gaussienne.
On peut encore vouloir comparer les pourcentages de réponses p1 et p2
pour deux modalités distinctes d’une même question. Par exemple, dans un
sondage aléatoire sur les intentions de vote, voir si les pourcentages obtenus
par deux candidats diﬀèrent ou non de façon statistiquement signiﬁcative.
Ici on est dans le cas d’une loi multinomiale (il y a plusieurs modalités de
réponse à la question) et les fréquences observées ne sont pas indépendantes.
Soit N1 et N2 ces fréquences aléatoires, nous avons vu en section 4.1.6 que
cov(N1 , N2 ) = −np1 p2 , d’où pour les proportions observées P1 = N1 /n et
P2 = N2 /n : cov(P1 , P2 ) = −p1 p2 /n. La variance de P1 − P2 est donc égale à :
V (P1 ) + V (P2 ) − 2cov(P1 , P2 ) =

p1 (1 − p1 ) p2 (1 − p2 ) 2p1 p2
+
+
.
n
n
n

Sous H0 : p1 = p2 = p on a, en admettant que la loi de P1 − P2 soit toujours
approximativement gaussienne :


2p
P1 − P2 ; N 0 ;
approx
n

Statistique − La théorie et ses applications

240

où p doit être estimé, un estimateur naturel étant P = (N1 + N2 )/2n qui est
sans biais. Dans le cas bilatéral on rejettera H0 si (cas bilatéral) :
p1 − p2
)
∈
/ [−z1−α/2 , z1−α/2 ].
2
p
n

9.7.7

Test sur la corrélation dans un couple gaussien

Soit le couple aléatoire (X, Y ) de loi gaussienne bivariée. Pour le cas général
d’un vecteur gaussien dans Rp on a vu en section 3.9 que la densité conjointe
de ses composantes au point (x1 , x2 , · · · , xp ) ∈ Rp est :
fX (x1 , x2 , · · · , xp ) =

1
1
exp{− (x − μ)t Σ−1 (x − μ)}
2
(2π)p/2 (det Σ)1/2

où μ est le vecteur des moyennes et Σ est la matrice des variances-covariances.
Pour un couple (p = 2) on a :




σ12
ρσ1 σ2
μ1
et
Σ=
μ=
μ2
ρσ1 σ2
σ22
où ρσ1 σ2 est la covariance des deux composantes et ρ est leur corrélation linéaire
(voir déﬁnition 3.6). La loi de (X, Y ) dépend donc de cinq paramètres et la
densité conjointe s’écrit :
fX,Y (x, y) =
1√

2πσ1 σ2

1−ρ2


x−μ1
σ1

1
exp − 2(1−ρ
2)

2

− 2ρ

x−μ1
σ1

y−μ2
σ2

+

y−μ2
σ2

2

!
.

On considère un échantillon de taille n : (X1 , Y1 ), (X2 , Y2 ), · · · , (Xn , Yn )
issu de cette loi dont les cinq paramètres sont inconnus et l’on souhaite tester
l’hypothèse nulle d’indépendance des deux composantes. Comme il a été vu en
section 3.6 ceci équivaut à tester que la corrélation est nulle, soit :
H0 : ρ = 0 vs. H1 : ρ = 0 .
*n
A partir de l’expression i=1 fX,Y (xi , yi ) de la densité conjointe de l’échantillon
on peut établir l’expression du rapport de vraisemblance généralisé et montrer (non sans diﬃcultés) qu’elle ne dépend des observations qu’à travers la
réalisation r de la corrélation linéaire empirique déﬁnie en section 5.2 :
n


R= 

(Xi − X)(Yi − Y )

,
n
n


(Xi − X)2
(Yi − Y )2
i=1

i=1

i=1

Chapitre 9. Tests d’hypothèses paramétriques

241



la région de rejet λ < k étant équivalente à |r| > k , ce qui semble naturel. On
utilise plutôt comme statistique de test la fonction croissante de R :
√
n − 2R
T = √
1 − R2
qui oﬀre l’avantage de suivre simplement une loi de Student à n − 2 degrés de
liberté sous H0 (une démonstration sera donnée en section 11.2.6). On rejette
donc H0 au niveau α si la réalisation t de T tombe en dehors de l’intervalle
(n−2)
(n−2)
[−t1−α/2 , t1−α/2 ].
Note 9.7 A partir de la loi de T on peut, par simple changement de variable,
établir la loi de R sous H0 et montrer que sa fonction de densité est :
fR (r) =

Γ( n−1
2 )
(1 − r 2 )(n−4)/2
1
Γ( 2 )Γ( n−2
)
2

si r ∈ [−1, 1] .

Cette forme n’est pas sans rappeler celle d’une loi bêta. En fait on peut voir
aisément que le coeﬃcient de détermination R2 suit une loi Beta(0,(n-4)/2) et
il est équivalent de fonder le test sur la réalisation r 2 de ce coeﬃcient avec un
rejet pour r2 > c1−α où c1−α est un quantile de cette loi.
On peut également déterminer l’expression de la densité de R pour ρ quelconque et établir la fonction puissance du test. Signalons que la loi de R ne
2 2
)
+o( n1 ).
dépend que du paramètre ρ, que E(R) = ρ+O( n1 ) et que V (R) = (1−ρ
n
R est donc un estimateur biaisé de r pour n ﬁni, mais il est asymptotiquement
sans biais et convergent en moyenne quadratique (et presque sûrement en tant
que fonction continue de moments empiriques). On démontrera également en
section 11.2.6 que R est l’estimateur du maximum de vraisemblance de ρ.
Fisher a établi un résultat asymptotique pour ρ quelconque :
1 1+R
ln
2 1−R

;

approx

1
1 1+ρ
,
)
N ( ln
2 1−ρ n−3

qui autorise un test approximatif pour une hypothèse générale H0 : ρ = ρ0
(bilatérale ou unilatérale).
Pour ces tests l’hypothèse d’une loi gaussienne pour le couple est essentielle
et les résultats obtenus sont assez peu robustes. Si cette condition est douteuse on pourra se tourner vers une procédure non paramétrique telle que le
test sur la corrélation des rangs de Spearman (voir section 10.5.5). Rappelons
qu’en dehors de la loi de Gauss l’hypothèse nulle ne signiﬁe qu’une absence
de corrélation et non pas l’indépendance des deux composantes du couple.
Pour tester l’indépendance on pourra recourir au test concernant les variables
catégorielles (voir section 10.3) en découpant X et Y en classes.

242

Statistique − La théorie et ses applications

9.8 Dualité entre tests et intervalles de conﬁance
La présentation des tests usuels a permis de voir que ces procédures et celles
utilisées pour la construction d’intervalles de conﬁance sont très voisines. Nous
allons voir que l’on peut même établir une sorte d’équivalence entre celles-ci.
Montrons-le sur un exemple avant de passer au cas général.
Considérons le test H0 : μ = μ0 vs. H1 : μ = μ0 pour une loi mère N (μ, σ 2 )
où (μ, σ 2 ) est inconnu. On accepte H0 au niveau α si et seulement si :
(n−1)

−t1−α/2 <

x − μ0
√ < t(n−1)
1−α/2 ,
s/ n

ce que l’on peut écrire de façon équivalente :
(n−1) s
(n−1) s
x − t1−α/2 √ < μ0 < x + t1−α/2 √ .
n
n
Ainsi pour qu’une valeur de μ hypothétique soit acceptée il faut et il suﬃt
(n−1)
(n−1)
qu’elle soit dans l’intervalle [x − t1−α/2 √sn , x + t1−α/2 √sn ], c’est-à-dire qu’elle
soit comprise dans l’intervalle de conﬁance de niveau 1 − α pour la moyenne
inconnue μ. Il y a donc équivalence pour μ entre le fait de prendre une valeur
acceptée dans le test de niveau α et le fait d’être situé dans l’intervalle de
conﬁance de niveau 1 − α. On peut donc voir aussi l’IC comme l’ensemble des
valeurs acceptées par le test. Essayons de formaliser cela dans la généralité.
IC dérivé d’un test
Soit pour l’hypothèse nulle H0 : θ = θ0 un test de niveau α déﬁni par
la région d’acceptation A(θ0 ) ⊂ Rn donc telle que Pθ0 (A(θ0 )) = 1 − α. Faisant varier θ0 dans Θ, on peut ainsi construire, pour chaque valeur de θ ∈ Θ,
une région d’acceptation qui dépend de cette valeur, notée A(θ) et telle que
Pθ (A(θ)) = 1 − α.
Soit maintenant une région de Θ construite de la façon suivante sur la base
d’une réalisation (x1 , x2 , · · · , xn ). On considère chaque valeur de θ dans Θ et
l’on inclut cette valeur dans la région si et seulement si (x1 , x2 , · · · , xn ) ∈ A(θ).
Passant
à
l’univers
des
réalisations
possibles
symbolisé
par
(X1 , X2 , · · · , Xn ), la région ainsi déﬁnie devient aléatoire. Or, pour un θ donné,
la probabilité que (X1 , X2 , · · · , Xn ) appartienne à A(θ) est égale à 1 − α par
construction même de A(θ). Comme il y a identité entre cet événement et le
fait d’inclure cette valeur de θ dans la région de Θ, la probabilité que θ soit
compris dans cette région (aléatoire) est égale à 1 − α. Ainsi on a construit une
procédure de région de conﬁance de niveau 1 − α pour le paramètre inconnu θ.
Quand Θ ⊆ R cette région sera généralement un intervalle quel que soit θ et
on aura une procédure d’intervalle de conﬁance.
On peut montrer que les propriétés d’optimalité du test se transfèrent à
la procédure d’IC. Ainsi un test UPP donnera une procédure uniformément

Chapitre 9. Tests d’hypothèses paramétriques

243

plus précise (voir section 7.7). Un test sans biais impliquera que la procédure
fournira une probabilité plus faible d’inclure une fausse valeur de θ que d’inclure
la vraie valeur. En général si l’alternative est bilatérale la région d’acceptation
est un intervalle à bornes ﬁnies et il en est de même pour l’IC. Une alternative
unilatérale conduira à un intervalle de conﬁance ouvert sur l’inﬁni pour un côté.
Dans le cas discret la dualité n’est valable qu’en «randomisant» les bornes
de l’intervalle. Il sera toutefois plus simple de partir de régions d’acceptation
conservatrices pour aboutir à un IC de niveau de conﬁance supérieur ou égal à
1 − α.
Pour illustrer l’intérêt de cette dualité prenons la méthode du test par le
rapport de vraisemblance (généralisé) avec Θ ⊆ R. En vertu de ses propriétés
asymptotiques on a une région d’acceptation au niveau approximatif α déﬁnie
par (voir note 9.4) :
2 (1)

−2 ln λ(x1 , x2 , ..., xn ) ≤ χ1−α
ou
ou

1

2 (1)

λ(x1 , x2 , ..., xn ) ≥ e− 2 χ1−α

2 (1)

1
f (θ; x1 , x2 , ..., xn ) ≥ e− 2 χ1−α f (θn ; x1 , x2 , ..., xn ).

Pour un échantillon réalisé x1 , x2 , · · · , xn on en déduit une région de conﬁance
de niveau 1−α en considérant l’ensemble des valeurs de θ vériﬁant cette dernière
inégalité, cette région étant généralement un intervalle étant donné les propriétés du RVG. Ainsi avec α =0,05 , on obtient un IC de niveau approximatif
0,95 contenant les valeurs de θ pour lesquelles la densité (ou la fonction de
probabilité) conjointe aux valeurs observées x1 , x2 , · · · , xn n’est pas inférieure
à 0,147 fois leur densité maximale f (θn ; x1 , x2 , ..., xn ) atteinte au maximum de
vraisemblance θn (voir note 9.4). Ceci est représenté sur la ﬁgure 9.2.
D’un point de vue numérique on voit qu’il suﬃt de connaı̂tre l’expression
de la densité (ou de la fonction de probabilité) conjointe des observations pour
donner un intervalle de conﬁance approximatif sur le paramètre inconnu.
Test dérivé d’un IC
On peut également envisager une démarche inverse permettant de déboucher sur un test à partir d’une procédure d’IC. Soit une famille d’intervalles
[t1 (x1 , x2 , ..., xn ) , t2 (x1 , x2 , ..., xn )] déﬁnie pour toute réalisation (x1 , x2 , ..., xn ),
où t1 (x1 , x2 , ..., xn ) et t2 (x1 , x2 , ..., xn ) sont à valeurs dans Θ ⊆ R, issue d’une
procédure d’IC de niveau 1 − α.
Pour tout θ0 on peut déﬁnir un test H0 : θ = θ0 consistant à accepter
H0 si et seulement si θ0 appartient à [t1 (x1 , x2 , ..., xn ), t2 (x1 , x2 , ..., xn )]. Par
construction de la procédure d’IC, pour toute valeur de θ on a :
Pθ (θ ∈ [t1 (X1 , X2 , · · · , Xn ), t2 (X1 , X2 , · · · , Xn )] ) = 1 − α.

Statistique − La théorie et ses applications

244

L(θ)
L(θˆn )

0,147 L (θˆn )

θˆn
θ1

IC 0,95

θ2

θ

Figure 9.2 - Intervalle de conﬁance dérivé du test du rapport de vraisemblance
généralisé.

Ceci est vrai en particulier pour θ0 et la probabilité d’accepter cette valeur sous
H0 est aussi égale à 1 − α. On a bien un test de niveau α.
Ainsi, par exemple, on peut recourir à la méthode des quantiles (voir section
7.5), facile à mettre en oeuvre, pour tester une valeur de θ : il suﬃt de voir si
cette valeur est ou non à l’intérieur des limites de conﬁance. En particulier les
abaques de conﬁance pour le paramètre p de la loi de Bernoulli ou λ de la loi
de Poisson peuvent être utilisés dans cette optique.
Nous concluons en disant qu’un intervalle de conﬁance donne une information plus riche qu’un simple test car il indique l’ensemble des valeurs qui
seraient acceptables via le test dual.
Pour approfondir la théorie des tests on pourra consulter l’ouvrage de
référence de Lehmann (1986) ou celui de Shao (1999). Par ailleurs on trouvera
dans le livre de Saporta (1990) un vaste éventail de méthodes où s’appliquent
les tests les plus divers.

9.9

Exercices

Exercice 9.1 Soit X1 , X2 , · · · , Xn issus d’une loi E(λ). On souhaite tester
H0 : λ = 1/2 vs. H1 : λ = 1. Quelle est la région de rejet au niveau 0,05 pour

Chapitre 9. Tests d’hypothèses paramétriques

245

le test du RV simple ?
Aide : on pourra utiliser le fait qu’une loi Γ(n, 1/2) est une loi χ2 (2n).
Exercice 9.2 Soit un échantillon aléatoire X1 , X2, ... , Xn issu d’une loi géométrique de paramètre inconnu p dont nous rappelons la fonction de probabilité :
f (x; p) = p(1 − p)x ; x = 0, 1, 2, ...
n
Expliquer pourquoi i=1 Xi suit une loi binomiale négative de paramètres n
1
2
et p. Soit à tester H0 : p = versus H1 : p = .
3
3
Montrer que la région
critique pour le test fondé sur le rapport de vraisemblance

est de la forme ni=1 xi < k.
Donner la règle de décision pour n = 4 et α = 0,05.
Quelle est la puissance de ce test ?
Exercice 9.3 Soit X1 , X2 , · · · , Xn issus de la loi N (μ , 1). Pour tester H0 :
μ ≤ 5 vs. H1 : μ > 5 on adopte la région de rejet :
1
{ (x1 , x2 , · · · , xn ) | x > 5 + √ }.
n
Quel est le risque de première espèce de ce test ? Déterminer sa fonction puissance.
Exercice 9.4 Soit la famille de lois de Pareto de paramètres a connu et θ
inconnu (voir section 4.2.6). Montrer qu’elle est à RV monotone. En déduire le
test UPP pour H0 : θ ≥ θ0 vs. H1 : θ < θ0 .
Application : pour a = 1 construire le test de l’hypothèse nulle : la moyenne
de la loi est inférieure ou égale à 2. (aide : la moyenne est θa/(θ − 1)).
Exprimer la valeur critique pour α = 0,05.
Aide : montrer que ln X suit une loi exponentielle.
Exercice 9.5 Soit la famille de lois U[0, θ] de paramètre θ inconnu. Montrer
qu’elle est à rapport de vraisemblance non croissant. En déduire un test UPP
de niveau α > 0 pour des alternatives unilatérales sur θ.
Exercice 9.6 Soit la situation du tirage sans remise de n individus dans une
population de N individus dont M ont un certain caractère, M étant inconnu.
On considère le nombre X ; H(N, M, n) d’individus ayant ce caractère dans
un échantillon de taille n.
Soit l’hypothèse H0 : M ≥ M0 vs. H1 : M < M0 , montrer que le test avec
région de rejet x ≤ cα , où cα est le quantile d’ordre α sur la loi H(N, M0 , n),
est UPP.
Aide : montrer que la famille des lois hypergéométriques avec M inconnu
est à rapport de vraisemblance croissant. Pour cela il suﬃra de montrer que
L(M + 1; x)/L(M ; x) est une fonction croissante de x.

246

Statistique − La théorie et ses applications

Exercice 9.7 Soit la famille de lois exponentielles E(λ). Sur la base d’une seule
observation donner, en accord avec la note 9.3, les deux équations conduisant
au choix des valeurs critiques pour le test UPP parmi les tests sans biais de
H0 : λ = λ0 vs. H1 : λ = λ0 . Montrer que la région de rejet n’est pas répartie
de façon égale selon un risque de première espèce α/2 sur chaque extrémité
(on pourra constater que le déséquilibre peut être très prononcé sur un cas
particulier en se donnant λ0 et α, et en recourant à un logiciel de résolution
d’équations).
Exercice 9.8 Soit une loi mère N (μ, σ 2 ), où μ est inconnu mais σ 2 est connu,
et la situation de test H0 : μ1 ≤ μ ≤ μ2 vs. H1 : μ < μ1 ou μ > μ2 . Déterminer
la forme de la région de rejet pour le test du RVG au niveau α. En admettant
que le risque de première espèce est maximal en μ = μ1 et μ = μ2 , et en
2
prenant naturellement des valeurs critiques symétriques par rapport à μ1 +μ
2
donner l’équation déﬁnissant ces valeurs critiques (ceci correspond au test UPP
parmi les tests sans biais). Application : résoudre approximativement l’équation
pour μ1 = 4, μ2 = 5, σ 2 = 1 et α = 0, 05. Tracer en un seul graphe la fonction
puissance (avec un choix de n) et la variation du risque de première espèce.
Déduire du test précédent le test UPP-sans biais pour H0 : μ = μ0 vs.
H1 : μ = μ0 et σ 2 connu. Montrer également que, dans ce cas, la loi asymptotique de −2 ln Λn , où Λn est le RVG, est en fait la loi exacte.
Exercice 9.9 Soit à tester H0 : λ = λ0 vs. H1 : λ = λ0 pour le paramètre λ
de la loi E(λ) à partir d’un échantillon de taille n. Établir formellement le test
du RVG. Application : établir le test pour λ0 = 1/4 et n = 30 en utilisant la
loi asymptotique du RVG.
Exercice 9.10 Soit une loi (de Raleigh) de densité f (x; a) = 2ax exp{−ax2 },
x ≥ 0, a > 0. Donner la statistique du RVG pour tester H0 : a = 1 vs.
H1 : a = 1 .
Exercice 9.11 Soit la loi de Pareto de paramètre a = 2 et θ inconnu. Donner
vs. H1 : θ = 3 en utilisant la loi asymptotique
le test du RVG pour H0 : θ = 3 
30
du RVG. Application : n = 30, i=1 ln xi = 31.
Exercice 9.12 Soit une loi mère P(λ) où λ est inconnu. On veut tester H0 :
λ = λ0 vs. H1 : λ = λ0 . Montrer que la région de rejet pour le test du RVG est
de la forme x ∈
/ [c1 , c2 ] avec une certaine contrainte liant c1 et c2 .
Application : pour λ0 = 5 et n = 10 résoudre au plus proche de la solution du
RVG en utilisant une région de rejet conservatrice par rapport au niveau 0,05.
Calculer le niveau exact de cette règle.
* Construire une règle de rejet «randomisée» selon le principe de la note 9.1.
Exercice 9.13 Montrer que le test du RVG conduit à la même statistique de
test que le test classique de Student vu en section 9.7.1 pour le test sur la
moyenne d’une loi de Gauss.

Chapitre 9. Tests d’hypothèses paramétriques

247

Exercices appliqués3
Exercice 9.14 Un producteur de pneus envisage de changer la méthode de
fabrication. La distribution de la durée de vie de ses pneus traditionnels est
connue : moyenne 64 000 km, écart-type 8 000 km ; elle est pratiquement gaussienne. Dix pneus sont fabriqués avec la nouvelle méthode et une moyenne de
67 300 km est constatée. En supposant que la nouvelle fabrication donnerait
une distribution à peu près gaussienne et de même variance, testez l’eﬃcacité
de la nouvelle méthode au niveau α = 0, 05. Tracez la fonction puissance de ce
test.
(aide : test de H0 : μ ≤ μ0 )
Exercice 9.15 Une étude approfondie a évalué à 69 800 euros/an le revenu
moyen imposable par ménage résidant à Neuilly-sur-Seine. Une enquête est
eﬀectuée auprès de 500 ménages pris au hasard, aﬁn de contrôler le résultat
de l’étude. Dans l’enquête on trouve une moyenne de 68 750 euros/an avec un
écart-type de 10 350 euros/an. Quelle est la P-valeur associée aux résultats du
contrôle ?
(aide : test de H0 : μ = μ0 )
Exercice 9.16 En un point de captage d’une source on a répété six mesures
du taux d’oxygène dissous dans l’eau (en parties par million). On a trouvé :
4,92

5,10

4,93

5,02

5,06

4,71.

La norme en dessous de laquelle on ne doit pas descendre pour la potabilité
de l’eau est 5 ppm. Au vu des observations eﬀectuées peut-on avec un faible
risque d’erreur aﬃrmer que l’eau n’est pas potable (admettre une distribution
quasi-gaussienne des aléas des mesures) ?
(aide : test de H0 : μ ≥ μ0 )
Exercice 9.17 Un service chargé de traiter des formulaires standard utilise un
réseau de dix micro-ordinateurs et une imprimante. Le temps moyen d’attente
en impression d’un formulaire est de 42,5 secondes (le temps entre l’envoi de la
commande d’impression et la réalisation de l’impression du formulaire).
Dix nouveaux micros et une imprimante sont ajoutés au réseau. Sur trente
demandes d’impression dans cette nouvelle conﬁguration on a constaté un
temps moyen de 39,0 secondes et un écart-type de 8,2 secondes.
Tester l’hypothèse que le temps moyen d’impression n’a pas été aﬀecté par
l’accroissement du réseau.
(aide : test de H0 : μ = μ0 )
3 Un ou deux de ces exercices appliqués sont des adaptations d’emprunts dont nous ne
sommes plus en mesure de retrouver la source. Nous nous en excusons auprès des involontaires
contributeurs.

248

Statistique − La théorie et ses applications

Exercice 9.18 On veut tester la précision d’une méthode de mesure d’alcoolémie sur un échantillon sanguin. La précision est déﬁnie comme étant
égale à deux fois l’écart-type de l’aléa (supposé pratiquement gaussien) de la
méthode.
On partage l’échantillon de référence en 6 éprouvettes que l’on soumet à
l’analyse d’un laboratoire. Les valeurs trouvées en g/litre sont :
1, 35

1, 26

1, 48

1, 32

1, 50

1, 44 .

Tester l’hypothèse nulle que la précision est inférieure ou égale à 0,1 g/litre au
niveau 0,05. Donner la P-valeur du résultat.
(aide : test de H0 : σ 2 ≤ σ02 )
Exercice 9.19 On sait que dans la population générale du nord de l’Italie le
pourcentage de prématurés (naissance avant le 8ème mois) est de 4 %. Dans une
région du nord de l’Italie contaminée par une pollution chimique on a observé
sur les dernières années 72 naissances prématurées sur 1 243 accouchements.
Y-a-t-il lieu, selon la P-valeur constatée, de penser que la proportion de
prématurés est plus élevée dans cette région que dans l’ensemble de la population du Nord du pays ? Donnez la fonction puissance du test de niveau 0,01.
(aide : test de H0 : p ≤ p0 )
Exercice 9.20 Le fournisseur d’un lot de 100 000 puces aﬃrme que le taux de
puces défectueuses ne dépasse pas 4 %. Pour tester cette hypothèse 800 puces
prises au hasard sont contrôlées et l’on en trouve 40 défectueuses. Eﬀectuer un
test de niveau 0,05.
(aide : test de H0 : p ≤ p0 )
Exercice 9.21 Dans une étude on a mesuré le taux de plomb dans le sang (en
mg/litre) de 67 enfants tirés au hasard dans les classes primaires d’une ville,
dont 32 ﬁlles et 35 garçons. Pour les ﬁlles on a trouvé une moyenne de 12,50
avec une variance de 3,39. Pour les garçons on a trouvé, respectivement, 12,40
et 3,94. Le taux moyen est-il signiﬁcativement diﬀérent entre garçons et ﬁlles ?
Quelle hypothèse supplémentaire doit-on faire pour pouvoir répondre à cette
question ?
(aide : test de H0 : μ1 = μ2 )
Exercice 9.22 L’an dernier, on a observé sur un échantillon de 29 appartements de 3 pièces situés en ville des dépenses de chauﬀage égales en moyenne
à 325 euros, avec un écart-type égal à 26 euros.
Cette année, pour un nouvel échantillon de 31 appartements de 3 pièces en
ville on a trouvé des valeurs respectives de 338 euros et 28 euros. L’hypothèse
à laquelle on s’intéresse est qu’il n’y a pas eu d’augmentation des dépenses, en
moyenne entre les deux années.

Chapitre 9. Tests d’hypothèses paramétriques

249

a) En supposant que toutes les conditions nécessaires à la validité du test utilisé
sont remplies, eﬀectuer un test de niveau 0,05 pour l’hypothèse ci-dessus.
b) Donner les conditions nécessaires pour que la procédure de test utilisée soit
applicable.
(aide : test de H0 : μ1 ≥ μ2 )
Exercice 9.23 Pour tester l’eﬃcacité d’un traitement destiné à augmenter
le rythme cardiaque, on a mesuré sur 5 individus ce rythme avant et après
administration du traitement. Est-il eﬃcace ?
Avant
Après

80
84

90
95.5

70.3
73.5

85
86

63
62

On supposera que le rythme cardiaque se répartit de façon quasi gaussienne
pour la population considérée (avant comme après traitement).
(aide : test «apparié» H0 : μ1 ≥ μ2 )
Exercice 9.24 Une entreprise qui commercialise des abonnements pour un
opérateur de téléphonie mobile, applique un nouveau régime horaire à ses employés. Pour 16 vendeurs pris au hasard, elle comptabilise le nombre d’abonnements vendus le mois précédant l’application du régime et le mois suivant :
vendeur
Mois précédent
Mois suivant
vendeur
Mois précédent
Mois suivant

1
39
43
9
69
69

2
28
51
10
41
43

3
67
64
11
52
53

4
45
35
12
60
47

5
28
18
13
50
34

6
73
53
14
46
39

7
67
66
15
53
49

8
53
61
16
47
56

En se fondant sur cette information, la direction annonce que le nouveau
régime provoque une baisse importante des ventes. Cette aﬃrmation est–elle
justiﬁée ? Donner une valeur approchée de la P–valeur du test eﬀectué. On
admettra que la loi du nombre de ventes mensuelles est suﬃsamment proche
d’une loi normale.
(aide : test «apparié» H0 : μ1 ≤ μ2 )
Exercice 9.25 Un nouveau vaccin contre le paludisme est expérimenté auprès
de la population d’une ville d’Afrique.
On prend deux échantillons A et B de 200 personnes chacun. On injecte le
vaccin aux individus de l’échantillon A et un placebo à ceux de l’échantillon B.
Au bout d’un an on constate que 40 personnes de l’échantillon A ont des accès
de palustres et 80 de l’échantillon B. Que dire de l’eﬃcacité du vaccin ?
(aide : test de H0 : p1 ≥ p2 )

250

Statistique − La théorie et ses applications

Exercice 9.26 Suite à des sondages, l’institut A donne 510 personnes favorables à telle mesure sur 980 personnes interrogées, l’institut B donne 505
favorables sur 1 030.
La diﬀérence des estimations de la proportion de personnes favorables estelle signiﬁcative ?
(aide : test de H0 : p1 = p2 )

Chapitre 10

Tests pour variables
catégorielles et tests
non paramétriques
Dans ce chapitre nous considérons tout d’abord la généralisation des tests
des sections 9.7.5 et 9.7.6 concernant des variables de Bernoulli, à des variables
catégorielles. Les tests sur des variables catégorielles sont voisins dans leur
esprit des tests non paramétriques et certains d’entre eux sont eﬀectivement de
nature non paramétrique. C’est pourquoi nous regroupons ces deux types de
tests dans un même chapitre.
Une variable catégorielle est une extension d’une variable de Bernoulli au
sens où il n’y a plus deux mais c ≥ 2 résultats possibles. Il s’agit d’une variable
aléatoire non pas à valeurs dans R comme les v.a. usuelles mais à valeurs dans
un ensemble de catégories. Ce sera, par exemple, la réponse d’un individu à une
question à c modalités dans une enquête par sondage. Une variable catégorielle
peut être une variable purement qualitative (ou nominale) ou une variable
ordinale si les catégories sont ordonnées. Elle peut aussi résulter d’une mise
en catégories d’une variable quantitative (par exemple constitution de classes
d’âge ou de revenus).
Une variable catégorielle est parfaitement déﬁnie par les probabilités respectives p1 , p2 , · · · , pc des c catégories. La somme des probabilités étant égale à
1, il y a en vérité c − 1 paramètres libres. Dans le contexte d’un tirage aléatoire
d’un individu dans une population ﬁnie, exposé en section 2.4, p1 , p2 , · · · , pc
coı̈ncident avec les fréquences relatives (ou proportions) des c catégories dans
cette population.
D’une façon générale nous nous intéresserons à l’observation de la variable
catégorielle sur un n−échantillon aléatoire, par exemple les catégories observées
sur n individus tirés au hasard dans une population. Seules importent (comme

Statistique − La théorie et ses applications

252

on le justiﬁera dans la note 10.1) les variables aléatoires N1 , N2 , ..., Nc correspondant aux fréquences respectives des c catégories après n observations
répétées et indépendantes de la variable catégorielle. Leur loi conjointe est la
loi multinomiale décrite en section 4.1.6 en tant qu’extension de la loi binomiale de deux à c catégories. Cette loi sera donc le point de départ des tests
concernant les variables catégorielles.

10.1

Test sur les paramètres d’une loi
multinomiale

Dans les mêmes notations que ci-dessus, la fonction de probabilité conjointe
des v.a. N1 , N2 , · · · , Nc est :
P (N1 = n1 , N2 = n2 , · · · , Nc = nc ) =

n!
pn1 pn2 · · · pnc c
n1 !n2 ! · · · nc ! 1 2

c
si j=1 nj = n et 0 sinon (voir section 4.1.6). Ces variables aléatoires ne sont
pas indépendantes puisque leur somme doit être égale à n.
On s’intéresse à l’hypothèse nulle :
H0 : p1 = p01 , p2 = p02 , · · · , pc = p0c
c
où les p0j sont des valeurs de probabilités spéciﬁées telles que j=1 p0j = 1,
l’hypothèse alternative étant qu’il existe au moins une catégorie j telle que
pj = p0j (en fait il y en aura au moins deux puisque le total doit rester égal à
1). Nous présentons deux approches de test dont on verra qu’elles sont asymptotiquement équivalentes.

10.1.1

Test du rapport de vraisemblance généralisé

Soit le RVG :
n!
(p01 )n1 (p02 )n2 · · · (p0c )nc
n1 !n2 ! · · · nc !
λ(n1 , n2 , · · · , nc ) =
n!
p n1 pn2 · · · pcnc
n1 !n2 ! · · · nc ! 1 2
où (
p1 , p2 , · · · , pc ) est l’estimation du MV de (p1 , p2 , · · · , pc ). Montrons que pj
est égal à nj /n, la proportion observée dans l’échantillon pour la
catégorie j.
c
En remplaçant pc par 1−p1 −· · ·−pc−1 pour intégrer la contrainte j=1 pj = 1
dans la recherche du maximum, la fonction de vraisemblance s’écrit :
n!
nc−1
pn1 pn2 · · · pc−1
(1 − p1 − · · · − pc−1 )nc
n1 !n2 ! · · · nc ! 1 2

si

c

j=1 nj

= n,

Chapitre 10. Tests non paramétriques

253

d’où la log-vraisemblance :
ln(
si

n!
) + n1 ln p1 + · · · + nc−1 ln pc−1 + nc ln(1 − p1 − · · · − pc−1 )
n1 !n2 ! · · · nc !

c

j=1 nj

= n.

En annulant la dérivée par rapport à chacun des paramètres p1 , p2 , · · · , pc−1
on obtient les c − 1 équations suivantes :
⎧ n
nc
1
⎪
−
=0
⎪
⎪
p
1
−
p
−
· · · − pc−1
⎪
1
1
⎪
⎪
⎪
⎪
nc
n2
⎪
⎨
−
=0
p2 1 − p1 − · · · − pc−1
⎪
⎪
⎪
···
⎪
⎪
⎪
⎪
n
nc
⎪
c−1
⎪
⎩
−
=0
pc−1 1 − p1 − · · · − pc−1

soit

c
n1
n2
nc−1
nc
j=1 nj
=
=
=
= c
= n,
p1
p2
pc−1
pc
j=1 pj

d’où (en admettant que la solution unique aux équations donne bien un maximum), pour tout j, la solution pj = nj /n.

Le rapport de vraisemblance pour tester H0 est donc :
(p01 )n1 (p02 )n2 · · · (p0c )nc
n1 n1 n2 n2
nc nc
···
n
n
n
n
c 
$
np0j j
=
.
nj
j=1

λ(n1 , n2 , · · · , nc ) =

En utilisant le théorème asymptotique 9.2 on a comme région de rejet de
niveau (approximativement) α :
−2 ln λ = 2

c

j=1

nj ln

nj
2 (c−1)
> χ1−α .
np0j

Notons que la loi du khi-deux a c − 1 degrés de liberté, car il n’y a que c − 1
paramètres libres spéciﬁés par H0 . Dans ce contexte de variables catégorielles la
statistique −2 ln λ est appelée déviance. Certains logiciels l’appellent toutefois
rapport de vraisemblance (alors que celui-ci est λ). Par commodité nous nous
autoriserons aussi ce glissement de langage.

254

Statistique − La théorie et ses applications

Note 10.1 Chaque observation d’une variable catégorielle peut être décrite par un
vecteur «indicateur» (X1 , X2 , · · · , Xc ) tel que la j -ème composante prenne la valeur
1 si le résultat est la j -ème catégorie, les autres composantes prenant alors la valeur
0. La densité conjointe des c composantes est :
f (x1 , x2 , · · · , xc ; p1 , p2 , · · · , pc ) = px1 1 px2 2 · · · pxc c
c
où, pour tout j, xj ∈ {0, 1} et j=1 xj = 1. Pour n observations répétées on a une
suite de n vecteurs : {(X1i , X2i , · · · , Xci ), i = 1, · · · , n}, avec densité conjointe :
n
$

px1 1i px2 2i · · · pxc ci = pn1 1 pn2 2 · · · pnc c

i=1

c

si
j=1 nj = n (0 sinon) où nj est le nombre d’observations tombant dans la
catégorie j parmi les n observations. Donc (N1 , N2 , · · · , Nc ) est une statistique
exhaustive. De plus on voit que lorsque l’on fait un rapport de vraisemblance pour
la suite des n vecteurs, on obtient la même expression qu’avec la loi multinomiale,
les termes avec factoriels de cette dernière s’éliminant. Ceci justiﬁe le fait de passer
directement par la loi multinomiale pour construire le test.

10.1.2

Test du khi-deux de Pearson

Ce test est historiquement le premier à avoir été proposé bien avant le
développement formel de la théorie des tests par Jerzy Neyman et par Egon
Pearson à partir de 1930. Il a été mis au point vers 1900 par Karl Pearson, le
père d’Egon, aﬁn de vériﬁer sur des données biologiques certaines hypothèses
tenant aux facteurs d’hérédité.
En utilisant des approximations gaussiennes Karl Pearson a montré que la
statistique de test
c

(Nj − np0j )2
Q=
np0j
j=1
admet, sous H0 , une loi asymptotique χ2 (c − 1). C’est pourquoi Q est couramment appelée statistique du khi-deux ( ou statistique de Pearson).
%
Remarquons que, sous l’hypothèse H0 , (Nj − np0j )/ np0j (1 − p0j ) est la
variable centrée-réduite de Nj dont la loi marginale est la loi binomiale B(n, p0j )
(voir section 4.1.6). Asymptotiquement cette v.a. suit
c une loi N (0 ; 1) et son
carré une loi χ2 (1). On montre que la contrainte j=1 Nj = n a pour eﬀet
d’éliminer les facteurs (1 − p0j ) pour donner une loi asymptotique du khi-deux
à c − 1 degrés de liberté. Intuitivement on voit que la valeur prise par Q est
d’autant plus petite que les fréquences observées sont proches des np0j appelées
fréquences attendues (ou fréquences théoriques) sous H0 . On ne rejette donc
l’hypothèse nulle que pour de grandes valeurs de réalisation q de Q, à savoir
2 (c−1)
lorsque q > χ 1−α pour un test de niveau (approximatif) α.

Chapitre 10. Tests non paramétriques

10.1.3

255

Équivalence asymptotique des deux tests

Nous donnons une démonstration abrégée de cette équivalence, des développements plus rigoureux se trouvant dans les ouvrages cités en référence (en ﬁn
de section 10.3). Pour ne pas alourdir les notations nous n’indiçons pas par n
les suites de v.a. ou de réalisations dont on considère ici la convergence quand
n → ∞.
N

Pour toute composante Nj du vecteur aléatoire (N1 , N2 , ..., Nc ), sous H0 , nj
tend presque sûrement vers p0j quand n → ∞ par la loi des grands nombres
p.s.
N −np
et, par conséquent, jnp0j 0j −→ 0. Pour toute réalisation1 (n1 , n2 , ..., nc ) de
(N1 , N2 , ..., Nc ) on a donc :
nj − np0j
−→ 0 quand n → ∞ , pour tout j.
np0j
n −np

n

Posant hj = jnp0j 0j on peut écrire nj = np0j (1 + hj ) et ln npj0j = ln(1 + hj ).
n
Pour chaque terme nj ln npj0j dans l’expression de la réalisation de la déviance
(i.e. −2 ln λ), développons ce logarithme au voisinage de 1 selon ln(1 + x) =
x − 12 x2 + O(x3 ) pour obtenir :
nj ln

nj
1
= np0j (1 + hj )(hj − h2j + O(h3j ))
np0j
2
1 2
= np0j (hj + hj + O(h3j ))
2
1 (nj − np0j )2
= nj − np0j +
+ np0j O(h3j ) .
2
np0j

Le terme np0j O(h3j ) qui est négligeable devant les deux autres termes est
d’ordre :
3

(1 − p0j )3/2
(nj − np0j )3
nj − np0j
%
=
.
(np0j )
√
(np0j )3
np0j
np0j (1 − p0j )
L’expression entre crochets étant
√ une réalisation d’une variable aléatoire N (0 ; 1),
le terme négligé est d’ordre 1/ n.
La réalisation de la déviance est donc :
c


c
c


(nj − np0j )2
nj
1
2
nj ln
=2
(nj − np0j ) +
+ O( √ ).
np
np
n
0j
0j
j=1
j=1
j=1

c
Comme
j=1 (nj − np0j ) = 0 on voit que la réalisation de la déviance est
équivalente à la réalisation q de la statistique du khi-deux quand n → ∞. Ceci
1 Plus formellement, se référant à la convergence presque sûre, on devrait dire «pour
presque toute réalisation».

Statistique − La théorie et ses applications

256

a pour conséquence que les statistiques du rapport de vraisemblance2
et du khi-deux ont la même loi asymptotique sous H0 . Elles conduisent
à des régions de rejet identiques et, donc, à des tests équivalents.
Note 10.2 Il est important de remarquer que le seul point crucial de la démonstration de l’équivalence asymptotique des deux statistiques est que
(nj − np0j )/np0j tende vers 0 quand n → ∞ ou, de même, que (ni /n)/p0j
tende vers 1. Ainsi on pourrait avoir en lieu et place de p0j une estimation
convergente de cette valeur. Dans les sections suivantes on aura à eﬀectuer de
telles estimations et on admettra alors que les deux statistiques conservent la
même loi asymptotique.
En pratique on préfère utiliser le test du khi-deux qui met en évidence les
écarts entre les fréquences observées et les fréquences attendues. D’autant plus
que la statistique du khi-deux converge plus vite que celle du RV et donne
donc une meilleure approximation (voir Agresti, 2002). On considère généralement, comme conditions de validité de l’approximation asymptotique, que les
fréquences attendues np0j doivent rester supérieures à 5. Lorsque ces
conditions ne sont pas remplies on s’arrange pour regrouper certaines catégories
proches.
Nous ne nous intéresserons pas à la puissance de ce test qui est un problème
complexe vu la multiplicité de formes que peut revêtir l’hypothèse alternative.
Ceci sera vrai a fortiori pour les tests introduits dans les sections suivantes.

10.1.4

Cas particulier de la loi binomiale

Appliquons la formule du khi-deux avec c = 2, p01 = p0 , p02 = 1 − p0 et
n2 = n − n1 . On a :
(n1 − np0 )2
(n − n1 − n(1 − p0 ))2
+
np0
n(1 − p0 )
(n1 − np0 )2
(n1 − np0 )2
+
=
np0
n(1 − p0 )
(
p − p0 )2
(n1 − np0 )2
=
=
p0 (1 − p0 )
np0 (1 − np0 )
n

q=

en posant p = n1 /n pour la fréquence relative de succès observée.
Remarquons maintenant que si Z est une v.a. de loi N (0 ; 1) alors, avec les
notations usuelles pour les quantiles de cette loi :
P (−z1−α/2 < Z < z1−α/2 ) = P (Z 2 < (z1−α/2 )2 ) = 1 − α.
2 Plus

exactement la statistique de la déviance.

Chapitre 10. Tests non paramétriques

257

Comme Z 2 suit une loi χ2 (1) on a, pour les quantiles, l’égalité (z1−α/2 )2 =
2 (1)
2 (1)
χ1−α . Ainsi la région d’acceptation q < χ1−α du test du khi-deux est identique
à :
p − p0
< z1−α/2
−z1−α/2 < )
p0 (1 − p0 )
n
qui est celle du test classique par approximation gaussienne pour une proportion
proposé en section 9.7.5 pour le test bilatéral H0 : p = p0 vs. H1 : p = p0 .

10.2

Test de comparaison de plusieurs lois
multinomiales

Ce test est une double généralisation du test de comparaison de deux lois
de Bernoulli vu en section 9.7.6, considérant à la fois plusieurs catégories et
plusieurs lois.
Soit J lois multinomiales ayant les mêmes catégories, en nombre I, et soit
pij la probabilité d’être dans la catégorie i pour la loi j. L’hypothèse nulle à
tester est que les probabilités associées aux I catégories sont identiques pour
toutes les J lois, soit :
H0 : pi1 = pi2 = · · · = piJ

,

i = 1, . . . , I ,

l’hypothèse alternative étant que pour au moins une catégorie (en fait il y
en aura au moins deux puisque le total doit rester égal à 1) ces probabilités
diﬀèrent pour au moins deux lois. Si l’on se réfère à la comparaison de populations ce test est un test d’homogénéité des populations au sens où H0
signiﬁe que la variable catégorielle étudiée se distribue de façon identique dans
ces populations (voir les exercices appliqués pour illustration).
Dans ce problème on est en présence de (I − 1)J paramètres inconnus dont
seulement (I − 1)(J − 1) sont spéciﬁés par H0 . En eﬀet, pour la catégorie i par
exemple, on peut écrire, en prenant la J-ème loi pour référence, pi1 = piJ + θ1 ,
pi2 = piJ + θ2 , . . . , pi,J−1 = piJ + θJ−1 . Pour cette catégorie, H0 équivaut donc
à θ1 = θ2 = . . . = θJ−1 = 0. Comme il suﬃt que ce type d’égalité soit vériﬁé
pour I − 1 catégories, l’égalité étant nécessairement vériﬁée pour la catégorie
restante, on a bien (I − 1)(J − 1) paramètres spéciﬁés par H0 .
On considère J échantillons mutuellement indépendants de tailles nj où
j = 1, 2, . . . , J, issus respectivement des J lois. Soit Nij la fréquence de la
catégorie i pour la loi multinomiale j, N
i.J le total des fréquences pour la
catégorie i sur l’ensemble des lois et n = j=1 nj l’eﬀectif englobant tous les
J échantillons. On pourrait développer aisément le test du rapport de vraisemblance. Par application du théorème 9.2, la statistique du RVG suit asymptotiquement, sous H0 , une loi du khi-deux à (I −1)(J −1) degrés de liberté. Comme

Statistique − La théorie et ses applications

258

précédemment nous préférons utiliser la statistique Q de Pearson. Toutefois,
ici, les pij ne sont pas totalement spéciﬁés sous H0 et il faut les estimer pour
calculer les fréquences attendues. En recourant aux estimateurs du maximum
de vraisemblance qui sont convergents, les deux tests restent asymptotiquement équivalents (voir note 10.2). La loi asymptotique de Q sera donc la loi du
khi-deux à (I − 1)(J − 1) degrés de liberté.
Sous H0 notons pi la probabilité de la catégorie i, commune à toutes les lois
(soit pi = pi1 = pi2 = · · · = piJ ) et déterminons les estimateurs du MV des pi .
Pour un échantillon, disons l’échantillon j, la vraisemblance est, comme vu au
début de la section 10.1.1,
nj !
n
n
nI−1,j
p 1j p 2j · · · pI−1
(1 − p1 − · · · − pI−1 )nIj
n1j !n2j ! · · · nIj ! 1 2

si

J

i=1 nij

= nj .

Pour l’ensemble des J échantillons la vraisemblance globale est le produit des
vraisemblances de chaque échantillon puisque ceux-ci sont indépendants, soit :
J
$

nj !
n
n
nI−1,j
p1 1j p2 2j · · · pI−1
(1 − p1 − · · · − pI−1 )nIj
n
!n
!
·
·
·
n
!
1j
2j
Ij
j=1
⎞
⎛
J
$
n
!
j
⎠ pn1 1. pn2 2. · · · pnI−1,. (1 − p1 − · · · − pI−1 )nI. ,
=⎝
I−1
n
!n
!
·
·
·
n
!
1j
2j
Ij
j=1
où ni. est le total observé pour la catégorie i sur toutes les lois.
On est ramené au même problème de maximisation qu’en section 10.1.1,
les ni. se substituant aux ni . Le maximum est donc atteint pour pi = ni. /n .
L’estimateur du maximum de vraisemblance de pi est donc l’estimateur naturel
Ni. /n égal à la fréquence relative de la catégorie i obtenue en fusionnant les J
échantillons. Pour la catégorie i de la loi j la fréquence attendue sous H0 est
donc estimée par nj Ni. /n . La statistique de test est alors :

Q=

Ni. nj 2
)
n
Ni. nj
n

I (Nij −
J 

j=1 i=1

dont la loi peut être approchée par la loi χ2 ((I − 1)(J − 1)).
Pour la mise en oeuvre de ce test par la statistique du khi-deux considérons les notations pour les fréquences observées comme indiqué dans le tableau
suivant :

Chapitre 10. Tests non paramétriques

Catégorie
1
..
.

loi 1
n11
..
.

···
···
..
.

loi j
n1j
..
.

···
···
..
.

loi J
n1J
..
.

n1.
..
.

i
..
.

ni1
..
.

···
..
.

nij
..
.

···
..
.

niJ
..
.

ni.
..
.

I

nI1
n.1

···
···

nIj
n.j

···
···

nIJ
n.J

nI.
n

259

Dans ce tableau les nj sont notés n.j pour donner un rôle symétrique aux
deux marges. La fréquence attendue pour la case (i, j) est obtenue en eﬀectuant
le produit des marges ni. et n.j divisé par n . On rejettera donc H0 au niveau
α si :
ni. n.j 2
J 
I (nij −
)

2 ((I−1)(J−1))
n
> χ 1−α
.
q=
ni. n.j
j=1 i=1
n
Pour un tableau 2 × 2 la condition de validité reste que les fréquences attendues soient supérieures à 5. Pour un tableau de dimensions supérieures de
nombreuses simulations ont montré que l’approximation était étonnamment
bonne même avec des eﬀectifs plus faibles. On montre aisément (voir exercices)
que, dans le cas I = 2 et J = 2, on retombe sur le test par approximations
gaussiennes de la section 9.7.6.

10.3

Test d’indépendance de deux variables
catégorielles

10.3.1

Test du RVG et test du khi-deux

On considère un couple de variables catégorielles, l’une à I catégories,
l’autre à J catégories, observables sur toute unité statistique sélectionnée (ou,
pour un sondage, sur chaque individu d’une population). Le croisement de ces
deux variables donne lieu à une variable catégorielle à I × J catégories avec
I ×J −1 paramètres libres. A la catégorie obtenue par croisement des catégories
i et j respectives de chaque variable est associée la probabilité pij . On a donc
I J
i=1
j=1 pij = 1. On s’intéresse à l’hypothèse d’indépendance de ces deux
variables.
Comme pour les variables aléatoires (voir déﬁnition 3.4) on déﬁnit que deux
variables catégorielles sont indépendantes par le fait que, pour tout événement
sur l’une et tout événement sur l’autre, la probabilité de leur intersection (ou
conjonction) est égale au produit des probabilités de chaque événement. Un événement étant un sous-ensemble de catégories et les catégories étant en nombre

Statistique − La théorie et ses applications

260

ﬁni on peut voir aisément (voir exercices) qu’il faut et qu’il suﬃt qu’il y ait
indépendance entre tous les couples élémentaires (i, j) de catégories pour assurer l’indépendance complète. Ainsi l’hypothèse nulle à tester est :
H0 : pij = pi. p.j

pour i = 1, · · · , I

et

j = 1, · · · , J ,

où pi. est la probabilité marginale pour la catégorie i de la première variable
et p.j pour la catégorie j de la deuxième variable. L’hypothèse alternative est
la négation de H0 à savoir qu’il y ait au moins un couple (i, j) pour lequel
pij = pi. p.j .
Pour un échantillon aléatoire de taille n on observe les fréquences au croisement des deux variables et l’on note Nij la fréquence au croisement (i, j). Une
réalisation de l’échantillon peut être représentée par le tableau de contingence
suivant :
PP
PP Var.2
1
Var.1 PPP
P
1
n11
..
..
.
.
i
ni1
..
..
.
.
I

nI1
n.1

···

j

···

J

···
..
.

n1j
..
.

···
..
.

n1J
..
.

n1.
..
.

···
..
.

nij
..
.

···
..
.

niJ
..
.

ni.
..
.

···
···

nIj
n.j

···
···

nIJ
n.J

nI.
n

Prenons l’approche par le test du rapport de vraisemblance. Considérant
la variable catégorielle à I × J catégories obtenue par le croisement on a pour
fonction de vraisemblance du tableau des nij (voir section 10.1) :
n! $ nij
*
p
nij ! i,j ij

pour

i,j



nij = n

et 0 sinon,

i,j

*

où i,j dénote, en abrégé,les produits de I × J termes avec i = 1, · · · , I et
j = 1, · · · , J (et de même i,j pour les sommes). Comme il a été démontré en
section 10.1.1 les estimations du maximum de vraisemblance sont pij = nij /n .
Sous H0 la vraisemblance est :
n! $ nij $ nij
n! $
*
(pi. p.j )nij = *
pi.
p.j
nij ! i,j
nij ! i,j
i,j
i,j

i,j

n! $ ni. $ n.j
=*
pi.
p.j .
nij ! i
i
i,j

Chapitre 10. Tests non paramétriques

261

La maximisation s’opère séparément sur chaque variable et revient pour chacune au problème d’une multinomiale d’où pi. = ni. /n et p.j = n.j /n . Le RVG
est donc :
*
*
ni.
n.j
i ni.
j n.j
*
λ=
.
n
nn nijij
i,j

Asymptotiquement la statistique −2 ln λ (obtenue en remplaçant les réalisations
nij par les v.a. Nij dans l’expression ci-dessus) suit une loi du khi-deux avec un
nombre de degrés de liberté égal au nombre de paramètres spéciﬁés. On pourrait trouver ce nombre en utilisant une reparamétrisation comme nous l’avons
fait en section 10.2, mais il est plus simple de constater que sous H0 il reste
I − 1 paramètres inconnus pour la première variable et J − 1 pour la deuxième.
Comme il y a globalement IJ − 1 paramètres inconnus cela signiﬁe que H0
spéciﬁe implicitement IJ − 1 − (I − 1) − (J − 1) = (I − 1)(J − 1) paramètres.
Donc la statistique −2 ln λ suit, sous H0 , une loi χ2 ((I − 1)(J − 1)) et l’on re2 ((I−1)(J−1))
jettera H0 au niveau α si la réalisation −2 ln λ est supérieure à χ1−α
.

De préférence, on utilisera la statistique Q de Pearson obtenue en estimant
les fréquences attendues sous H0 par le maximum de vraisemblance, soit :
n
pi. p.j = n

Ni. N.j
Ni. N.j
=
,
n n
n

d’où :
Q=

Ni. N.j 2
)
n
.
Ni. N.j
n

J 
I (Nij −

j=1 i=1

Cette statistique est asymptotiquement de même loi que celle issue du RVG
(voir note 10.2) sous H0 . Cette hypothèse sera donc rejetée au niveau α si la
2 ((I−1)(J−1))
réalisation q de Q est telle que q > χ1−α
.
Ceci conduit à un test dont la mise en oeuvre est en tous points
identique à celui proposé pour la comparaison de lois multinomiales.
Il convient toutefois d’insister sur la diﬀérence entre les deux situations. Dans
le test d’indépendance seule est ﬁxée la taille globale de l’échantillon n, les
fréquences marginales étant aléatoires. Dans la situation précédente une des
marges (celle du bas du tableau) est ﬁxée dans le plan d’échantillonnage par
les eﬀectifs choisis pour les diﬀérentes populations.
Notons aussi que le tableau des fréquences attendues (les ni. n.j /n) est un
tableau dont toutes les lignes (respectivement toutes les colonnes) sont proportionnelles, ce qui correspond bien à l’idée d’indépendance intuitive sur un
tableau empirique.

Statistique − La théorie et ses applications

262

10.3.2

Test exact de Fisher (tableau 2 × 2)

On considère le cas I = 2 et J = 2 avec le tableau des réalisations suivant :
PP
PP Var.2
1
Var.1 PPP
P
1
n11
2
n21
n.1

2
n12
n22
n.2

n1.
n2.
n

On peut montrer que la loi conditionnelle conjointe de (N11 , N12, N21, N22 )
sachant les fréquences marginales N1. , N2. , N.1 , N.2 est indépendante des pij
sous H0 . En d’autres termes les fréquences marginales sont des statistiques
exhaustives pour les paramètres (p1. , p2. , p.1 , p.2 ) en cas d’indépendance (voir
déﬁnition 6.8). Plus précisément, en raison des contraintes, (N1. , N.1 ) est statistique exhaustive si l’on prend comme seuls paramètres inconnus (p1. , p.1 ).
La démonstration est simple car, les marges étant ﬁxées à n1. , n2. , n.1 , n.2 , il
suﬃt de considérer la probabilité P (N11 = n11 |n1. , n2. , n.1 , n.2 ) puisque les
v.a. N12, N21, N22 sont liées à N11 respectivement par n1. − N11 , n.1 − N11 et
n2. − n1. + N11 . On a alors une démonstration analogue à celle exposée en
section 9.7.6 (voir exercices). On obtient :

P (N11 = n11 |n1. , n2. , n.1 , n.2 ) =

 
n.2
n.1
n11
n
 12
n
n1.

qui montre que, conditionnellement aux marges, N11 suit une loi hypergéométrique H(n, n1., n.1 ).
Le test proposé par Fisher consiste à prendre une région critique de niveau
α choisi, sur cette loi conditionnelle. On peut établir (voir note 10.3 cidessous) que ceci est légitime, donnant bien un test de niveau α dans l’absolu.
Cela conduit à une règle de décision totalement identique à celle utilisée en
section 9.7.6 pour tester de façon exacte l’égalité de deux proportions (ou, plus
généralement, des paramètres de deux lois de Bernoulli). Pour cette hypothèselà il suﬃsait toutefois de conditionner sur une seule marge, l’autre étant ﬁxée
par le plan d’échantillonnage.
On déﬁnit donc une région de rejet sur la base, par exemple, de la valeur
n11 observée, selon :
n11 ∈
/ [cα1 , cα2 ]
où cα1 et cα2 sont les quantiles d’ordres α1 et α2 tels que α1 + α2 = α, issus
de la loi H(n, n1., n.1 ). Ce test est UPP-sans biais.

Chapitre 10. Tests non paramétriques

263

La plupart des logiciels se chargent d’eﬀectuer les calculs fastidieux et fournissent la P-valeur relative au tableau observé.
On pourrait étendre ce test exact à un tableau de plus grande dimension.
Outre que les calculs se complexiﬁent rapidement ceci n’est pas utile du fait
que l’approximation du test du khi-deux devient très vite satisfaisante avec des
conditions de validité identiques à celles de la section précédente, à savoir que
les fréquences attendues restent pour la plupart supérieures ou égales à 5.
Note 10.3 Montrons que, d’une façon générale, un test de niveau α conditionnellement à une statistique exhaustive sous H0 est un test de niveau α dans l’absolu. Soit dans la famille {f (x; θ), θ ∈ Θ} l’hypothèse nulle H0 : θ ∈ Θ0 et T
une statistique exhaustive sous H0 . Soit, pour un échantillon (X1 , X2 , · · · , Xn ), un
test déﬁni par une région de rejet A de Rn de niveau α pour la loi conditionnelle
f (x1 , x2 , · · · , xn |T = t) qui, par déﬁnition pour T, ne dépend pas de θ sous H0 .
Alors, notant en raccourci (x1 , x2 , · · · , xn ) par x et dx1 dx2 · · · dxn par dx, on a
pour tout θ ∈ Θ0 , avec des notations évidentes (T étant de dimension k) :


Pθ (A) =

f (x; θ)dx

A 
f (x| T = t)fT (t; θ)dt dx
=
A
Rk



=
fT (t; θ)
f (x| T = t)dx dt
Rk
A

fT (t; θ)dt = α ,
=α
Rk

ce qui prouve que A déﬁnit un test de niveau α de façon non conditionnelle. Dans le
cas discret ceci vaut si l’on randomise le test pour atteindre exactement le niveau α.
Si l’on utilise un test conservateur conditionnellement il restera toutefois conservateur
non conditionnellement.

Exemple 10.1 Une pré-enquête a été eﬀectuée auprès de 50 personnes (supposées sélectionnées par sondage aléatoire simple dans la population cible) pour
évaluer le taux d’acceptation pour participer à une étude de suivi médical. On
s’intéresse au croisement des variables catégorielles sexe et réponse oui/non
pour participer. On a obtenu les résultats suivants :
XXX
particip.
XXX
XXX oui non
sexe
X
femme
9
20
29
homme
5
16
21
14
36
50
Choisissant de fonder le test sur la fréquence du croisement (femme, oui) on
doit lire les probabilités sur une loi H(50, 29, 14). C’est-à-dire qu’on examine la

264

Statistique − La théorie et ses applications

v.a. X «nombre de femmes parmi les 14 oui» sachant qu’il y a globalement 29
femmes parmi les 50 personnes, sous l’hypothèse d’indépendance entre sexe et
participation ou non. On doit rejeter cette hypothèse si le nombre de femmes
répondant oui est soit trop élevé, soit trop faible, par rapport à la fréquence
attendue qui est égale à 14 29
50 = 8,12 . Pour cette loi hypergéométrique les sauts
de probabilité sont assez élévés. On a, par exemple, une région de rejet de niveau
0,024 avec {0, 1, 2, 3, 4, 12, 13, 14} pour les valeurs de X et de niveau 0,11 avec
{0, 1, 2, 3, 4, 5, 11, 12, 13, 14}. On a donc avantage à considérer la P-valeur. Avec
la fonction « loi hypergéométrique » dans un tableur on trouve que P (X ≥ 9) =
0,41. La P-valeur est donc égale à 0,82 ce qui rend l’hypothèse d’indépendance
tout à fait acceptable.
Notons qu’on aurait pu aussi bien prendre la loi H(50, 14, 29), considérant
le nombre de oui parmi les 29 femmes sachant qu’il y a globalement 14 oui
parmi les 50 personnes, ce qui est rigoureusement équivalent.
Voyons ce que donne l’approche approximative par la statistique de Pearson.
Le tableau des fréquences attendues est
8,12
5,88
d’où :
q = (0,88)2 (

20,88
15,12

1
1
1
1
+
+
+
) = 0,315
8,12 20,88 5,88 15,12

qui correspond au quantile d’ordre 0,43 sur une loi χ2 (1). La P-valeur est donc
ici donnée à 0,57 ce qui est sensiblement diﬀérent de la valeur exacte de 0,82
mais conduit à la même décision d’acceptation de l’indépendance.
Sur cet exemple, de la façon dont les choses sont présentées, on a le sentiment
que l’on compare deux proportions : celle des femmes et celle des hommes
acceptant de participer à l’étude. Néanmoins il ne s’agit pas d’un test d’égalité
entre ces deux proportions car un tel test supposerait que l’on ait ﬁxé a priori
la taille des échantillons de chaque sexe (par un plan de sondage stratiﬁé selon
le sexe), alors que dans notre exemple les eﬀectifs des hommes et des femmes
résultent du tirage au hasard. La diﬀérence mérite d’être précisée même si elle
n’a pas d’incidence sur la règle de décision. Elle n’est toutefois pas neutre pour
le calcul de la puissance.

Pour approfondir la théorie et la pratique des données catégorielles on
pourra consulter les ouvrages suivants : Agresti (2002), Chap (1998), Droesbeke, Lejeune et Saporta (2004).

10.4

Test d’ajustement à un modèle de loi

Le problème envisagé ici est de décider, au vu d’un échantillon X1 , X2 , . . . ,
Xn , si la loi mère de cet échantillon est du type spéciﬁé par une hypothèse

Chapitre 10. Tests non paramétriques

265

H0 . Un test aura pour but d’examiner si la distribution des valeurs de l’échantillon s’ajuste suﬃsamment bien à une distribution théorique donnée. On parle
également de test d’adéquation (en anglais : goodness-of-ﬁt test). A défaut
de pouvoir rejeter H0 on acceptera le modèle théorique proposé. Les développements précédents relatifs à des comparaisons de fréquences observées et de
fréquences attendues ou «théoriques» vont nous fournir un test de portée très
générale. Outre ce test fondé sur une statistique du khi-deux il existe bien
d’autres tests d’ajustement d’inspirations diverses et nous étudierons en particulier le test de Kolmogorov-Smirnov de portée également générale et, par
là-même, très répandu.
Nous distinguons deux situations pour l’hypothèse nulle. En premier nous
étudions le cas plus simple où la loi est parfaitement spéciﬁée par H0 , puis
nous passerons au cas où H0 spéciﬁe une famille paramétrique particulière sans
préciser la valeur du paramètre qui reste inconnu.

10.4.1

Ajustement à une loi parfaitement spéciﬁée

Nous nous plaçons dans une optique non paramétrique au sens où les
tests considérés devront s’appliquer quelle que soit la nature du modèle de loi
mère envisagé.
La fonction de répartition étant l’objet mathématique le plus approprié pour
spéciﬁer une loi, qu’elle soit discrète ou continue, nous conviendrons d’écrire
l’hypothèse nulle sous la forme H0 : F = F0 où F0 caractérise donc le modèle
de loi spéciﬁé, l’alternative étant H1 : F = F0 . Ce genre de situation n’est
pas rare, par exemple lorsqu’une théorie a été élaborée pour un phénomène
quantiﬁable et qu’il s’agit de la mettre à l’épreuve des faits.
Test du khi-deux
Son principe repose sur la transformation de la variable aléatoire en une
variable catégorielle pour se ramener au test sur une loi multinomiale comme
en section 10.1. Pour cela on découpe R (ou sa partie utile) en intervalles
pour obtenir des classes comme on le ferait pour un histogramme. A l’instar
de ce qui a été fait en section 8.5.2 ce découpage se déﬁnit comme une suite
double de valeurs croissantes {· · · , a−i , · · · , a−1 , a0 , a1 , · · · , ai , · · · } et l’on note
nk la fréquence des observations situées dans l’intervalle ]ak−1 , ak ] pour un
échantillon de taille n. La fréquence attendue sous H0 est npk où pk est la
probabilité pour la loi F0 associée à l’intervalle ]ak−1 , ak ], i.e.
pk = F0 (ak ) − F0 (ak−1 ).
Pour ce découpage il faut toutefois veiller à remplir les conditions de validité de l’approximation asymptotique, à savoir faire en sorte que les npk
restent supérieurs ou égaux à 5. Cela amènera à reconsidérer éventuellement

266

Statistique − La théorie et ses applications

le découpage initial et à regrouper des classes contiguës à faible probabilité.
Pour une extrémité inﬁnie on constituera un dernier intervalle ouvert sur l’inﬁni de probabilité supérieure à 5/n. Notons que dans le cas d’une loi discrète
concentrée sur un faible nombre de valeurs, chaque valeur (sauf peut-être sur
les extrémités) peut constituer naturellement une classe en soi.
Par ailleurs le choix du nombre de classes, voire même des frontières de ces
classes, inﬂue sur la puissance. Mais il est diﬃcile d’orienter ce choix et l’on
recommande, pour la pratique, de rester proche de classes à probabilités égales.
Remarquons que le passage d’une variable aléatoire (au sens strict, c’està-dire quantitative) à une variable catégorielle induit une perte d’information.
En eﬀet dans une variable catégorielle il n’y a pas d’ordre des catégories et la
statistique du khi-deux est indiﬀérente à une permutation de celles-ci. Ainsi
l’échelle numérique de la variable aléatoire est ignorée ce qui laisse supposer
une perte de puissance.
Test de Kolmogorov-Smirnov
Ce test tient compte de l’échelle des observations mais ne s’applique en
principe qu’aux lois continues. Il est fondé sur l’écart constaté entre la fonction de répartition empirique Fn et F0 . Nous avons vu en section 8.5.3 diverses propriétés de la fonction de répartition empirique comme estimateur de
la fonction de répartition de la loi mère : elle est l’estimateur fonctionnel du
maximum de vraisemblance et est convergente presque sûrement uniformément
(voir théorème 8.1). De plus, Fn (x) est sans biais en tout x ﬁxé. On s’attend
donc, si H0 est vraie, à ce qu’elle reste proche de F0 . En fait le théorème 8.2
dû à Kolmogorov et à Smirnov fournit la statistique de test
Dn = sup |Fn (x) − F0 (x)| .
x∈R

Son intérêt est que, sous H0 , sa loi ne dépend pas de la nature de F0 ce qui
donne lieu à des tables uniques quel que soit le type de modèle à tester.
Rappelons, suite au théorème 8.2, que pour n ≥ 40 et x > 0,8 on peut
utiliser :
√
2
P ( nDn < x)  1 − 2e−2x ,
√ ) 0,95. Comme on rejette nace qui conduit, par exemple, à P (Dn < 1,36
n
turellement H0 si Dn est trop grand, on a comme région de rejet au niveau
√
où dn est la réalisation observée de Dn .
0,05 : dn > 1,36
n

D’un point de vue pratique il faut tenir compte du fait que Fn est constante
par morceaux. S’il est clair qu’en raison de la croissance de F0 l’écart absolu
maximal doit se situer en un point de discontinuité de Fn (donc en une valeur
observée xi ) il y a toutefois lieu de comparer, pour tout i = 1, . . . , n, la valeur
de F0 (xi ) à la fois à Fn (xi ) et à Fn (x−
i ) = Fn (xi−1 ). Une illustration est donnée
en ﬁgure 10.1.

Chapitre 10. Tests non paramétriques

267

1
F0(x)
0,8

dn

0,6
0,4
0,2
Fn(x)

x

0
0

5

10

15

Figure 10.1 - Illustration du test de Kolmogorov-Smirnov.

On peut appliquer le test à des observations regroupées en classes. Dans
ce cas il ne faut comparer que les valeurs aux frontières des classes : si ak est
une valeur frontière on comparera uniquement Fn (ak ) et F0 (ak ). En eﬀet les
valeurs ne sont comptabilisées que sur ces frontières et on ignore l’allure de
Fn à l’intérieur des classes. Il se trouve qu’ainsi le test est conservateur (i.e. le
niveau réel reste inférieur au niveau nominal de la table).
De nombreuses études ont été eﬀectuées pour comparer les puissances du
test de Kolmogorov-Smirnov et du test du khi-deux. Bien qu’on ne puisse tirer
de conclusions générales il est vrai que le plus souvent le test du khi-deux est
moins puissant. Ceci s’explique notamment par le fait que, contrairement au
test de Kolmogorov-Smirnov, il ne tient pas compte de l’échelle des valeurs. Il
est par ailleurs intéressant de noter que si les deux tests sont convergents, ils ne
sont pas sans biais vis-à-vis de toutes les fonctions de répartitions autres que
F0 tant leur multiplicité est grande.

10.4.2

Ajustement dans une famille paramétrique donnée

On suppose maintenant, comme c’est le plus souvent le cas en pratique, que
H0 spéciﬁe une famille de loi paramétrique sans précision sur le paramètre de
la loi qui reste inconnu, ce que nous pouvons écrire :
H0 : F ∈ {f (x; θ), θ ∈ Θ}

268

Statistique − La théorie et ses applications

où seul θ est inconnu. Par exemple on souhaite tester que la loi mère est gaussienne (ou, plus exactement, que le modèle gaussien est acceptable au vu des
observations dont on dispose). Pour pouvoir élaborer une statistique mesurant
d’une certaine façon, que ce soit par la statistique du khi-deux ou par celle
de Kolmogorov-Smirnov, l’écart entre ce que l’on a observé et une référence
théorique sous H0 , il est nécessaire de passer par une estimation du paramètre
inconnu θ.
Test du khi-deux
Pour la statistique du khi-deux le théorème ci-après dont la démonstration a
été eﬀectuée par Cramer (1946) permet de prendre en compte cette estimation
de θ. Donnons tout d’abord le cadre général d’application du théorème.
On considère une loi multinomiale à c catégories dont les probabilités dépendent d’un paramètre inconnu θ ∈ Θ de dimension r < c − 1 et sont
notées pj (θ), j = 1, · · · , c. Pour un n-échantillon aléatoire et les fréquences
observées n1 , n2 , ..., nc , les estimations du MV de ces probabilités se déduisent,
en tant que fonctions de θ, de l’estimation du maximum de vraisemblance θ de θ
(voir note 6.4). Celui-ci est obtenu en maximisant la fonction de vraisemblance
L(θ) =
avec la contrainte

c

n!
[p1 (θ)]n1 [p2 (θ)]n2 · · · [pc (θ)]nc
n1 !n2 ! · · · nc !

pj (θ) = 1. On en déduit alors les estimations du maxi p2 (θ),
 · · · , pc (θ).
 Dans l’énoncé du théorème nous
mum de vraisemblance p1 (θ),
utilisons, pour alléger, ces mêmes notations pour les estimateurs, les ni devant
être remplacés par les Nj .
j=1

Théorème 10.1 Soit une loi multinomiale à c catégories de probabilités
p1 (θ), p2 (θ), · · · , pc (θ), où θ est un paramètre inconnu de dimension r < c−1 et
soit θ l’estimateur du maximum de vraisemblance de θ pour les v.a. N1 , N2 , ...,
Nc . Alors (sous certaines conditions de régularité) on a :
Q=

c

 2
(Nj − npj (θ))
; χ2 (c − r − 1).

npj (θ)
j=1

En fait, nous avons déjà rencontré une telle situation dans le test d’indépendance en section 10.3. En eﬀet sous l’hypothèse d’indépendance H0 les pij de la
variable catégorielle croisée à I × J catégories s’exprimaient selon pij = pi. p.j
et étaient donc des fonctions de (I − 1) + (J − 1) paramètres correspondant
aux probabilités marginales pi. , i = 1, · · · , I − 1 et p.j , j = 1, · · · , J − 1 (pI. et
p.J se déduisant des précédents). On retrouve ici la règle des degrés de libertés
pour Q, à savoir IJ − (I − 1) − (J − 1) − 1 = (I − 1)(J − 1) établie alors en
se rapportant à l’équivalence avec la statistique du RVG. On pourrait penser

Chapitre 10. Tests non paramétriques

269

que ce théorème est superﬂu, mais il mérite d’être présenté du fait qu’il découle
historiquement d’une approche directe du comportement de la statistique du
khi-deux et ceci, c’est à noter, sans faire référence à un test d’hypothèse.
Si l’on se replace dans une situation de test on peut au premier abord
s’étonner que le fait de devoir estimer des paramètres diminue les degrés de
libertés. En eﬀet un quantile d’ordre donné diminuant avec les degrés de liberté la valeur critique en est abaissée par rapport à une même situation où θ
serait connu. La raison tient au fait que la statistique Q sous-évalue les écarts
aux vraies fréquences attendues en leur substituant des fréquences calculées
sur l’échantillon lui-même et ceci d’autant plus que le nombre de paramètres
à estimer est grand (à la limite, si θ était de dimension c − 1 on serait simplement en présence d’une reparamétrisation du vecteur (p1 , p2 , · · · , pc−1 ) et l’on
prendrait pj = Nj /n réduisant ainsi l’expression de Q à 0).
L’application du théorème à l’ajustement dans une famille de lois est immédiate. On procède comme en section 10.4.1 en opérant un découpage en classes,
mais ici les probabilités pk associées aux intervalles ]ak−1 , ak ] dépendent du
paramètre θ. Il s’agit alors d’exprimer chaque pk comme une fonction de θ et
d’en déduire l’estimateur du MV comme indiqué ci-dessus. Remarquons bien
que cet estimateur n’est pas, hélas, celui que l’on obtient directement de la
façon classique sur la base des observations X1 , X2 , · · · , Xn . Illustrons cela par
un exemple.
Exemple 10.2 Soit à tester l’hypothèse que les observations proviennent d’une
loi de Poisson P(λ). Les observations au-delà de 3 étant rares, supposons que
l’on eﬀectue un découpage en 4 classes : {0}, {1}, {2} et {3 et plus}. Les probabilités associées à ces classes sont, respectivement :
e−λ , e−λ λ , e−λ

λ2
λ2
et 1 − (e−λ + e−λ λ + e−λ ) .
2
2


Soit ni , i = 1, · · · , 4, les fréquences observées dans les 4 classes. L’estimation λ
du MV approprié est obtenue en maximisant la fonction de vraisemblance de
λ suivante :
L(λ) =

n!
λ2
λ2
[e−λ ]n1 [e−λ λ]n2 [e−λ ]n3 [1 − (e−λ + e−λ λ + e−λ )]n4 .
n1 !n2 ! · · · nc !
2
2

Cette fonction de λ n’est pas simple et il faut recourir à un algorithme d’optimisation numérique. Il est clair que la solution est diﬀérente de celle de l’estimateur du MV classique fondé sur les observations brutes x1 , x2 , · · · , xn et
égal à la moyenne x des observations. Prenons les 100 observations suivantes :
valeurs
fréquences

0
38

1
40

2
12

3
7

4
2

5
1

Le maximum de L(λ) obtenu par un logiciel mathématique est 9,68 alors que
la moyenne des observations est 9,8.

270

Statistique − La théorie et ses applications

Notons que si n4 est petit ces valeurs sont proches car les fonctions à maximiser sont similaires. En eﬀet pour L(λ) il faut maximiser
e−λ(n1 +n2 +n3 ) λn2 +2n3 [1 − (e−λ + e−λ λ + e−λ

λ2 n4
)]
2

alors que la fonction de vraisemblance classique est proportionnelle à :
n
$

n

e−λ λ

i=1

xi









= e−λ(n1 +n2 +n3 +n4 +n5 ··· ) λn2 +2n3 +3n4 +4n5 +···

i=1

où n4 , n5 , etc. sont les fréquences observées des valeurs 3, 4, etc.



Cet exemple simple illustre le fait que l’estimation appropriée du maximum
de vraisemblance est généralement diﬃcile. Pour tester que la loi mère est
gaussienne le problème est encore plus complexe. La probabilité pk associée à
a
−μ
), où Φ est la fonction de
l’intervalle ]ak−1 , ak ] est égale à Φ( akσ−μ ) − Φ( k−1
σ
répartition de la loi de Gauss centrée-réduite, ce qui complique fortement la
fonction de vraisemblance L(μ, σ 2 ). En pratique on utilise l’estimation classique
du paramètre qui est d’autant plus proche de l’estimation appropriée que le
découpage en classes est ﬁn (mais avec les limitations qui demeurent, à savoir
que les fréquences attendues ne descendent pas en dessous de 5). Chernoﬀ et
Lehmann (1954) ont montré que, dans ce cas, la statistique Q ne suit pas une
loi du khi-deux mais une loi encadrée par les lois χ2 (c − r − 1) et χ2 (c − 1).
On se rapproche donc du cas où les fréquences attendues sont parfaitement
connues du fait que l’estimateur usuel du MV est plus eﬃcace. En gardant
c − r − 1 degrés de liberté, comme le font les praticiens et la plupart des
logiciels, on eﬀectue un test anti-conservateur (i.e. de niveau réel supérieur au
niveau nominal) puisque le quantile est inférieur à ce qu’il devrait être. Une
procédure assurément conservatrice, mais souvent trop, consisterait à prendre
le quantile sur la loi χ2 (c − 1).
Si le nombre de classes c est assez élevé, la diﬀérence entre les quantiles sera
peu sensible, sachant que presque toutes les familles paramétriques courantes
ont un paramètre à une ou deux dimensions (r ≤ 2).
Test de Kolmogorov-Smirnov
En pratique on adapte le test vu en section 10.4.1 en calculant la statistique :

' n = sup |Fn (x) − F (x; θ)|
D
x∈R

où F (x; θ) est la fonction de répartition pour la famille paramétrique considérée
et θ est l’estimateur usuel du maximum de vraisemblance. Mais alors la loi de
' n sous H0 n’est plus indépendante de la vraie loi et il faut étudier chaque
D
cas de famille séparément. Fort heureusement le fait d’utiliser la valeur critique

Chapitre 10. Tests non paramétriques

271

propre à la statistique Dn de la section 10.4.1 conduit à un test conservateur.
Ceci découle de raisons semblables à celles invoquées pour la statistique du khi' n tend à sous-estimer l’écart-réel et devrait être rejetée à
deux, à savoir que D
des valeurs critiques inférieures.
Remarques diverses
1. Il existe des tests spéciﬁques à chaque famille qui, de ce fait, sont en principe
plus puissants que les tests généraux précédents. Citons en particulier le test de
normalité de Shapiro-Wilk (1965) fondé sur la corrélation entre les statistiques
d’ordre et leurs espérances mathématiques sous hypothèse gaussienne.
2. Souvent on n’a pas d’idée préconçue de modèle et l’on recherche, parmi
les modèles courants, celui qui est le plus proche des observations. L’avantage
d’une procédure générale telle que celles présentées ci-dessus est qu’elle fournit
le même critère pour comparer plusieurs modèles, les statistiques Dn ou Q
tenant lieu de distance à minimiser. Paradoxalement, on ne souhaite pas avoir
un test trop puissant, car on se contente de s’assurer que le modèle le plus
proche est accepté par le test d’adéquation.
3. Il existe aussi des méthodes graphiques telle que la droite de Henri pour l’hypothèse gaussienne et sa version générale non-paramétrique du QQ-plot qui est
le graphe de la variation des quantiles empiriques en fonction des quantiles
théoriques sous H0 . Ces méthodes ont l’avantage de mettre en évidence les
zones de forte déviation par rapport au modèle supputé et donc d’orienter, le
cas échéant, la recherche d’un meilleur modèle. L’inconvénient est que le jugement d’acceptabilité repose sur une appréciation graphique et reste fortement
subjectif.
4. Le test du khi-deux s’étend aisément au test d’égalité de deux lois (ou des
distributions de deux populations). Il suﬃt d’utiliser un découpage en classes
et de se ramener ainsi à la comparaison de deux lois multinomiales exposée
en section 10.2. Il existe également une version à deux échantillons du test de
Kolmogorov-Smirnov fondé sur la statistique
∗
= sup |Fn (x) − Fm (x)|
Dn,m
x∈R

où Fn et Fm sont les fonctions de répartition empiriques des échantillons de
tailles respectives n et m. Quand n et m tendent vers l’inﬁni on a :
P (( n1 +

1 −1/2
Dn,m
m)

2

< x)  1 − 2e−2x

√
qui est la même probabilité que pour nDn < x dans le cas d’un seul échantillon.
Ceci permet de construire un test approché. Il existe également des tables pour
les faibles valeurs de n et m.

272

Statistique − La théorie et ses applications

10.5

Tests non paramétriques sur des
caractéristiques de lois

10.5.1

Introduction

Il y a une certaine ambiguı̈té en ce qui concerne les tests sur le terme de non
paramétrique auquel les anglo-saxons préfèrent parfois celui de «distribution
free» pour qualiﬁer les procédures valides quelle que soit la loi mère.
En particulier la loi F peut alors être totalement non spéciﬁée ce qui permet
de parler de procédures non paramétriques. Une autre ambiguı̈té vient du fait
que l’on assimile généralement les tests non paramétriques aux tests fondés sur
les rangs des observations. Il est vrai que les tests de rangs sont, par essence,
applicables indépendamment de la nature de la loi mère et qu’ils oﬀrent de
nombreuses possibilités, mais il existe d’autres tests de type «distribution free»
comme, par exemple, les tests d’ajustement du khi-deux et de KolmogorovSmirnov vus en section 10.4.1 ou certains des tests qui vont suivre.
Nous ne présenterons que les tests les plus courants pour illustrer la philosophie générale de l’approche non paramétrique. Étant donné, malgré tout,
la place importante des rangs dans cette approche, nous donnons tout d’abord
quelques propriétés les concernant.

10.5.2

Les statistiques de rang

On considère un échantillon aléatoire (X1 , X2 , . . . , Xn ) de loi F. Pour des
réalisations (x1 , x2 , . . . , xn ), le rang ri d’une valeur xi est la position qu’elle
occupe quand les valeurs sont rangées dans l’ordre croissant. A tout vecteur
de réalisations on peut donc associer le vecteur des rangs (r1 , r2 , . . . , rn ) qui
consiste en une permutation des nombres {1, 2, . . . , n}. Par exemple, avec n = 5,
au vecteur (8,2 ; 7,4 ; 9,2 ; 5,1 ; 6,7) on associe le vecteur des rangs (4, 3, 5, 1, 2).
Cette fonction appliquée à (X1 , X2 , . . . , Xn ) procure les statistiques de rang
(R1 , R2 , . . . , Rn ). La v.a. Ri sera appelée le rang de Xi . Notons que si Xi est
la statistique d’ordre k alors Ri est égal à k. On supposera que F est continue
aﬁn d’ignorer, pour l’heure, le problème des valeurs identiques.
La proposition suivante indique que les statistiques de rang ont une loi
conjointe indépendante de F et, par conséquent, que toute inférence fondée sur
les rangs sera de nature non paramétrique. Qui plus est, cette loi est parfaitement déterminée et permettra d’établir les distributions d’échantillonnage des
statistiques de test reposant sur les rangs.
Proposition 10.1 La loi de (R1 , R2 , . . . , Rn ) est la loi uniforme sur l’ensemble
des n! permutations des nombres {1, 2, . . . , n}.
Ce résultat découle du fait que toutes les v.a. X1 , X2 , . . . , Xn sont échangeables, c’est-à-dire que toute permutation des composantes de (X1 , X2 , . . . , Xn )

Chapitre 10. Tests non paramétriques

273

a la même loi conjointe que (X1 , X2 , . . . , Xn ). On en déduit également les lois
marginales des rangs.
Proposition 10.2 Pour tout i = 1, . . . , n le rang Ri suit une loi discrète uniforme sur {1, 2, . . . , n}.
Ainsi (voir section 4.1.1) :
n+1
2
n2 − 1
V (Ri ) =
.
12
E(Ri ) =

De plus on démontre que, pour tout i et tout j distincts,
cov(Ri , Rj ) = −

n+1
.
12

Outre leur non dépendance vis-à-vis de la loi mère les statistiques de rang
ont l’avantage de pouvoir s’appliquer lorsque les données sont peu précises et
même simplement ordinales. Ceci est souvent le cas dans les tests psychologiques et dans les études de comportement d’achat ou de consommation, où les
sujets sont amenés à exprimer des préférences ou à eﬀectuer des classements.
De plus ces statistiques sont peu sensibles aux valeurs extrêmes ou aberrantes.

10.5.3

Tests sur moyenne, médiane et quantiles

Test sur la moyenne
En section 8.2.1 on a pu voir que, si μ est la moyenne de la loi, on a :
X −μ
√
σ/ n

;

approx

N (0 ; 1)

si n est assez grand, pourvu que la loi mère admette une variance. En conséquence le test de Student de la section 9.7.1 fournit un test approché pour une
hypothèse du type H0 : μ = μ0 .
Si l’échantillon est trop petit pour garantir une bonne approximation, ou
si la loi mère peut produire des valeurs extrêmes (queues de distribution allongées), ou s’il y a risque de présence de valeurs aberrantes, il sera préférable
de recourir à un test non paramétrique concernant la médiane μ
' de la loi.
Test sur la médiane : le test du signe
Ce test est le dual de la procédure d’intervalle de conﬁance pour la médiane
vue en section 8.3. Comme alors, nous supposons simplement que la fonction de
répartition de la loi mère F est continue et strictement croissante pour garantir
'=μ
'0 avec une alternative
l’unicité de la médiane. L’hypothèse nulle est H0 : μ

274

Statistique − La théorie et ses applications

soit unilatérale soit bilatérale. Pour l’échantillon aléatoire X1 , X2 , . . . , Xn la
' , le nombre de ces v.a. inférieures ou égales à μ
statistique de test est N
'0 . Sous
' suit une loi B(n, 1 ).
H0 , pour tout i on a P (Xi ≤ μ
'0 ) = 12 , donc N
2
' prend une valeur soit trop grande soit
Pour le cas bilatéral on rejette H0 si N
trop petite, les valeurs critiques devant être choisies de façon conservatrice en
raison du caractère discret de la loi binomiale (plus commodément on pourra
se contenter d’indiquer la P-valeur de la valeur observée de la statistique).
'=μ
'0 (ou H0 : μ
'≤μ
'0 ) versus H1 : μ
'≥μ
'0 .
Considérons le test unilatéral H0 : μ
'0 est inférieure à la médiane et la probabilité d’observer une valeur
Sous H1 , μ
' suit donc une loi B(n, p) où p < 1 et l’on
inférieure à μ
'0 est inférieure à 12 . N
2
' prendra une valeur trop petite. A l’inverse
rejettera l’hypothèse nulle lorsque N
' prendra une valeur trop grande.
'≤μ
'0 on rejettera H0 lorsque N
pour H1 : μ
Ce test est appelé test du signe car, en retranchant préalablement μ
'0 à
' devient le nombre de valeurs négatives (ou nulles).
chaque observation, N
Étant identique à un test sur le paramètre p d’une loi de Bernoulli le test est
'0 ) qui dépend de F
sans biais. Le calcul de sa puissance repose sur p = P (Xi ≤ μ
choisi dans l’alternative H1 . Étant donné que l’information initiale est ramenée
à une information binaire on ne peut s’attendre à un test très puissant. Ceci
est la contrepartie de sa validité très générale.
F étant supposée continue la probabilité qu’une valeur soit exactement égale
à μ
'0 est nulle. Toutefois, en raison du caractère discret de toute mesure pratique, il se peut qu’une ou plusieurs valeurs soient égales à μ
'0 et donc inclassables dans la procédure. On dit avoir aﬀaire à un problème d’ex aequo. Le
remède recommandé par Lehmann (1975) consiste à ignorer ces valeurs, diminuant d’autant la taille de l’échantillon.
Le test du signe s’applique également au cas des échantillons appariés.
Pour ﬁxer les idées prenons le cas de n individus observés avant et après un
traitement. Soit (Xi , Yi ), i = 1, · · · , n , les n couples d’observations correspondantes et p = P (Yi > Xi ). L’hypothèse nulle que la distribution est identique
avant et après un traitement, à savoir qu’il n’y a pas d’eﬀet du traitement, implique que p = 12 . La statistique est alors le nombre de v.a. Yi −Xi négatives, les
valeurs nulles devant être écartées. Il est intéressant de noter que la procédure
s’applique au cas discret (le test étant conditionnel aux diﬀérences non nulles)
et lorsque l’information sur chaque couple est un simple classement.
Le test s’étend aisément à un test portant sur un quantile en remplaçant la
valeur p = 12 dans H0 par l’ordre du quantile considéré.

10.5.4

Tests de localisation de deux lois

On considère ici qu’on est en présence de deux lois dont les fonctions de
répartition F1 et F2 ont la même forme mais peuvent être localisées diﬀéremment. En d’autres termes leurs graphes sont identiques à une translation près.

Chapitre 10. Tests non paramétriques

275

Nous supposons que ces lois sont continues, alors leurs densités sont également
translatées l’une par rapport à l’autre. Une telle situation n’est pas rare, notamment pour les échantillons appariés qui feront l’objet d’une attention particulière. Remarquons d’ailleurs que la condition d’égalité des variances imposée
en section 9.7.3 pour tester l’égalité des moyennes de deux lois de Gauss induit
une situation de ce type.
Mathématiquement le modèle de localisation (ou modèle de position) s’écrit :
F2 (x) = F1 (x − δ), pour tout x ∈ R,
où δ est une constante inconnue caractérisant le décalage des deux lois. Si δ est
positif le graphe de la densité de la deuxième loi est translaté à droite de celui
de la première, il est à gauche si δ est négatif. Le test porte sur l’hypothèse
nulle d’identité de ces lois ce qui équivaut à :
H0 : δ = 0 vs.

H1 : δ = 0.

On peut également envisager des tests unilatéraux.
Nous présentons en premier lieu le test de Wilcoxon qui illustre bien l’usage
des rangs dans l’approche non paramétrique.
Test de Wilcoxon ou de Mann-Whitney
Ce test a été proposé initialement par Wilcoxon (1945). Par la suite Mann
et Whitney (1947) ont proposé une forme équivalente qui permit de préciser
ses propriétés.
Soit deux échantillons indépendants X1 , X2 , . . . , Xn1 et Y1 , Y2 , . . . , Yn2
issus respectivement de chaque loi. Considérons la fusion des n1 + n2 valeurs
en un seul échantillon et les rangs associés à celui-ci. La statistique de test de
Wilcoxon est la somme des rangs de l’un des échantillons initiaux. Il est plus
rapide de choisir celui de plus petite taille et nous supposerons qu’il s’agit du
premier (soit n1 ≤ n2 ), notant alors la somme de ses rangs Tn1 . La valeur
minimale pour Tn1 est atteinte lorsque toutes les réalisations x1 , x2 , . . . , xn1
sont situées à gauche des réalisations y1 , y2 , . . . , yn2 sur la droite réelle et elle
vaut 1 + 2 + · · · + n1 = 12 n1 (n1 + 1). La valeur maximale est atteinte lorsque
toutes les observations xi sont situées à droite des observations yj sur la droite
réelle et elle vaut :
1
(n2 + 1) + (n2 + 2) + · · · + (n2 + n1 ) = n1 n2 + n1 (n1 + 1).
2
Intuitivement on est enclin à rejeter H0 lorsque la valeur de Tn1 s’approche
de l’un ou l’autre de ces extrêmes (mais un seul d’entre eux pour un cas unilatéral). Pour déterminer les valeurs critiques il est nécessaire d’établir la loi de
cette statistique sous H0 . Cela peut être fait par des considérations combinatoires, lesquelles ont conduit à la construction de tables bien répandues. Nous
montrons sur un exemple la démarche utilisée.

276

Statistique − La théorie et ses applications

Exemple 10.3 Soit les résultats suivants :

valeurs
rangs

échant. 1 n1 = 4
15 24 12 10
3
6
2
1

échant. 2 n2 = 5
35 25 20 29 16
9
7
5
8
4

La statistique de test Tn1 prend la valeur 12. Calculons la P-valeur correspondante pour un test bilatéral en établissant la loi de Tn1 sous H0 , en partant
des valeurs extrêmes 10 et 30.
Sous H0 toutes les 9 v.a. sont de même loi et la proposition 10.1 s’applique :
les 9 ! permutations des rangs sont équiprobables. Pour calculer P (Tn1 = 10)
il faut dénombrer les permutations de rangs aboutissant à une somme 10 pour
les rangs du premier échantillon. Pour cela il faut que ces rangs soient {1,2,3,4}
dans un ordre quelconque soit 4 ! possibilités. Pour chacune de ces possibilités
on peut permuter les 5 rangs restant pour le deuxième échantillon. Il y a donc
en tout 4! 5! cas possibles, d’où :
P (Tn1 = 10) =

4! 5!
= 0,0079365 .
9!

Pour l’événement (Tn1 = 30) il faut que les rangs du premier échantillon soient
une permutation sur {6,7,8,9} ce qui conduit à la même probabilité. Examinons
maintenant l’événement (Tn1 = 11). Il n’y a toujours qu’une possibilité pour
la liste des rangs du premier échantillon, soit {1,2,3,5}, donc encore la même
probabilité que ci-dessus. Ceci vaut également pour (Tn1 = 29). Enﬁn, pour
obtenir (Tn1 = 12), deux listes sont possibles : {1,2,4,5} et {1,2,3,6}. De même
pour (Tn1 = 28) on a deux listes : {5,6,8,9} et {4,7,8,9}. Donc P (Tn1 = 12) =
P (Tn1 = 28) = 2 4!9!5! = 0,015873. Finalement :
P (Tn1 ≤ 12) = P (Tn1 ≥ 28) = 4

4! 5!
 0,032
9!

et la P-valeur, pour une alternative bilatérale, est 0,064. Si l’on se ﬁxe comme
niveau α = 0,05 on doit accepter H0 . Pour une alternative unilatérale, par
exemple que le graphe de la densité de la loi mère du deuxième échantillon soit
translaté à droite de celui du premier, i.e. H1 : δ > 0, alors la P-valeur serait
égale à 0,032 et il faudrait rejeter H0 , donc considérer qu’il y a bien translation
à droite.

Cet exemple est instructif sur plusieurs aspects. Tout d’abord on voit que
le point crucial pour déterminer la loi de Tn1 est de dénombrer les façons
d’obtenir un total donné en choisissant k entiers parmi les n premiers entiers
(ici k = n1 et n = n1 +n2 ). Ceci est un problème de combinatoire résolu par une
relation de récurrence en partant du plus petit total. De plus, ce dénombrement
est identique en partant du plus grand total et, par conséquent, la loi de la
statistique est symétrique. Ainsi on peut aisément construire des tables de

Chapitre 10. Tests non paramétriques

277

valeurs critiques, lesquelles doivent être conservatrices vu le caractère discret
de la loi. Enﬁn on voit l’intérêt de la procédure dans le cas de petits échantillons.
Pour les grandes tailles d’échantillons (en fait n1 > 10 et n2 > 10 sufﬁsent) on peut utiliser une approximation gaussienne découlant du comportement asymptotique de Tn1 sous H0 . Pour cela il faut utiliser la moyenne et la
variance de cette statistique sous H0 :
n1 (n1 + n2 + 1)
2
n1 n2 (n1 + n2 + 1)
.
V (Tn1 ) =
12
E(Tn1 ) =

L’espérance mathématique, en raison de la symétrie de la loi, est simplement
la demi-somme des deux valeurs extrêmes données plus haut. Pour établir la
variance on peut utiliser les formules générales sur les moments des rangs indiquées à la suite de la proposition 10.2, de la façon suivante.
 1
Comme Tn1 = ni=1
Ri , on a, par extension de la formule sur la variance
d’une somme de deux v.a. non indépendantes vue en section 3.5,
V (Tn1 ) =

n1


V (Ri ) + 2

i=1

où la somme


i<j



cov(Ri , Rj )

i<j

est à eﬀectuer sur tous les

n1 (n1 −1)
couples (Ri , Rj ) du
2
(n1 +n2 )2 −1
=
et cov(Ri , Rj ) =
12

premier échantillon tels que i < j. Comme V (Ri )
2 +1
quels que soient i et j, on obtient ﬁnalement :
− n1 +n
12

(n1 + n2 )2 − 1
n1 (n1 − 1) (n1 + n2 + 1)
−2
12
2
12
n1 (n1 + n2 + 1)
[(n1 + n2 − 1) − (n1 − 1)]
=
12
n1 n2 (n1 + n2 + 1)
=
.
12

V (Tn1 ) = n1

La statistique U proposée par Mann et Whitney est la suivante : pour
chaque Yj on compte le nombre d’observations Xi qui lui sont supérieures puis
on totalise ces nombres pour j = 1, · · · , n2 . En posant :
Zij =

1
0

cette statistique s’écrit :
U=

si Xi > Yj
,
si Xi < Yj
n1 
n2

i=1 j=1

Zij .

278

Statistique − La théorie et ses applications

On montre (voir exercices) que U est égale à Tn1 à une constante près :
U = Tn1 −

n1 (n1 + 1)
.
2

Ainsi on peut aussi établir la procédure et les résultats précédents à partir
de U. Certaines tables sont d’ailleurs données en fonction de cette statistique.
Nous avons ignoré jusqu’ici le problème des ex aequo, c’est-à-dire lorsque
deux (ou plusieurs) valeurs sont égales et ne peuvent être rangées. Le remède
le plus simple est celui des rangs moyens qui consiste à attribuer à chacune de
ces valeurs la moyenne des rangs qu’elles auraient totalisés si elles avaient été
diﬀérentiées (si, par exemple, il y a deux valeurs identiques après la sixième
valeur, chacune reçoit le rang 7,5 ; trois valeurs identiques recevraient le rang
8). En théorie ceci nécessite un correctif pour la statistique de test mais qui
reste mineur si les ex aequo ne sont pas trop nombreux. Une autre méthode
plus eﬃcace mais plus lourde consiste à attribuer les rangs de façon aléatoire.
De nombreuses études, soit asymptotiques, soit par simulations pour des
tailles d’échantillons réduites, ont été eﬀectuées pour étudier la puissance du
test en fonction de divers types de lois mères. Ceci est notamment vrai pour
le cas de lois de Gauss qui mène à une comparaison avec le test de Student
classique. Asymptotiquement le rapport de la puissance du test de Wilcoxon à
celle du test de Student est de 0,96, cette valeur étant pratiquement atteinte
avec des tailles d’échantillons de l’ordre de 50. Hodges et Lehmann (1956) ont
établi que le rapport asymptotique ne descend pas au-dessous de 0,86 quelle
que soit la loi.
Ces résultats justiﬁent certainement l’usage répandu de ce test. Cependant
la condition d’un modèle de localisation est une restriction importante. Cette
condition peut toutefois être assouplie. Si l’hypothèse alternative est de la forme
F1 (x) > F2 (x) pour tout x ou F1 (x) < F2 (x), c’est-à-dire que les graphes
restent totalement décalés, les propriétés du test sont globalement conservées.
Echantillons appariés : test des rangs signés
Ce test, également dû à Wilcoxon, est un dérivé du précédent. Il repose sur
le fait que, si les lois mères sont identiques, la loi des diﬀérences Xi −Yi doit être
symétrique par rapport à 0. Ainsi on s’attend à ce que les rangs des diﬀérences
absolues |Xi − Yi | se partagent équitablement pour les diﬀérences positives et
pour les diﬀérences négatives. Ayant rangé les |Xi − Yi | par valeurs croissantes,
la statistique de test est la somme T + des rangs associés aux diﬀérences Xi −Yi
positives.
Soit les v.a. Zi , i = 1, · · · , n , déﬁnies par
Zi =

1
0

si Xi − Yi > 0
.
si Xi − Yi < 0

Chapitre 10. Tests non paramétriques

279

n
Alors T + est égal à i=1 iZi . Sous H0 , Zi suit une loi de Bernoulli B( 12 ) ce
qui permet d’établir la loi de T + . Ses valeurs possibles sont 0, 1, · · · , n(n+1)
,
2
la valeur 0 étant atteinte lorsqu’il n’y a aucune diﬀérence positive et la valeur
n(n+1)
lorsque toutes les diﬀérences sont positives. Dans le premier cas cela
2
porte à croire que la (densité de la) loi mère des Xi est décalée à gauche de
celle des Yi et, dans le deuxième cas, qu’elle est décalée à droite. Ceci oriente
donc le sens du rejet pour un test unilatéral.
On trouve aisément des tables de valeurs critiques et l’on peut utiliser une
approximation gaussienne dès que n > 30 en utilisant la moyenne et la variance de T + . Comme E(Zi ) = 12 , V (Zi ) = 14 et que les Zi sont mutuellement
indépendantes (comme fonctions respectives des paires (Xi , Yj ) ), on a :
1
n(n + 1)
i=
E(T ) =
2 i=1
4
n

+

1 2
n(n + 1)(2n + 1)
i =
4 i=1
24
n

V (T + ) =

sachant que 12 + 22 + · · · + n2 = 16 n(n + 1)(2n + 1).
Sous H1 les Zi ont une loi B(p) avec p = P (Xi −Yi > 0) et la puissance peut
être calculée en fonction de p, lequel dépend toutefois de la loi mère des Xi −Yi .
Dans un problème de localisation le test est sans biais et est plus puissant que
le test du signe mentionné en section 10.5.3.
Le test des rangs signés est parfois employé comme alternative au test du
signe dans le cas d’un seul échantillon, mais cette pratique est contestable en
raison de la condition forte de symétrie de la loi mère, peu réaliste dans
une approche non paramétrique.
Test de la médiane
Ce test opère dans le même cadre que le test de Wilcoxon pour deux échantillons indépendants et relève du même esprit que le test du signe. On détermine
la médiane des n1 + n2 valeurs fusionnées et l’on considère le nombre d’observations du premier échantillon inférieures à cette médiane globale. Désignons
'1 la statistique correspondante. Supposons par commodité que n1 + n2
par N
est pair et posons n1 + n2 = 2r, de façon qu’il y ait exactement r observations
à gauche comme à droite de la médiane globale (celle-ci peut être indiﬀéremment n’importe quelle valeur entre les deux observations les plus centrales).
'1
Sous l’ hypothèse nulle d’identité des deux lois on s’attend à une valeur de N
proche de n1 /2 et l’on rejettera donc H0 si la réalisation de cette statistique
s’éloigne trop de n1 /2. Étant donné que l’échantillon fusionné a été divisé en
deux parties de taille égale et que, sous H0 , toutes les v.a. sont i.i.d., la loi
'1 , nombre d’observations parmi n1 observations appartenant à l’ensemble
de N

280

Statistique − La théorie et ses applications

des r plus petites valeurs, correspond à la déﬁnition même (voir section 4.1.5)
d’une loi hypergéométrique H(2r, r, n1 ). On en déduit immédiatement :
'1 ) =
E(N

n1
2

et

'1 ) =
V (N

n1 n2
,
4(n1 + n2 − 1)

valeurs à utiliser pour une approximation gaussienne dès lors que n1 et n2 sont
assez grands. Si n1 + n2 est impair on montre, en posant n1 + n2 = 2r + 1, que
'1 suit une loi H(2r + 1, r, n1 ). Ainsi, dans ce cas :
N
'1 ) =
E(N

n1 (n1 + n2 − 1)
2(n1 + n2 )

et

'1 ) = n1 n2 (n1 + n2 + 1) .
V (N
4(n1 + n2 )2

Comme on peut s’y attendre ce test est moins puissant que le test de
Wilcoxon du fait qu’il ignore les rangs des observations (ainsi, pour la comparaison dans le cas de lois mères gaussiennes, le rapport asymptotique de sa
puissance à celle du test de Student tombe à 0,63 contre 0,96 pour le test de
Wilcoxon). En contrepartie sa portée dépasse le seul modèle de localisation. Il
peut être appliqué, par exemple, comme test d’égalité des médianes des deux
lois. Par ailleurs il oﬀre un substitut au test de Wilcoxon si le nombre d’ex
aequo est important suite à une forte discrétisation des données recueillies. Son
pendant pour deux échantillons appariés est le test du signe décrit plus haut.
Estimateur de Hodges-Lehmann du décalage des deux lois
On cherche à estimer le paramètre δ qui caractérise le modèle de localisation,
à savoir tel que :
F2 (x) = F1 (x − δ), pour tout x ∈ R,
où F1 est la fonction de répartition des Xi et F2 celle des Yj. On peut estimer
ponctuellement δ par la diﬀérence y − x des moyennes observées dans chaque
échantillon et fournir un intervalle de conﬁance approché avec la procédure
classique de Student (supposant que ces lois admettent une moyenne et une
variance). Toutefois, pour de petits échantillons et/ou en présence de valeurs
extrêmes, il est souhaitable de disposer de procédures plus ﬁables. Hodges et
Lehmann (1963) ont proposé une approche en relation avec les tests non paramétriques que nous exposons dans le cas du test de Wilcoxon-Mann-Whitney,
le principe étant identique pour d’autres tests. Nous nous intéressons particulièrement à l’intervalle de conﬁance.
Pour une valeur arbitraire δ0 , on détermine la valeur tn1 (δ0 ) prise par la
statistique de Wilcoxon pour la série fusionnée :
x1 , . . . , xi , . . . , xn1 , y1 − δ0 , . . . , yj − δ0 , . . . , yn2 − δ0 .
En vertu de l’équivalence test-IC (voir section 9.8) on prend comme IC de
niveau 1−α pour δ l’ensemble des valeurs δ0 telles que tn1 (δ0 ) reste à l’intérieur

Chapitre 10. Tests non paramétriques

281

des valeurs critiques au niveau α du test. Ceci est justiﬁé par le fait que, si δ0
est la vraie valeur, les Xi et les Yj − δ0 ont la même loi et la statistique Tn1 (δ0 )
suit alors la loi parfaitement déterminée sous H0 pour des échantillons de taille
n1 et n2 .
Faire varier δ0 peut être fastidieux mais cela n’est pas nécessaire car on
montre que cet IC peut être obtenu de façon simple et directe comme suit. On
a vu plus haut que la statistique Tn1 de Wilcoxon a, sous H0 , une loi symétrique
sur l’ensemble des entiers de 12 n1 (n1 + 1) à n1 n2 + 12 n1 (n1 + 1). Soit kα le plus
petit entier tel que, sur cette loi, la probabilité associée à l’intervalle :
1
1
[ n1 (n1 + 1) + kα , n1 n2 + n1 (n1 + 1) − kα ]
2
2
soit au moins égale à 1 − α. On considère alors la série des n1 n2 valeurs yj − xi
pour tous i et j. L’intervalle de conﬁance de niveau (conservateur) 1−α s’obtient
en prenant comme bornes les statistiques d’ordres kα et n1 n2 − kα + 1 de cette
série de valeurs.
Pour l’estimation ponctuelle on choisit la valeur δ0 telle que tn1 (δ0 ) coı̈ncide
avec la valeur de probabilité maximale sur la loi de référence. On montre que
cette estimation est simplement la médiane de la série des yj −xi . Cette méthode
fournit des estimateurs sans biais.
Exemple 10.4 Reprenons l’exemple 10.3. On a vu que, pour n1 = 4 et n1 = 5,
la statistique Tn1 peut prendre les valeurs entières de 10 à 30 et que
P (Tn1 ≤ 11) + P (Tn1 ≥ 29) = 0,032 .
Donc P (12 ≤ Tn1 ≤ 28) = 1−0,032 alors que P (13 ≤ Tn1 ≤ 27) = 1−0,064.
Pour α = 0,05 on a donc k0,05 = 2. La série des yj − xi ordonnée est :
−8, −4, 1, 1, 4, 5, 5, 6, 6, 8, 10, 10, 13, 14, 15, 15, 17, 18, 19, 20
donc l’intervalle de conﬁance est [−4 ; 19] et l’estimateur ponctuel est 9 (en
prenant pour médiane la valeur milieu entre 8 et 10).
L’approche classique donne pour estimation ponctuelle y−x = 9,75. L’intervalle de conﬁance ci-dessus étant en fait de niveau 0,968 , celui de même niveau
(7)
obtenu par la formule de Student (voir section 7.4.3) avec t0,984 = 2,67 est
[−2,7 ; 22,2]. Ce dernier a une amplitude plus grande mais les deux intervalles
restent assez semblables du fait qu’il n’y a pas de valeurs extrêmes.


10.5.5

Test pour la corrélation de Spearman

Nous présentons brièvement ce test pour illustrer encore l’intérêt des procédures reposant sur les rangs. On considère un couple (X, Y ) de fonction
de répartition conjointe FX,Y (x, y) inconnue et un échantillon de taille n :

282

Statistique − La théorie et ses applications

(X1 , Y1 ), (X2 , Y2 ), · · · , (Xn , Yn ), issu de cette même loi. On souhaite tester l’hypothèse nulle d’indépendance de ces deux composantes du couple, soit :
H0 : FX,Y (x, y) = FX (x)FY (y) pour tout (x, y) ∈ R2 ,
où FX et FY sont les lois marginales des composantes. L’alternative H1 est la
négation de H0 , à savoir qu’il existe au moins un couple de valeurs (x, y) tel
que FX,Y (x, y) = FX (x)FY (y).
En 1904 Spearman a proposé comme statistique de test la corrélation des
rangs que nous notons RS . Elle est obtenue simplement en remplaçant, séparément, dans chaque composante les observations par leurs rangs et en calculant
sur ces derniers la corrélation linéaire empirique vue en section 9.7.7. Notons
que la valeur prise par RS sera égale à 1 (respectivement -1) si Y est une
fonction croissante (respectivement décroissante) de X. La corrélation de rangs
a pour intérêt de mettre en évidence des liens non linéaires.
Sous H0 on s’attend à ce que Rs reste proche de zéro. Comme cette statistique repose sur les rangs sa loi ne dépend pas de FX ni de FY . En établissant
(voir exercices) que Rs peut aussi s’écrire :
n


Rs =

i=1

n(n + 1)2
4
,
2
n(n − 1)
12

Ri Si −

où Ri est le rang de Xi et Si celui de Yi , et en utilisant l’espérance et la variance
d’un rang données à la suite de la proposition 10.2, on montre aisément que :
E(Rs ) = 0
V (Rs ) =
Par ailleurs on montre que :

1
.
n−1

√
n − 2RS
%
1 − RS2

suit approximativement une loi de Student à n − 2 degrés de liberté, ce qui est
à rapprocher du résultat de la section 9.7.7 concernant la corrélation linéaire.
Pour les faibles valeurs de n on dispose de tables des valeurs critiques à diﬀérents
niveaux.

Nous avons vu quelques tests non paramétriques parmi les plus courants. Les
tests fondés sur les rangs sont d’une grande variété et font l’objet d’ouvrages
spéciﬁques. Citons par exemple le livre collectif édité par Droesbeke et Fine
(1996), celui de Lecoutre et Tassi (1987) ainsi qu’en anglais Lehmann (1975)
et Gibbons (1985).

Chapitre 10. Tests non paramétriques

10.6

283

Exercices

Exercice 10.1 Montrer que le test du khi-deux de la section 10.2 est le test
d’égalité des paramètres de deux lois de Bernoulli vu en section 9.7.6 .
Aide : on s’inspirera de la démarche de la section 10.1.4.
Exercice 10.2 Soit un couple (X , Y) de variables catégorielles, X avec I catégories notées {1, · · · , i, · · · , I} et Y avec J catégories notées {1, · · · , j, · · · , J}.
Montrer qu’elles sont indépendantes si et seulement si il y a indépendance entre
tous les couples élémentaires (i, j) de catégories croisées.
Aide : Soit A = {i1 } un événement sur X et B = {j1 , j2 } un événement sur
Y. (i.e. le résultat de l’expérience est la catégorie j1 ou j2 ), montrer que A et B
sont indépendants si {i1 } est indépendant de {j1 } et de {j2 } respectivement,
puis généraliser.
Exercice 10.3 Dans le contexte du test exact de Fisher (section 10.3.2) montrer que sous l’hypothèse d’indépendance on a :
P (N11 = n11 |n1. , n2. , n.1 , n.2 ) =

n1. ! n2. ! n.1 ! n.2 !
n! n11 ! n12 ! n21 ! n22 !

avec les contraintes nécessaires sur n11 , n12 , n21 et n22 .
En déduire que la loi de la v.a. N11 conditionnellement aux marges est une loi
H(n, n1., n.1 ).
Aide : on suivra la même démarche que dans la démonstration du même
type de la section 9.7.6 .
Exercice 10.4 (Test de McNemar) Soit un échantillon apparié de n couples
d’individus. Sur chacun des 2n individus on observe une même variable binaire succès/échec. Pour chaque couple on a donc une variable catégorielle à
4 catégories. Les probabilités et les fréquences (entre parenthèses) sont notées
selon le tableau ci-après :
XXX
X

XXIndiv.2
succès
XXX
Indiv.1
X
succès
p11 (n11 )
échec
p21 (n21 )
p.1 (n.1 )

échec
p12 (n12 )
p22 (n22 )
p.2 (n.2 )

p1. (n1. )
p2. (n2. )
1 (n)

On considère l’hypothèse nulle que la probabilité de succès est la même
pour les deux individus d’un couple, i.e. H0 : p1. = p.1 .
1. Donner la fonction de vraisemblance pour cette loi multinomiale à 4 catégories
sous H0 . (aide : noter que H0 équivaut à p12 = p21 et intégrer les contraintes
dans la fonction de vraisemblance comme en section 10.1.1)
2. Donner les estimations du MV des pij sous H0 .

Statistique − La théorie et ses applications

284

3. En déduire les estimations des fréquences attendues sous H0 et montrer que
la réalisation q de la statistique Q du test du khi-deux pour H0 est :
q=

(n12 − n21 )2
.
n12 + n21

4. Quels sont les degrés de liberté de la loi asymptotique de cette statistique
sous H0 ?
Exercice 10.5 Démontrer la relation entre la statistique U de Mann-Whitney
et Tn1 de Wilcoxon.
Aide : on considérera les statistiques d’ordre X(1) , X(2) , . . . , X(n1 ) et les
rangs correspondants R(1) , R(2) , . . . , R(n1 ) . On exprimera alors chacun de ces
rangs en fonction du nombre d’observations Yj inférieures à la statistique
d’ordre à laquelle il correspond.
Exercice 10.6 Montrer que la corrélation de Spearman peut s’écrire
n
Rs =

i=1

n(n+1)2
4
n(n2 −1)
12

Ri Si −

.

Aide : utiliser les formules générales de décentrage :
n

i=1

(xi − x)(yi − y) =

n


xi yi − n x y et

i=1

n


(xi − x) =
2

n


i=1

x2i − nx2 .

i=1

Exercices appliqués
Exercice 10.7 Une enquête sur la gêne causée par la proximité d’un aéroport
a donné, par sexe, les résultats suivants :
PP
PP Sexe Femmes
Gêne PPP
P
Aucune
75
Faible
25
Moyenne
17
Forte
3
120

Hommes

Tous

35
27
8
12
82

110
52
25
15
202

Identiﬁer la situation d’échantillonnage et poser l’hypothèse nulle correspondant à la question informelle : la gêne est-elle identique pour les deux sexes ?
Tester cette hypothèse nulle.

Chapitre 10. Tests non paramétriques

285

Exercice 10.8 Lors d’une enquête auprès de 825 familles ayant eu 3 enfants
on a relevé le nombre de garçons dans chaque famille comme suit :
Nombre de garçons
fréquences

0
71

1
297

2
336

3
121

Tous
825

On fait l’hypothèse que les sexes des enfants lors des naissances successives
au sein d’une famille sont des variables catégorielles indépendantes et que la
probabilité p d’avoir un garçon reste constante. Déterminer en fonction de p
la loi du nombre de garçons pour une famille de 3 enfants. Estimer p et tester
l’hypothèse de départ.
Aide : on utilisera le test du khi-deux avec l’estimation de p usuelle par le
maximum de vraisemblance.
Exercice 10.9 On donne, pour une agglomération, la répartition du nombre
de jours sans accident, avec un accident etc., parmi 50 jours d’observation tirés
au hasard dans une année :
nombre d’accidents
0
1
2
3
4
total

nombre de jours
21
18
7
3
1
50

Tester que la répartition du nombre quotidien d’accidents suit une loi de Poisson.
Aide : on eﬀectuera le test du khi-deux en regroupant les catégories de façon
à ne pas avoir de fréquences inférieures à 5. Pour simpliﬁer on estimera λ par
l’estimation usuelle du maximum de vraisemblance.
Exercice 10.10 Dans une enquête auprès de 93 étudiant(e)s sélectionnés au
hasard dans une université on pose une question sur le mode de logement avec
4 modalités de réponse : seul (S), dans la famille (F), en couple (C) et autres
modes (A). Les résultats obtenus par sexe sont les suivants :

Féminin
Masculin

S
12
15

F
11
6

C
14
9

A
12
14

Tester l’hypothèse d’indépendance du mode de logement et du sexe.
Exercice 10.11 Un échantillon de 490 utilisateurs de téléphones portables a
été constitué avec des quotas d’âge, c’est-à-dire qu’on a sélectionné des personnes au hasard jusqu’à atteindre un nombre ﬁxé de personnes dans chaque

286

Statistique − La théorie et ses applications

classe d’âge. Celles-ci ont été interrogées sur l’opérateur choisi. Le tableau cidessous donne la répartition des choix eﬀectués en fonction de l’âge de l’utilisateur.
opérateur 1 opérateur 2 opérateur 3
10-19
17
32
57
20-35
38
72
64
36-50
53
42
39
51 +
30
19
27
Identiﬁer la situation d’échantillonnage appropriée et exprimer formellement
l’hypothèse correspondant à la formulation suivante : il n’y a pas de relation
entre l’âge et le type d’opérateur choisi. Tester cette hypothèse.
Exercice 10.12 Une enquête par sondage est menée parallèlement dans deux
pays de l’Union Européenne sur la répartition des revenus dans une catégorie
bien déterminée de salariés. On obtient les résultats suivants :
Salaire mensuel (euros)
<1200
1200-1600
1601-2000
>2000
Ensemble

Pays A
4
22
20
14
60

Pays B
6
18
18
6
48

Identiﬁer la situation d’échantillonnage appropriée. La diﬀérence entre les répartitions des revenus observées dans les deux pays est-elle signiﬁcative ?
Exercice 10.13 Les données du tableau qui suivent ont été étudiées par le
statisticien belge A. Quetelet (1796-1874) et reprises de l’ouvrage de W.S.
Peters : Counting for Something (Springer-Verlag, N.Y., 1986). Elles concernent
les mesures (en pouces) de tour de poitrine de 5 738 soldats écossais.
Mesure
Fréquence
Mesure
Fréquence

33
3
41
934

34
18
42
658

35
81
43
370

36
185
44
92

37
420
45
50

38
749
46
21

39
1073
47
4

40
1079
48
1

Tester l’ajustement d’un modèle gaussien pour ces données.
Aide : il s’agit de données regroupées (par arrondi au pouce le plus proche)
pour lesquelles on procédera comme indiqué en section 10.4.1 à ce propos.
Exercice 10.14 On a interrogé deux échantillons indépendants de 30 personnes chacun. Le premier échantillon est constitué de personnes appartenant à des ménages avec enfant(s), le deuxième de personnes appartenant à
des ménages sans enfant. A la question «Considérez-vous que l’éducation des
enfants est actuellement trop permissive ?» on a demandé aux enquêtés de

Chapitre 10. Tests non paramétriques

287

répondre en se positionnant sur une échelle ordonnée de 1 (oui, tout à fait) à 5
(non, pas du tout). Les résultats obtenus sont les suivants :
Réponse :
avec enfant
sans enfant
Ensemble

1
4
2
6

2
11
6
17

3
5
8
13

4
8
8
16

5
2
6
8

6
30
30
60

On veut tester l’hypothèse qu’il n’y a pas de diﬀérence d’attitude entre
les deux types de personnes. Étant donné les tailles relativement réduites des
échantillons d’une part, et le ﬂou de la mesure eﬀectuée d’autre part, on
procédera à un test non paramétrique.
Aide : vu qu’il n’y a que 5 valeurs possibles on aura un nombre d’ex aequo
important. On préférera donc le test de la médiane au test de Wilcoxon. Les
observations correspondant à la valeur médiane générale devront être ignorées.

Chapitre 11

Régressions linéaire,
logistique et non
paramétrique
11.1

Introduction à la régression

A diﬀérentes reprises nous avons dit dans les chapitres précédents que tel ou
tel résultat avait une validité au-delà du cadre strict des échantillons aléatoires
constitués de variables aléatoires i.i.d.. L’objectif de ce chapitre est de montrer comment les méthodes classiques doivent être adaptées lorsque les v.a.
observées restent indépendantes mais ne sont plus identiquement distribuées.
Ceci peut être illustré avec proﬁt dans les modèles explicatifs appelés modèles
de régression.
De façon informelle, un modèle explicatif est un modèle exprimant une
variable Y, appelée variable à expliquer (ou réponse), comme une fonction d’une
ou de plusieurs variables dites variables explicatives ou prédicteurs1 . Toutefois
si l’entité Y est considérée comme une variable aléatoire Y, un terme aléatoire,
caractérisant l’incertitude de la prédiction, doit être introduit d’une certaine
façon dans l’équation du modèle.
Dans un modèle de régression, on cherche essentiellement à déterminer la
variation de l’espérance mathématique de Y en fonction des variables explicatives. En d’autres termes on étudie comment Y évolue «en moyenne» en
fonction de ces variables explicatives. Dans ce chapitre, par souci de simpliﬁcation, nous ne considérons qu’une seule variable explicative ce qui constitue
la régression simple par opposition à la régression multiple. De plus cette entité
1 Un tel modèle ne restituant pas nécessairement une relation de cause à eﬀet directe le
terme de prédicteur serait plus approprié.

290

Statistique − La théorie et ses applications

explicative, que nous symboliserons par la lettre X , sera une variable quantitative2 , pouvant prendre toute valeur dans un intervalle I de R. Aux diﬀérentes
valeurs de X dans I correspondent, par hypothèse, des v.a. distinctes et on est
donc, en fait, en présence d’une famille de v.a. {Y (x) | x ∈ I}. Admettant que
pour tout x l’espérance mathématique existe, alors E(Y (x) ) est la fonction
g(x) qu’il s’agit de rechercher. Cette fonction mettant en évidence l’évolution
moyenne de l’entité Y à expliquer en fonction de x est appelée fonction de
régression. Dans cette approche on considère naturellement que l’incertitude
de la prédiction de Y pour le «niveau» x de X , se manifeste par une v.a.
ε(x) venant s’ajouter à la composante déterministe g(x). Dans sa forme la plus
générale un modèle de régression simple s’écrit donc :
Y (x) = g(x) + ε(x).
Puisque E(Y (x) ) = g(x), on a nécessairement E(ε(x)) = 0, quel que soit x. La
v.a. ε(x) est appelée erreur ou aléa (d’où la notation habituelle du «e» grec).
Dans la plupart des modèles on suppose que l’erreur est de même loi quel que
soit x ce qui permet d’écrire Y (x) = g(x)+ε (on écrit même parfois simplement
Y = g(x) + ε en omettant d’indiquer que la v.a. Y est assujettie à la valeur x).
Le premier modèle que nous étudierons est le modèle de régression linéaire
où g(x) = β0 + β1 x, que l’on écrira donc :
Y (x) = β0 + β1 x + ε.
Ce modèle est le plus simple qui soit et, de ce fait, est celui qui est utilisé le
plus fréquemment. Il stipule qu’en moyenne l’entité Y varie linéairement en
fonction du niveau de l’entité X , ce qui est une hypothèse souvent réaliste. Par
exemple le poids moyen des individus (adultes d’un même sexe) ayant une taille
donnée x peut être considéré comme une fonction croissant linéairement avec x.
La régression linéaire constitue le point de départ historique et méthodologique
de toute la modélisation explicative. Ce modèle a été proposé par Francis Galton dans son ouvrage Natural Inheritance publié en 1889, notamment pour
l’étude de la variation de la taille d’un homme en fonction de celle de son père.
Il a choisi le terme de «régression» constatant qu’en moyenne un père grand
tendra à avoir un ﬁls plus petit que lui (et vice-versa pour un père petit).
Le deuxième modèle présenté dans le chapitre est le modèle logistique dont
la particularité est que la variable à expliquer est binaire, du type «succès» ou
«échec». On la codera comme précédemment par 1 ou 0 pour que Y (x) soit,
pour tout x ∈ I, une v.a. de Bernoulli. On essaie, par exemple, de déterminer
dans quelle mesure le fait d’avoir ou de ne pas avoir d’incident cardiaque à un
certain âge est lié au taux sanguin de cholestérol. Dans l’écriture de ce modèle
de régression nous introduirons une fonction g(x) particulièrement adaptée au
fait que Y (x) prend les valeurs 1 ou 0.
2 La variable explicative pourrait être catégorielle ce qui, dans le cas de la régression
linéaire, correspond à l’analyse de variance à un facteur.

Chapitre 11. Régressions linéaire, logistique et non paramétrique

291

Ces deux premiers types de modèles sont des modèles paramétriques car la
fonction de régression g(x) est de forme connue mais dépendant de paramètres
inconnus (comme β0 et β1 dans le cas de la régression linéaire) qu’il s’agira
d’estimer. Tous deux permettront d’illustrer l’application des méthodes paramétriques classiques des chapitres 6, 7 et 9. Dans le premier cas on obtient
des solutions exactes simples. Dans le second cas on verra comment utiliser
la méthode du maximum de vraisemblance et les propriétés asymptotiques de
l’EMV lorsqu’il n’y a pas de solution explicite. L’intérêt de cette présentation
réside dans le fait que la démarche est celle appliquée en statistique pour la
plupart des modèles complexes.
Le troisième type de modèle sera le modèle non paramétrique où la fonction
de régression g(x) est totalement inconnue et doit être estimée. Nous sommes là
face à un problème d’estimation fonctionnelle comme pour l’estimation d’une
densité ou d’une fonction de répartition, vue en section 8.5.
Le modèle conditionnel
Dans notre exposé l’entité explicative X est une variable déterministe,
ce qui a des implications sur la façon dont les observations sont eﬀectuées.
Cela suppose en eﬀet que l’on se trouve dans des conditions expérimentales
avec un choix planiﬁé des valeurs x1 , x2 , · · · , xn de X , c’est-à-dire ﬁxées à
l’avance selon ce qu’on appelle un plan d’expérience. Supposons par exemple
que l’on veuille étudier l’inﬂuence du taux d’engrais (en kg/hectare) sur le rendement (en tonnes/hectare) d’un type de céréale. On sème alors n parcelles
expérimentales traitées avec des taux d’engrais choisis x1 , x2 , · · · , xn (certaines
valeurs pouvant être répétées). On considère que le rendement de chaque parcelle est une variable aléatoire du fait des multiples facteurs, autres que le
taux d’engrais, qui le déterminent (pour preuve, les valeurs seront certainement
diﬀérentes si l’on répète l’expérience). Les valeurs de rendement y1 , y2 , · · · , yn
eﬀectivement observées seront donc traitées comme des réalisations de variables
aléatoires Y1 , Y2 , · · · , Yn . Ici Yi symbolise la loi du rendement pour un niveau
d’engrais xi , dont la moyenne est E(Yi ) = g(xi ). Ces v.a. Y1 , Y2 , · · · , Yn sont
supposées indépendantes mais non de même loi puisqu’elles diﬀèrent au moins
par leur moyenne. Il s’agira alors d’estimer, à partir de ces observations effectuées en quelques valeurs de X dans I, la fonction g(x) pour tout x ∈ I, où
I est la plage de variation du taux d’engrais qui intéresse l’expérimentateur.
Bien souvent, on ne se trouve pas dans de telles conditions expérimentales
mais plutôt dans le cadre d’observations répétées d’un couple de v.a. (X, Y ).
Prenons, par exemple, le cas d’un sondage eﬀectué pour étudier la variation
du revenu Y en fonction de l’âge X dans une population. En tirant au hasard un individu on génère un couple aléatoire (X, Y ) où X est la valeur
de l’âge et Y est la valeur du revenu. Pour n individus on observe alors un
échantillon aléatoire (X1, Y1 ), (X2 , Y2 ), · · · , (Xn , Yn ) à valeurs dans R2 . Pour les
valeurs eﬀectivement observées (x1, y1 ), (x2 , y2 ), · · · , (xn , yn ), les x1 , x2 , · · · , xn
doivent être considérées comme des réalisations de v.a. au même titre que les

Statistique − La théorie et ses applications

292

y1 , y2 , · · · , yn . En eﬀet, contrairement aux circonstances précédentes les valeurs
des xi ne pouvaient être connues avant expérience, car résultant du processus de sélection au hasard. L’objectif essentiel restant d’étudier comment «en
moyenne» le revenu varie en fonction de l’âge, la fonction de régression g(x) est
alors l’espérance mathématique de la loi conditionnelle de Y sachant
X = x, notée E(Y |X = x), soit :
g(x) = E(Y |X = x) .
Néanmoins, dans le modèle classique de régression, les valeurs x1 , x2 ,
· · · , xn ne sont pas traitées comme des réalisations de variables aléatoires, ce qui facilite grandement les calculs. Stricto sensu les développements
de ce chapitre ne sont valables que conditionnellement aux valeurs prises par
les Xi . Le modèle de régression n’est donc, en principe, pas approprié dans
une situation d’observations répétées d’un couple de variables aléatoires. Les
propriétés d’optimalité conditionnelle des estimateurs, par exemple, peuvent
être perdues lorsqu’on prend en compte le caractère aléatoire des Xi . Toutefois
on constate généralement que ces estimateurs conservent des qualités assez
proches, voire même identiques. Aussi le praticien applique-t-il les procédures
de la régression quelles que soient les circonstances de la collecte des données.
Le formalisme conditionnel étant plus général car permettant d’envisager
que la variable explicative puisse avoir un statut aléatoire, nous l’adopterons
comme la plupart des auteurs. Mais il est clair que les résultats issus du modèle
conditionnel s’appliqueront de la même façon à la situation décrite initialement
avec l’exemple du rendement d’une céréale.

11.2

La régression linéaire

11.2.1

Le modèle

Nous supposons donc que la fonction de régression est de la forme :
E(Y |X = x) = β0 + β1 x
pour x appartenant à un certain intervalle I. De plus nous supposons que la
variance de la loi conditionnelle de Y sachant X = x ne dépend pas de x et est
égale à σ 2 . Enﬁn nous faisons l’hypothèse que cette loi est gaussienne, quel que
soit x. Nous reviendrons toutefois par la suite sur cette condition qui n’est pas
cruciale. Notons que la linéarité du modèle est relative aux√paramètres β0 et
β1 , et que l’on peut substituer à X des transformées ln X, X, X 2 etc., pour
atteindre éventuellement la linéarité du modèle.
Le modèle contient donc trois paramètres inconnus : β0 , β1 et σ 2 . Pour
estimer ces paramètres nous considérons une série d’observations indépendantes
Y1 , Y2 , · · · , Yn situées, respectivement, aux niveaux x1 , x2 , . . . , xn de la variable
explicative ﬁxés (on suppose naturellement qu’au moins deux de ces valeurs sont

Chapitre 11. Régressions linéaire, logistique et non paramétrique

293

distinctes). Ainsi, pour tout i, Yi ; N (β0 + β1 xi , σ 2 ). Il est aussi commode
d’utiliser la notation :
Yi = β0 + β1 xi + εi

,

i = 1, . . . , n.

Pour tout i on a donc εi ; N (0 , σ 2 ). L’indépendance des Yi entraı̂ne celle des
«erreurs» ε1 , ε2 , . . . , εn qui sont donc des variables aléatoires i.i.d..

11.2.2

Les estimateurs du maximum de vraisemblance

La fonction de vraisemblance des trois paramètres, associée aux réalisations
y1 , y2 , · · · , yn , est :
n
$
1
1
√
exp{− 2 [yi − (β0 + β1 xi )]2 }.
L(β0 , β1 , σ2 ) =
2
2σ
2πσ
i=1
D’où la log-vraisemblance :

n
√
1
1 
ln L(β0 , β1 , σ 2 ) = −n(ln 2π + ln σ 2 ) − 2
[yi − (β0 + β1 xi )]2 .
2
2σ i=1

En annulant les dérivées partielles successivement par rapport β0 , β1 et σ 2 on
obtient les équations de vraisemblance :
⎧ n
[yi − (β0 + β1 xi )] = 0
⎪
⎪
⎨ i=1
n
i=1 xi [yi − (β0 + β1 xi )] = 0
⎪
⎪
n
⎩ −n
1
2
i=1 [yi − (β0 + β1 xi )] = 0 .
2σ 2 + 2σ 4
Les deux premières équations ne dépendent pas de σ 2 et, étant linéaires en β0
et β1 , peuvent être résolues. De la première équation on déduit β0 = y − β1 x,
puis en remplaçant dans la deuxième :
β1 (

n

i=1

x2i − x

n

i=1

xi ) =

n

i=1

xi y i − y

n


xi .

i=1

n
n
n
n
Or i=1 x2i − x i=1 xi = i=1 x2i − nx2 = i=1 (xi − x)2 selon la formule
bien connue
descriptive et, de la même
n
n de la statistique
n de centrage-décentrage
n
façon, i=1 xi yi − y i=1 xi = i=1 xi yi − n x y = i=1 (xi − x)(yi − y). D’où
ﬁnalement, en substituant les Yi aux yi , les estimateurs du MV de β0 et β1 :
⎧
β0 = Y − β1 x
⎪
⎪
⎨
n
(x − x)(Yi − Y ) .
1 = i=1
⎪
n i
β
⎪
2
⎩
i=1 (xi − x)
Il est intéressant de noter que les deux premières
n équations de vraisemblance correspondent à la minimisation du terme i=1 [yi − (β0 + β1 xi )]2 dans
l’expression de la log-vraisemblance. Pour une solution quelconque (β0∗ , β1∗ ), la

Statistique − La théorie et ses applications

294

diﬀérence yi − (β0∗ + β1∗ xi ) est appelée résidu car elle correspond à l’écart entre
la valeur observée et celle donnée par le modèle ainsi estimé. On voit donc que
le couple (β0 , β1 ) est la solution qui minimise la somme des carrés des résidus.
De ce fait β0 et β1 sont aussi appelés estimateurs des moindres carrés.
Montrons que β0 et β1 sont respectivement sans biais pour β0 et β1 . On a :
1
1
E(Yi ) =
(β0 + β1 xi ) = β0 + β1 x
n i=1
n i=1
n

E(Y ) =

n

E(Yi − Y ) = β0 + β1 xi − (β0 + β1 x) = β1 (xi − x).
D’où :
E(β1 ) =

n

(xi − x)E(Yi −
i=1
n
2
i=1 (xi − x)

Y)

= β1 .

E(β0 ) = E(Y ) − E(β1 )x = β0 + β1 x − β1 x = β0 .
On en déduit que β0 + β1 x est sans biais pour E(Y |X = x), l’espérance de la
réponse pour la valeur x de la variable explicative.
Pour calculer les variances 
de ces estimateurs notons
 que le numérateur de
l’expression de β1 s’écrit aussi ni=1 (xi −x)Yi puisque ni=1 (xi −x) = 0. Donc :
n
n
2
σ 2 i=1 (xi − x)2
σ2
i=1 (xi − x) V (Yi )

= n
= n
.
V (β1 ) = n
2
2
2
( i=1 (xi − x)2 )
( i=1 (xi − x)2 )
i=1 (xi − x)
Pour β0 on a V (β0 ) = V (Y ) + x2 V (β1 ) − 2 x cov(Y , β1 ). Or :


n
1
cov Y , i=1 (xi − x)Yi
2
i=1 (xi − x)


n
1
= n
i=1 (xi − x)cov Y , Yi
2
i=1 (xi − x)

cov(Y , β1 ) = n





et, comme cov(Yj , Yi ) = 0 si j = i , cov Y , Yi = cov n1 Yi , Yi =
cov(Y , β1 ) =

n

σ2
n ,

on a :

n
σ2
i=1 (xi − x) = 0 ,
2
(x
−
x)
i=1 i

n

d’où :
σ2
σ2
= σ2
+ x2 n
V (β0 ) =
2
n
i=1 (xi − x)



x2
1
+ n
2
n
i=1 (xi − x)


.

De plus :
xσ 2
.
cov(β0 , β1 ) = cov(Y − β1 x , β1 ) = cov(Y , β1 ) − x V (β1 ) = − n
2
i=1 (xi − x)

Chapitre 11. Régressions linéaire, logistique et non paramétrique

295

 = (β0 , β1 )t nous résumons ces résultats (avec
En posant β = (β0 , β1 )t et β
les notations pour les vecteurs aléatoires introduites en section 3.8) par :


⎛
⎞
x2
xσ 2
1
2

−
+ n
σ
n
2
2 ⎟
⎜
n
i=1 (xi − x)
i=1 (xi − x)
⎟
) = β et V(β
) = ⎜
E(β
⎜
⎟.
2
2
⎝
⎠
xσ
σ
n
− n
2
2
i=1 (xi − x)
i=1 (xi − x)
Comme β0 et β1 sont chacun une combinaison linéaire des Yi , d’après le
 est gaussien. Sa loi est
théorème de caractérisation 3.1 le vecteur aléatoire β
donc parfaitement déﬁnie.
Déterminons maintenant l’estimateur du MV de σ 2 . Il se déduit de la
dernière équation de vraisemblance selon :
1
[Yi − (β0 + β1 xi )]2 ,
n
n

σ
2 =

i=1

expression dans laquelle on retrouve la somme des carrés des résidus (sous
sa forme aléatoire). Pour étudier la distribution d’échantillonnage de σ
2 on
admettra la proposition suivante.
n
Proposition 11.1 La v.a. σ12 i=1 [Yi − (β0 + β1 xi )]2 suit une loi du khi-deux
.
à n − 2 degrés de liberté. De plus elle est indépendante de l’estimateur β
Pour la première assertion cette proposition est à rapprocher du théorème
5.1 concernant la variance d’un échantillon, la démonstration faisant appel à
des considérations similaires. La perte de deux degrés de liberté s’explique par
le fait qu’il y a deux liaisons linéaires déterministes entre les Yi − (β0 + β1 xi )
correspondant aux deux premières équations de vraisemblance. L’indépendance
est à rapprocher de celle vue en proposition 5.3 entre moyenne et variance
empiriques.
n
De cette proposition nous déduisons que E( i=1 [Yi − (β0 + β1 xi )]2 ) =
2
(n − 2)σ 2 . L’espérance mathématique de σ
2 est donc n−2
n σ : cet estimateur est
biaisé. C’est pourquoi on lui préfère l’estimateur sans biais, obtenu en divisant
la somme des carrés des résidus par n − 2, que nous noterons S 2 , soit :
1 
[Yi − (β0 + β1 xi )]2 .
n − 2 i=1
n

S2 =

On montre que les estimateurs β0 , β1 et S 2 sont chacun UMVUE.

296

Statistique − La théorie et ses applications

Si lesxi sont choisis de telle sorte que pour tout n leur variance desn
2
criptive
i=1 (xi − x) /n admette une borne inférieure strictement positive
indépendante de n et leur moyenne x admette une borne supérieure également
indépendante de n, on voit immédiatement que V (β0 ) et V (β1 ) tendent vers
0 quand n → ∞. Alors β0 et β1 sont convergents en moyenne quadratique.
Sachant que la variance d’une v.a. de loi χ2 (n − 2) est 2(n − 2) on a V (S 2 ) =
2σ 4 /(n − 2) et S 2 converge aussi (vers σ2 ) en moyenne quadratique.

11.2.3

Intervalles de conﬁance

Pour β0 et β1 on dispose d’une quantité pivot de même type que celle utilisée
pour la moyenne d’une loi de Gauss en section 7.4.1. Pour β1 , par exemple, on
a:
β − β1
%1n
; N (0 ; 1)
2
σ/
i=1 (xi − x)
d’où, en estimant σ 2 par S 2 , on obtient comme pour le théorème 5.2 :
β − β1
%1n
; t(n − 2).
2
S/
i=1 (xi − x)

On posera S 2 (β1 ) = S 2 / ni=1 (xi − x)2 pour noter l’estimateur de la variance
de β1 . La variable aléatoire (β1 − β1 )/S(β1 ) est donc une fonction pivot qui
conduit immédiatement à l’intervalle de conﬁance suivant :
(n−2)
(n−2)
IC0,95 (β1 ) = [β1 − t0,975 s(β1 ) ; β1 + t0,975 s(β1 )]

où :

s(β1 ) = %n

s

i=1 (xi

− x)2

,

s désignant la réalisation de S (et β1 désignant indiﬀéremment l’estimation ou
l’estimateur de β1 ). De même on obtient :
(n−2)
(n−2)
IC0,95 (β0 ) = [β0 − t0,975 s(β0 ) ; β0 + t0,975 s(β0 )]
)
2
1
où s(β0 ) = s
+  n x(xi −x)2 .
n
i=1

Il est intéressant de construire un IC pour l’espérance de la réponse β0 +β1 x
au niveau x de la variable explicative. On a :
V (β0 + β1 x) = V (β0 ) + 2x cov(β0 , β1 ) + x2 V (β1 )


2xx
x2
x2
1
2
=σ
− n
+ n
+ n
2
2
2
n
i=1 (xi − x)
i=1 (xi − x)
i=1 (xi − x)


2
(x − x)
1
+ n
.
= σ2
2
n
i=1 (xi − x)

Chapitre 11. Régressions linéaire, logistique et non paramétrique

297

Par des développements tout à fait analogues aux précédents on obtient :
IC0,95 (β0 + β1 x) =
(n−2)
(n−2)
[β0 + β1 x − t0,975 s(β0 + β1 x) ; β0 + β1 x + t0,975 s(β0 + β1 x)]

où :


s(β0 + β1 x) = s

1
(x − x)2
.
+ n
2
n
i=1 (xi − x)

On constate que la largeur de l’IC est d’autant plus grande que l’on s’éloigne
de la valeur centrale x des valeurs ﬁxées pour la variable explicative.
On peut également établir (voir les exercices) un intervalle de prédiction
pour une observation au niveau x de la variable explicative.

11.2.4

Test H0 : β1 = 0

Ce test est essentiel car il décide de l’intérêt du modèle (ou de la «signiﬁcativité» de la variable explicative). En utilisant le résultat de la section précédente
on déduit que, sous H0 , β1 /S(β1 ) suit une loi de Student à n − 2 degrés de liberté. On rejettera donc H0 au niveau α si :
β1
(n−2)
(n−2)
∈
/ [−t1−α/2 , t1−α/2 ].

s(β1 )
Ce test est uniformément plus puissant parmi les tests sans biais. Comme pour
le test de Student usuel vu en section 9.7.1 sa puissance pour une vraie valeur
β1 se
( lit sur une loi de Student non centrale de paramètre de non centralité
β1 / V (β1 ). On ne peut obtenir qu’une valeur approchée de cette puissance
du fait que V (β1 ) doit être estimé.

Notons que si le test est accepté la loi conditionnelle de Y sachant X = x
ne dépend pas de x : les v.a. Yi sont toutes i.i.d. de loi N (β 0 , σ 2 ) et l’on se
retrouve dans la situation classique d’un échantillon issu d’une loi de Gauss.
Approche par l’analyse de variance3
On peut préférer aborder ce test par la voie de la relation de décomposition
de la somme des carrés totale. En eﬀet cette relation et la formulation du test
qui s’ensuit ont une validité générale pour tout type de modèle linéaire (en
3 Ne pas confondre avec le modèle d’analyse de variance qui concerne des variables explicatives catégorielles.

298

Statistique − La théorie et ses applications

particulier, on indiquera cela pour la régression multiple en section 11.2.7).
Posons, pour simpliﬁer les développements, yi = β0 + β1 xi . La relation est :
n


(yi − y) =
2

i=1

n


(
yi − y) +
2

n


i=1

(yi − yi )2 .

i=1

Sa démonstration est proposée dans les exercices. Le premier terme est appelé
somme des carrés totale car il exprime la variabilité des yi indépendamment
de tout modèle explicatif. Le deuxième terme se nomme somme des carrés expliquée par le modèle du fait qu’il ne prend en compte que les valeurs modélisées
dont il rend compte de la variabilité (on vériﬁera sans peine que la moyenne
des yi est égale à y). Le troisième terme est la somme des carrés des résidus.
Démontrons la proposition suivante
n concernant la somme des carrés expliqués
en tant que variable aléatoire : i=1 (Yi − Y )2 .
Proposition 11.2 Sous l’hypothèse H0 : β1 = 0, on a :
n 
2
i=1 (Yi − Y )
; χ2 (1).
2
σ
Démonstration : en vertu de la relation de décomposition exprimée en termes
aléatoires on peut écrire :
n


(Yi − Y )2 =

i=1

n


(Yi − Y )2 −

i=1

n


(Yi − β0 − β1 xi )2 ,

i=1

soit, en substituant β0 = Y − β1 x :
n


n


(Yi − Y )2 =

i=1

(Yi − Y )2 −

i=1

= 2β1

n +


,2

(Yi − Y ) − β1 (xi − x)

i=1
n


(xi − x)(Yi − Y ) − β12

i=1

n


(xi − x)2 .

i=1

De l’expression de β1 en section 11.2.2 on déduit :
n
n


(xi − x)(Yi − Y ) = β1
(xi − x)2
i=1

i=1

d’où, en substituant ce terme :
n
n


2
2


(Yi − Y ) = β1
(xi − x)2 .
i=1

i=1

%n
2
2
Or, sous l’hypothèse β1 = 0, β1
i=1 (xi − x) /σ suit une loi N (0 ; 1) et

son carré suit une loi χ2 (1), ce qui prouve la proposition.

Chapitre 11. Régressions linéaire, logistique et non paramétrique

299

n
Selon la proposition 11.1, σ12 i=1 (Yi − Yi )2 est indépendant de β1 et donc
n
n
de σ12 i=1 (Yi − Y )2 = σ12 β12 i=1 (xi − x)2 . Sous H0 ces deux v.a. suivent,
respectivement, des lois χ2 (n − 2) et χ2 (1). Le rapport de la seconde à la
première, après division par leurs degrés de liberté, suit donc une loi de Fisher
F (1, n − 2). Le paramètre σ 2 disparaissant et la somme des carrés des résidus
s’écrivant (n − 2)S 2 , ce rapport est :
n
F =



i=1 (Yi −
S2

Y )2

.

Or, de façon générale, E(S 2 ) = σ 2 et :
n
n


(xi − x)2
E( (Yi − Y )2 ) = E(β12 )
i=1



i=1

n
6
72  


= V (β1 ) + E(β1 )
(xi − x)2
i=1

= σ2 + β12

n


(xi − x)2 .

i=1

On voit que si β1 = 0 la somme des carrés expliquée tendra à être supérieure
à S 2 et F peut être envisagée comme statistique de test incitant à rejeter H0
pour des valeurs trop élevées sur la loi F (1, n−2). En eﬀet il n’y a pas lieu de
rejeter H0 lorsque F est faible puisque cela abonde dans le sens de l’hypothèse
β1 = 0. Notons que ce test est équivalent au test de Student présenté initialement car, d’une part, la statistique F est le carré de la statistique β1 /S(β1 ) et,
d’autre part (voir en ﬁn de section 5.5), le carré d’une v.a. de loi t(n − 2) est
une v.a. de loi F (1, n − 2).
L’usage veut que l’on présente les résultats menant à cette statistique sous
forme d’un tableau d’analyse de variance (voir le tableau de l’exemple 11.1 ciaprès), cette appellation venant du fait que l’on y met en évidence les termes
de la décomposition de la variabilité totale à travers les sommes de carrés.

11.2.5

Cas non gaussien

Le couple (β0 , β1 ) n’est plus estimateur du MV
 mais demeure l’estimateur
des moindres carrés de (β0 , β1 ) car minimisant ni=1 [yi − (β0 + β1 xi )]2 . Par
analogie avec l’estimation de la moyenne d’une
n loi à partir de la moyenne
empirique x qui est la valeur de a minimisant i=1 (xi − a)2 , l’estimation de la
droite de régression y = β0 +β1 x par la droite des moindres carrés y = β0 + β1 xi
semble être assez naturelle. Ceci est corroboré par le théorème suivant qui
s’applique à toute la modélisation linéaire et y justiﬁe la prééminence de la
méthode des moindres carrés.

300

Statistique − La théorie et ses applications

Théorème 11.1 (Gauss-Markov) Les estimateurs des moindres carrés β0 et
β1 sont, respectivement, estimateurs de variance minimale pour β0 et β1 parmi
les estimateurs sans biais fonctions linéaires des Yi .
La démonstration est proposée dans la section des exercices. En réalité cette
proposition s’étend à toute combinaison linéaire de β0 et β1 , et en particulier,
pour tout x, β0 + β1 x est estimateur de variance minimale de β0 +β1 x parmi les
estimateurs linéaires sans biais. Bien que la classe où β0 et β1 sont optimaux soit
plus réduite qu’avec l’hypothèse gaussienne, la propriété n’en reste pas moins
intéressante, d’autant plus qu’elle n’exige aucune hypothèse sur le type de loi
des erreurs (hormis bien sûr l’existence de la variance qui est implicite). Notons
que les trois estimateurs β0 , β1 et S 2 restent sans biais car les démonstrations
précédentes ne recouraient pas à la nature gaussienne des Yi . Pour β0 et β1 la
convergence en moyenne quadratique est assurée dans les mêmes conditions que
ci-dessus. Quant à celle de S 2 elle n’exige que l’existence du moment d’ordre 4
de la loi des erreurs.
Les tests et intervalles de conﬁance sont étonnamment robustes, le théorème
central limite agissant indirectement. Toutefois cette robustesse connaı̂t des limitations de même nature que pour l’inférence sur les moyennes de lois. En
premier lieu la loi ne doit pas produire de valeurs extrêmes (i.e. pas de queues
de distribution trop allongées). Par ailleurs la condition de variance constante
(«homoscédasticité») ne peut être assouplie que dans une faible mesure. On
peut éventuellement recourir à une transformation de Y pour stabiliser la variance (par exemple par la fonction Arcsin si Y est une proportion, par sa racine
carrée si c’est un comptage de type Poisson).

11.2.6

Régression et corrélation linéaires

2
Considérons un couple de v.a. (X, Y ) et adoptons les notations μX , μY, σX
,
et ρ pour leurs moyennes, leurs variances et leur coeﬃcient de corrélation
linéaire. Supposons que la fonction de régression E(Y |X = x) soit linéaire. On
montre alors (voir exercices) qu’elle est nécessairement de la forme :

σY2

E(Y |X = x) = μY + ρ

σY
(x − μX ).
σX

Y
Donc on a, dans les notations précédentes, β1 = ρ σσX
. On peut constater
immédiatement que la même relation vaut pour les estimateurs correspondants,
Y
où R est la corrélation linéaire empirique déﬁnie à la ﬁn de la
i.e. β1 = R SSX
section 5.2. On en déduit que R est l’estimateur du maximum de vraisemblance
de ρ. En eﬀet :
SX
S'X
R = β1
= β1
SY
S'Y

Chapitre 11. Régressions linéaire, logistique et non paramétrique

301

2
et β1 , S'X , S'Y sont les estimateurs du MV respectifs de β1 , σX , σY (S'X
et S'Y2
2
2
désignent les variances empiriques, SX et SY les variances d’échantillon, voir
déﬁnitions 5.4 et 5.5).

Comme σY et σX sont strictement positifs, β1 = 0 si et seulement si ρ = 0.
En particulier les hypothèses H0 : β1 = 0 et H0 : ρ = 0 sont équivalentes.
Par conséquent, si l’on dispose d’un test pour l’une des hypothèses il vaut
pour l’autre. Nous pouvons appliquer cela au cas d’un vecteur gaussien car il
est facile de montrer que la fonction de régression est linéaire (voir exercices).
Distinguons les deux situations de recueil des données, à savoir avec un plan
d’expérience (où les valeurs de X sont ﬁxées a priori) ou avec des observations
de X elles-mêmes aléatoires. Dans le premier cas le test H0 : β1 = 0 de la
section 11.2.4 peut être considéré également comme un test de non corrélation
entre les deux variables aléatoires X et Y. Nous examinons maintenant plus en
détail la deuxième situation.
Soit un échantillon de taille n : (X1, Y1 ), (X2 , Y2 ), · · · , (Xn , Yn ). Nous avons
vu en section 9.7.7 un test de √
H0 : ρ = √0 fondé sur un tel échantillon. Ce
test reposait sur la statistique n − 2R/ 1 − R2 qui, sous H0 , suit une loi
t(n − 2). Nous avons admis ce résultat que nous sommes maintenant en mesure de démontrer. Notons X = (X1 , X2 , . . . , Xn ) et x = (x1 , x2 , . . . , xn ) une
réalisation de X. Considérons la statistique F vue plus haut (section 11.2.4)
dans laquelle on remplace les xi par les v.a. Xi . Les développements précédents
indiquent que la loi de F conditionnellement à X = x est, sous H0 , la loi
F (1, n − 2). Cette loi ne dépend pas de x ce qui signiﬁe que, sous H0, la
statistique F est indépendante de X. La loi non conditionnelle de F (ou loi
marginale) est donc aussi la loi F (1, n − 2). Ainsi F calculé à partir des couples
(Xi , Yi ) peut être utilisé comme statistique de test pour H0 : ρ = 0. Pour ce
qui concerne la mise en oeuvre ce test ne se distingue donc pas de celui vu plus
haut lorsque les xi sont ﬁxés. Montrons qu’on a aﬀaire au même test que celui
proposé en section 9.7.7 et, pour simpliﬁer, raisonnons sur les réalisations.
D’une relation établie lors de la démonstration de la proposition 11.2, on
déduit :
n
2
[ i=1 (xi − x)(yi − y)]
2


r = n
n
2
2
i=1 (xi − x)
i=1 (yi − y)

n
(xi − x)2
β2
= 1n i=1
(yi − y)2
n i=1
(
yi − y)2
= i=1
.
n
2
i=1 (yi − y)
En raison de la décomposition de la somme des carrés totale on voit que le
rapport de la somme des carrés des résidus à cette dernière est égal à 1 − r2
(r2 est appelé coeﬃcient de détermination, voir section 9.7.7). Donc F (réalisé)
peut s’écrire :

Statistique − La théorie et ses applications

302

(n − 2)r2
.
1 − r2
Sa racine carrée est la réalisation de la statistique de Student (en valeur absolue)
de la section 9.7.7. Or une v.a. de loi t(n − 2) élevée au carré est une v.a. de
loi F (1, n − 2) ce qui prouve que les deux tests sont identiques.
F =

Il est intéressant de noter que la loi non conditionnelle de F est une loi
F (1, n − 2), sous H0 , quelle que soit la loi marginale de X. Pour que le test
s’applique rigoureusement il suﬃt donc que, pour tout x, la loi conditionnelle
de Y sachant X = x soit gaussienne de variance σ 2 indépendante de x et que
la fonction de régression de Y sur X soit linéaire.
Nous venons de voir que le test usuel H0 : β1 = 0 est utilisable même si les
xi sont des réalisations de v.a. Xi . On montre que les autres résultats établis
conditionnellement aux xi ﬁxés restent également valables à condition que la loi
marginale de X ne dépende pas des paramètres β0 et β1 déﬁnissant la fonction
de régression de Y sur X.
Exemple 11.1 Pour une enquête on a eu recours à 54 enquêteurs. Pour chacun
d’entre eux on dispose du nombre d’entretiens qu’il a eﬀectués et de la durée
médiane de ceux-ci4 . On cherche à vériﬁer si le nombre d’entretiens eﬀectués X
est un facteur explicatif de la durée de l’entretien Y . On a calculé initialement :
x = 53 ; y = 30,535 ;

54


x2i

= 4274,8 ;

i=1

54


yi2

= 957,23 ;

i=1

54


xi yi = 1531,7 .

i=1

On en déduit les estimations suivantes :
β0 = 33,668
s2 (β0 ) = 1,1057

;
;

β1 = −0,05911 ; s2 = 20,473
s2 (β1 ) = 0,0002589 ; cov(
8 β0 , β1 ) = −0,01371.

Pour le(
test de l’hypothèse H0 : β1 = 0, la statistique de test prend la valeur

t = β1 / s2 (β1 )= -3,68 ce qui correspond, pour la loi de Student t(52), à une Pvaleur de l’ordre de 0,001. Le nombre d’entretiens eﬀectués est donc un facteur
explicatif très signiﬁcatif de la durée médiane de ces entretiens. Le même test
peut être conduit à partir du tableau d’analyse de variance ci-après.
Source
Expliquée
Résiduelle
Totale

Somme des Carrés
276,53
1 064,59
1 341,12

ddl
1
52
53

Carrés Moyens
276,53
20,47

F
13,51

P-valeur
0,001

Pour un niveau x = 50 entretiens on obtient une estimation de l’espérance
de la durée médiane des entretiens égale à :
β0 + β1 .50 = 30,713
4 Source

: Centre d’Etudes des Supports de Publicité, Paris.

Chapitre 11. Régressions linéaire, logistique et non paramétrique

303

et un intervalle de conﬁance de niveau 0,95 associé :
(52)

(52)

IC0,95 (β0 + β1 .50) = [30,713 − t0,975 .0,618 ; 30,713 + t0,975 .0,618]
(52)

soit, avec t0,975 = 2,007, l’intervalle [29,47 ; 31,95]. La ﬁgure 11.1 indique les
limites de conﬁance en fonction du niveau de x ainsi que les limites de prédiction
(plus larges) décrites dans les exercices. Au vu de la dispersion des points autour
de la droite de régression la condition de variance constante est plausible. 

Figure 11.1 - Données «enquêteurs» : droite de régression, limites de conﬁance,
limites de prédiction.

11.2.7

Extension à la régression multiple

Dans cette section nous montrons succinctement que l’étude du modèle
de régression simple s’étend sans diﬃcultés en présence de plusieurs variables
explicatives. Dans le formalisme du modèle conditionnel présenté en section
11.1 on considère p prédicteurs X1 , X2 , . . . , Xp et une fonction de régression de

Statistique − La théorie et ses applications

304

Y de la forme :
E(Y | X1 = x1 , X2 = x2 , . . . , Xp = xp ) =β0 + β1 x1 + · · · + βp xp
pour les niveaux respectifs x1 , x2 , . . . , xp de ces prédicteurs. Les autres hypothèses restent identiques : lois conditionnelles de Y gaussiennes et de même
variance σ 2 . Soit une série d’observations indépendantes Y1 , Y2 , . . . , Yn où Yi est
observé pour les valeurs xi1 , xi2 , . . . , xip des variables explicatives. On recourt
alors à l’écriture matricielle suivante. On déﬁnit le vecteur des observations
Y = (Y1 , Y2 , . . . , Yn )t , puis le vecteur des p + 1 paramètres inconnus
β = (β0 , β1 , . . . , βp )t et la n × (p + 1)-matrice du plan d’expérience X dont
la i-ème ligne est (1, xi1 , xi2 , . . . , xip ). On a alors, avec les notations de la section 3.8 :
E(Y) = Xβ
β , V(Y) = σ 2 In
où In désigne la matrice identité d’ordre n.
La log-vraisemblance s’écrit comme en section 11.2.2 en remplaçant β0 +
β1 xi par β0 + β1 xi1 + · · · + βp xip . Les équations de vraisemblance obtenues
en dérivant par rapport à chacun des p + 1 paramètres forment un système
t
β = Xt Y. On
linéaire de p + 1 équations dont l’écriture matricielle est (X X)β
suppose que les vecteurs colonnes de X sont linéairement indépendants (i.e.
n > p et pas de redondance d’information dans les vecteurs prédicteurs ni de
combinaison linéaire entre eux donnant un vecteur constant, ce qui signiﬁerait
une surparamétrisation du modèle). Alors la matrice Xt X est inversible et on
a la solution unique :
 = (Xt X)−1 Xt Y .
β
 est également le vecteur des estimateurs des moindres carrés, c’est-à-dire tel
β
que :

9
92
9
2
9
β
9Y − Xβ
9 = min Y − Xβ
β
2

où . représente la norme euclidienne usuelle d’un vecteur de Rn . D’après la
proposition 3.13 on a :
) = (X X) Xt E(Y) = (X X) Xt Xβ
E(β
β =β
) = (Xt X)−1 Xt V(Y) X(Xt X)−1 = σ 2 (Xt X)−1 .
V(β
t

−1

t

−1

t
−1
 ; Np+1 (β
Sachant que β
β , σ 2 (X X) ) on peut procéder à tout type d’inférence concernant les paramètres du modèle. Il est notamment intéressant de
tester des hypothèses du type H0 : βk = 0 permettant (pour k ≥ 1) de décider
de la pertinence de tel prédicteur particulier en présence des autres prédicteurs. La statistique de test est analogue à celle de la régression simple pour

Chapitre 11. Régressions linéaire, logistique et non paramétrique

305

H0 : β1 = 0 à cette diﬀérence près que le nombre de degrés de liberté de la loi
de Student devient égal à n − (p + 1). Ceci découle du fait que la somme des
carrés des résidus ne comporte plus que n − (p + 1) degrés de liberté. Compte
tenu de cette modiﬁcation on peut construire un tableau d’analyse de variance
semblable à celui de la régression simple. La statistique F permet alors de
tester l’hypothèse globale H0 : β1 = β2 = · · · = βp = 0, les valeurs critiques se
rapportant à la loi F (p, n − (p + 1)).
Au-delà du modèle de régression qui stipule l’existence d’une fonction de
régression, un modèle linéaire dans sa forme la plus générale se déﬁnit comme
un vecteur d’observations Y tel que E(Y) = Xβ
β et V(Y) = σ 2 In comme
ci-dessus. Ceci inclut notamment les modèles d’analyse de variance où les variables explicatives (appelées alors facteurs) sont catégorielles ce qui conduit à
introduire dans la matrice X des variables indicatrices des diﬀérentes catégories
induites par ces facteurs.
Les modèles linéaires généralisés constituent un vaste ensemble d’extensions du modèle de régression multiple où, d’une part, les Yi répondent à
d’autres types de lois paramétriques que la loi de Gauss et, d’autre part, la
fonction de régression s’exprime sous la forme g(β0 + β1 x1 + · · · + βp xp ) où
g est une fonction connue. Le modèle logistique présenté ci-après en oﬀre une
illustration particulièrement importante.
Pour un traitement plus complet de la régression linéaire on pourra consulter, pour les aspects mathématiques, l’ouvrage classique de Seber (1977) et,
pour les aspects pratiques, le livre de Dodge (1999).

11.3

La régression logistique

11.3.1

Le modèle

Ce modèle est adapté au cas où la variable à expliquer est binaire. En
utilisant le codage 1/0 on la transforme en variable aléatoire de Bernoulli. Plus
précisément, dans le formalisme conditionnel exposé en section 11.1, la loi de
Y sachant X = x est une loi B(p(x)). La fonction de régression à estimer est
donc :
E(Y |X = x) = p(x)
où p(x) = P (Y = 1|X = x). Plus prosaı̈quement, le problème est de déterminer
comment la probabilité de «succès» évolue en fonction du niveau de la variable
X. Par exemple : quelle est la probabilité que le client d’une banque détienne
des valeurs mobilières, en fonction de son niveau de revenu ?
Nous ne sommes donc plus dans le cadre précédent et le modèle de régression
linéaire usuel n’est, en principe, pas approprié. Nous disons «en principe» car
ce modèle est très robuste vis-à-vis de l’hypothèse gaussienne dans la mesure où

Statistique − La théorie et ses applications

306

l’on a un nombre suﬃsant d’observations, au même titre que l’approximation
d’une loi binomiale par une loi de Gauss. Mais le modèle linéaire pose problème
pour une raison majeure : la fonction p(x) n’y est pas contrainte dans l’intervalle [0, 1] et les estimations peuvent donc produire des valeurs négatives ou
supérieures à 1. Le modèle logistique remédie à cela.
Ce modèle stipule que la probabilité conditionnelle de succès est de la forme :
p(x) =

eβ0 +β1 x
.
1 + eβ0 +β1 x

La fonction :

eu
1 + eu
est appelée fonction logistique, elle est strictement croissante et prend ses valeurs dans l’intervalle [0,1] (voir ﬁgure 11.2). Sa fonction inverse est :
u
g −1 (u) = ln
1−u
g(u) =

p
et s’appelle fonction logit 5 . Pour une loi de Bernoulli B(p) le rapport 1−p
a
une certaine signiﬁcation. On l’appelle parfois la chance ou la cote de succès
(en anglais : odds). Dans le modèle logistique le logarithme de ce rapport est
donc une fonction linéaire de la variable explicative :

ln

p(x)
= β0 + β1 x.
1 − p(x)

Le modèle comporte donc deux paramètres inconnus β0 et β1. On notera par
β le couple (β0 , β1 ) ou, indiﬀéremment, le vecteur (β0 , β1 )t . Contrairement à la
régression classique il n’y a pas de variance de l’erreur à estimer puisqu’une loi
de Bernoulli B(p(x)) ne dépend que du paramètre p(x).

11.3.2

Estimation de la fonction p(x)

Supposons que nous observions indépendamment les v.a. binaires Y1 , Y2 , · · · ,
Yn aux points x1 , x2 , · · · , xn de la variable explicative et déterminons l’estimateur du maximum de vraisemblance de β . Pour tout i, Yi ; B(p(xi )) et la
fonction de probabilité de Yi est (voir section 4.1.2) :
p(y) = p(xi )y (1 − p(xi ))1−y , y ∈ {0, 1} .
La fonction de vraisemblance de β associée à une réalisation (y1 , y2 , · · · , yn ) de
(Y1 , Y2 , · · · , Yn ) est donc :
L(β
β) =

n
$

p(xi )yi (1 − p(xi ))1−yi

i=1
5 Ceci

amène une confusion entre modèle logit et modèle logistique. L’usage le plus répandu
est de parler de régression logistique lorsque, comme ici, la (ou les) variable explicative est
quantitative et de modèle logit lorsqu’elle (ou elles) est catégorielle.

Chapitre 11. Régressions linéaire, logistique et non paramétrique

g (u ) =

1

307

eu
1 + eu

0,5

u

0
-6

-4

-2

0

2

4

6

Figure 11.2 - Fonction logistique.

avec :
p(xi ) =

exp(β0 + β1 xi )
.
1 + exp(β0 + β1 xi )

La log-vraisemblance est égale à :
ln L(β
β) =

n


{yi ln p(xi ) + (1 − yi ) ln[1 − p(xi )]} .

i=1

Les deux équations de vraisemblance sont établies en dérivant cette fonction
par rapport à β0 et par rapport à β1 . Dans un premier temps considérons la
dérivée de la fonction logistique g(u) :
g  (u) =

eu
eu
1
=
= g(u)[1 − g(u)].
u
2
u
(1 + e )
1 + e 1 + eu

Ainsi :
∂
p(xi ) = p(xi )[1 − p(xi )]
∂β0
∂
p(xi ) = xi p(xi )[1 − p(xi )].
∂β1
La dérivée du i-ème terme de la log-vraisemblance par rapport à β0 est donc :
yi

p(xi )[1 − p(xi )]
p(xi )[1 − p(xi )]
− (1 − yi )
= yi − p(xi )
p(xi )
1 − p(xi )

308

Statistique − La théorie et ses applications

et, de même, celle par rapport à β1 est xi [yi − p(xi )]. D’où les équations de
vraisemblance :
⎧
n

∂
⎪
⎪
ln
L(β
β
)
=
{yi − p(xi )} = 0
⎪
⎪
⎨ ∂β0
i=1
⎪
⎪
⎪
⎪
⎩


∂
ln L(β
β) =
{xi [yi − p(xi )]} = 0
∂β1
i=1
n

ou, faisant apparaı̂tre β0 et β1 :
⎧ 
n
n

exp(β0 + β1 xi )
⎪
⎪
=
yi
⎪
⎪
⎨
1 + exp(β0 + β1 xi )
i=1

n
⎪

⎪
⎪
⎪
xi
⎩
i=1

i=1


exp(β0 + β1 xi )
=
xi y i
1 + exp(β0 + β1 xi )
i=1
n

.

Ces équations n’ont pas de solution explicite et paraissent complexes. Toutefois
elles ne posent pas de diﬃcultés pour être résolues de façon itérative pour don = (β0 , β1 ). On en déduit l’estimateur de la fonction
ner l’estimateur du MV β
de régression en x quelconque :


p(x) =



eβ0 +β1 x
1 + eβ0 +β1 x

.

Matrice des variances-covariances de β

11.3.3

En approximation on utilise les propriétés asymptotiques de l’estimateur du
MV. Soit I(β
β ) la 2×2-matrice d’information de Fisher de β , on a alors (voir sec)  [I(β
tion 6.7.4) V(β
β )]−1 . Explicitons la matrice I(β
β ). En posant f (y; β ) pour
la fonction de probabilité conjointe du vecteur aléatoire Y = (Y1 , Y2 , · · · , Yn )
au point y = (y1 , y2 , · · · , yn ), on a (voir section 6.6.4) :


 ⎞
⎛
 2
∂2
∂
ln
f
(Y;
β
)
−E
ln
f
(Y;
β
)
−E
⎟
⎜
2
∂β0 ∂β1
0


 ⎟.
 ∂ β
I(β
β) = ⎜
2
2
⎠
⎝
∂
∂
ln f (Y; β )
−E
ln f (Y; β )
−E
2
∂β0 ∂β1
∂ β1
Or ln f (y; β ) n’est autre que la log-vraisemblance vue ci-dessus dont il faut
calculer les dérivées partielles secondes, soit :
n
n
n


∂ 
∂2
∂
ln
L(β
β
)
=
[y
−
p(x
)]
=
−
p(x
)
=
−
p(xi )[1 − p(xi )].
i
i
i
∂ 2 β0
∂β0
∂β0
i=1

i=1

i=1

De même :
n
n

∂ 
∂2
ln
L(β
β
)
=
x
[y
−
p(x
)]
=
−
x2i p(xi )[1 − p(xi )]
i i
i
∂ 2 β1
∂β1 i=1
i=1

Chapitre 11. Régressions linéaire, logistique et non paramétrique

309

et :
n
n

∂ 
∂2
ln L(β
β) =
[yi − p(xi )] = − xi p(xi )[1 − p(xi )] .
∂β0 ∂β1
∂β1
i=1

i=1

Ces dérivées secondes ne dépendant plus des yi elles sont inchangées quand on
passe aux espérances mathématiques6 , d’où :
⎛

n


p(xi )[1 − p(xi )]

⎜
⎜ i=1
I(β
β) = ⎜ 
n
⎝
xi p(xi )[1 − p(xi )]
i=1

⎞
xi p(xi )[1 − p(xi )] ⎟
⎟
i=1
⎟.
n

⎠
2
xi p(xi )[1 − p(xi )

n


i=1

), c’est-à-dire substituer p(xi )
Comme β est inconnu il faut estimer I(β
β ) par I(β
) on obtient une
β ). En inversant I(β
à p(xi ) dans l’expression ci-dessus de I(β

estimation de V(β ) que nous notons :

s2 (β0 )
s2 (β0 , β1 )


V(β ) =
s2 (β1 )
s2 (β0 , β1 )
où s2 (β0 ) est une estimation de la variance de β0 , s2 (β1 ) est une estimation de
la variance de β1 et s2 (β0 , β1 ) est une estimation de la covariance entre β0 et
β1 .
Grâce à ces estimations on peut obtenir des intervalles de conﬁance et eﬀectuer le test essentiel H0 : β1 = 0 pour décider de la signiﬁcativité de la variable
explicative.

11.3.4

Test H0 : β1 = 0

En raison de la normalité asymptotique du maximum de vraisemblance,
sous H0 la statistique β1 /s(β1 ) suit approximativement une loi N (0 ; 1) et on
rejettera l’hypothèse de nullité au niveau 0,05 si sa réalisation n’est pas comprise
dans l’intervalle ±1,96. Ce test est le test de Wald donné dans les logiciels.
Parfois ce test est présenté avec le carré de la statistique ci-dessus dont la
valeur critique doit alors être lue sur une loi χ2 (1).
On peut également envisager le test du rapport de vraisemblance généralisé
fondé sur la déviance :
+
,
 ) − ln L(β
)
−2 ln L(β
H0
6 Si, comme généralement dans les modèles complexes, les dérivées secondes avaient été
fonction des yi on aurait été contraint d’estimer les espérances en prenant les expressions des
dérivées secondes telles qu’elles apparaı̂traient ci-dessus.

Statistique − La théorie et ses applications

310


où β
β ) sous l’hyH0 est la valeur de β maximisant la log-vraisemblance ln L(β
pothèse H0 , c’est-à-dire avec :
p(xi ) =

exp β0
= p0 .
1 + exp β0

n
Seule subsiste alors la première
équation de vraisemblance i=1 (yi − p0 ) = 0

n
dont la solution est p0 = n1 i=1 yi , la proportion de succès observée. Cette
solution est naturelle puisque sous H0 les Yi sont des variables aléatoires de
même moyenne et donc i.i.d.. On en déduit :
 = ln
β
H0

p0
1 − p0

qui permet de calculer la déviance ci-dessus. Celle-ci suit approximativement
une loi χ2 (1) car H0 ne spéciﬁe qu’un seul paramètre. Ce test donne des
décisions généralement en accord avec celles du test de Wald. Notons qu’il
existe un autre test, appelé test du score, qui est encore plus proche du test du
RV.

11.3.5

Intervalles de conﬁance

 on peut utiliser le
Toujours en vertu de la normalité asymptotique de β


calcul de la matrice V(β ) pour établir un IC sur chaque composante de β . Par
exemple, pour β0 , on a :
IC0,95 (β0 )  [β0 − 1, 96 s(β0 ) ; β0 + 1, 96 s(β0 )].
Toutefois l’intervalle de conﬁance qui nous intéresse le plus concerne la
proportion de succès p(x) pour une valeur donnée x de la valeur explicative.
Considérons tout d’abord un IC sur β0 + β1 x. Asymptotiquement, son estimateur du MV β0 + β1 x est gaussien et de variance :
V (β0 + β1 x) = V (β0 ) + 2x cov(β0 , β1 ) + x2 V (β1 )
que l’on estime par :
s2 (β0 + β1 x) = s2 (β0 ) + 2x s2 (β0 , β1 ) + x2 s2 (β1 ).
D’où l’IC approché :
IC0,95 (β0 + β1 x)  [β0 + β1 x − 1,96 s(β0 + β1 x); β0 + β1 x + 1,96 s(β0 + β1 x)] .
De cet intervalle on déduit celui sur p(x) en appliquant aux deux bornes la
eu
fonction croissante g(u) =
.
1 + eu
Cette procédure est la procédure duale (voir section 9.8) du test de Wald.

Chapitre 11. Régressions linéaire, logistique et non paramétrique

311

Exemple 11.2 Lors d’une enquête de santé publique 307 individus d’âges variant entre 18 et 85 ans ont été étudiés7 . Parmi ceux-ci 133 souﬀraient d’une
maladie chronique. Sachant que la proportion de personnes ayant une maladie
chronique augmente avec l’âge, on envisage un modèle logistique pour estimer la probabilité d’un tel type d’aﬀection en fonction de l’âge. La solution
des deux équations de vraisemblance obtenue par un logiciel mathématique ou
statistique est :
β0 = −2,284
β1 = 0,04468 .
La probabilité d’avoir une maladie chronique à l’âge x est donc estimée par :
p(x) = −2,284+0,04468 x .
En calculant les p(xi ) pour chacune des observations on déduit l’expression de
) qui est inversée pour donner :
la matrice I(β


0,1349
−0,2639 × 10−2
) =
 β
V(
.
−0,2639 × 10−2 0,5814 × 10−4
La réalisation
de la statistique de Wald pour tester H0 : β1 = 0 est égale à
%
0,04468/ 0,5814 × 10−4 = 5,86 ce qui correspond à une P-valeur pratiquement
nulle (2×10−9 ). L’âge est donc un facteur explicatif très signiﬁcatif pour la
présence d’une maladie chronique.
Pour le test du RVG on a :
 n

 )=
yi
ln L(β
H0


ln p0 +

i=1

n


(1 − yi ) ln(1 − p0 )

i=1

avec p0 = 133/307 = 0,4332. Soit :
 ) = 133 ln (0,4332) + 174 ln (0,5668)= − 210,1 .
ln L(β
H0
De même on calcule :
) =
ln L(β

n


{yi ln p(xi ) + (1 − yi ) ln[1 − p(xi )]}

i=1

= −190,4
pour obtenir ﬁnalement la valeur prise par la déviance :
+
,
 ) − ln L(β
) = 39,4
−2 ln L(β
H0
7 Echantillon non représentatif extrait de l’enquête Health Statistics 1990 auprès de 7200
personnes, eﬀectuée par Statistics Netherlands.

Statistique − La théorie et ses applications

312

qui donne aussi une P-valeur quasi nulle sur la loi χ2 (1). Notons que le carré
de la statistique de Wald réalisée est (5,86)2 = 34,3, ce qui est proche de la
valeur obtenue par le RVG.
Donnons un IC à 95% pour β1 . On a :
%
%
IC0,95 (β1 ) = [0,04468 − 1,96 0,5814 × 10−4 ; 0,04468 + 1,96 0,5814 × 10−4 ]
= [0,02974 ; 0,05963].
Certains logiciels statistiques donnent un IC pour eβ1 , soit ici [1,030 ; 1,060].

Cette valeur, estimée ici par eβ1 = 1,046, a une signiﬁcation particulière.
Comme β1 correspond à l’accroissement du logit de p(x) quand x s’accroı̂t
d’une unité, eβ1 est le rapport des chances (odds ratio) d’une année à l’autre.
Voyons maintenant un IC sur la probabilité d’avoir une maladie chronique
à l’âge de 50 ans. On a β0 + β1 × 50 = −0,050 et p(50) = 0,488 . Puis :
s2 (β0 + β1 × 50) = 0,1349 + 2 × 50 × (−0,2639)2 + (50)2 × 0,5814 × 10−4
= 0,01635 .
√
L’intervalle de conﬁance pour β0 + β1 × 50 est donc −0,050±1,96 0,01635 soit
[−0,301 ; 0,201], d’où ﬁnalement :
IC0,95 (p(50)) = [0,425 ; 0,550].

Il est intéressant de constater qu’un modèle linéaire ajusté sur la variable
réponse 1/0 donne une estimation :
p'(x) = −0,0166 + 0,0101 x
qui se diﬀérencie très peu de celle du modèle logistique. En eﬀet pour x = 50 on
obtient une probabilité identique 0,488 et pour les âges extrêmes de 20 et 80 ans
on obtient respectivement 0,185 et 0,791 contre 0,199 et 0,784 pour l’approche
logistique. Ceci s’explique par le fait que la plage d’âges observée se situe dans
la partie centrale et quasi linéaire de la courbe logistique (voir ﬁgure 11.2). 

11.3.6

Remarques diverses

1. La régression logistique illustre l’intérêt de la méthode du maximum de
vraisemblance. Dans un modèle complexe il est peu probable de pouvoir
dégager des estimateurs optimaux des paramètres. Cette méthode garantit (moyennant des conditions faibles de régularité) des estimateurs à
faibles biais - et ceci d’autant plus que la taille d’échantillon est élevée
- dont on peut par ailleurs estimer les variances et covariances pour
construire des tests et intervalles de conﬁance approchés.

Chapitre 11. Régressions linéaire, logistique et non paramétrique

313

2. Le modèle logistique n’est évidemment pas la panacée. Une condition
nécessaire, mais non suﬃsante, pour qu’il s’applique est que la probabilité
soit de toute évidence une fonction monotone de la variable explicative.
Parmi d’autres possibilités signalons les modèles probit et gompit. Le
premier utilise à la place de la fonction logit g(u) la fonction de répartition
Φ(u) de la loi N (0 ; 1). Celle-ci restant toutefois très proche de g(u),
le modèle correspondant ne se diﬀérencie pratiquement pas du modèle
logistique. Le modèle gompit utilise la fonction h(u) = 1 − exp(− exp(u))
qui permet d’attribuer des probabilités plus fortes sur les extrêmes mais
n’est pas symétrique. A l’instar de l’exemple précédent, bien des situations
peuvent simplement être modélisées par une fonction linéaire. Même pour
une réponse binaire les tests et IC vus en section 11.2 fournissent des
résultats approchés corrects. Ceci conﬁrme la forte robustesse du modèle
linéaire vis-à-vis de l’hypothèse gaussienne.
3. Divers diagnostics et tests ont été proposés pour vériﬁer l’adéquation du
modèle (notamment le test de Hosmer et Lemeshow, 2000). Une façon
simple et rapide de vériﬁer si p(x) semble bien suivre l’allure voulue est
de grouper les données par classes (par exemple former des classes d’âge
dans l’exemple ci-dessus), de calculer dans chaque classe la proportion
de succès observée et de tracer de manière lisse la courbe passant par les
points dont les abscisses sont les milieux des classes et les ordonnées sont
les proportions correspondantes.
4. Comme pour la régression linéaire la régression logistique peut s’étendre
à une régression multiple (plusieurs variables explicatives). Les principes
de calculs sont une extension naturelle de ceux vus ci-dessus qui ne pose
pas de diﬃcultés spéciﬁques.
5. Si la variable explicative est catégorielle on peut appliquer la régression logistique en introduisant les variables indicatrices de chaque catégorie, sauf
pour l’une d’entre elles qui sert alors de catégorie de référence. Ceci est à
rapprocher de l’analyse de variance à un facteur du modèle linéaire. Toutefois, comme dans le cas linéaire, on a aﬀaire à des interprétations particulières qui relèvent d’une méthodologie propre. A cette ﬁn, et comme
il a été indiqué en note de bas de page dans la section 11.3.1, on préfère
parler de modèle logit lorsque la ou les variables explicatives sont toutes
catégorielles.

Pour approfondir le sujet de la régression avec réponse binaire on pourra
consulter les ouvrages : Droesbeke, Lejeune et Saporta (2004) ou, en anglais,
Agresti (2002) et Chap (1998).

314

Statistique − La théorie et ses applications

11.4

La régression non paramétrique

11.4.1

Introduction

Nous nous situons ici, comme en section 8.5, dans le cadre de l’estimation
fonctionnelle : la fonction de régression g(x) est totalement inconnue et est
l’objet même à estimer. Une telle approche peut se révéler utile si l’on n’a pas
d’idée précise sur une forme fonctionnelle adéquate ou si la forme de la fonction
est complexe et se prête mal à une modélisation par une forme paramétrique
simple.
Un avantage de la régression non paramétrique est de fournir une procédure
automatique d’ajustement quel que soit le type de données. Elle est à classer
parmi les méthodes dites adaptatives. On peut voir comme un inconvénient le
fait qu’elle ne livre pas un modèle sous forme de formule facilement réutilisable
pour la prévision, mais donne uniquement une description point par point de
la fonction. Toutefois on pourra concevoir la procédure comme une première
étape, sans aucune restriction, pour orienter ensuite la recherche d’une forme
paramétrique adaptée.
Nous présenterons la méthode des noyaux qui, plus que toute autre, est une
approche très intuitive du problème et qui sera en cohérence avec l’estimation
de densité ou de fonction de répartition exposée en section 8.5. Par ailleurs
la régression polynomiale locale, plus performante, que l’on étudiera en ﬁn de
section, en est un prolongement naturel.
Les méthodes usuelles ne concernent que les phénomènes où la variable
réponse varie en moyenne de façon lisse en fonction de la variable
explicative. De fait on fera l’hypothèse que la fonction de régression g(x) est
dérivable au moins à l’ordre 2. Comme pour les modèles paramétriques de ce
chapitre on supposera que la variance conditionnelle de la variable Y sachant
X = x est indépendante de x et égale à σ 2 .

11.4.2

Déﬁnition des estimateurs à noyaux

Les estimateurs à noyaux de régression (en anglais : kernel estimators) ont
été introduits simultanément par Nadaraya (1964) et Watson (1964) qui se
sont inspirés des développements accomplis dans le domaine de l’estimation de
densité. Ils reposent sur une idée très intuitive proche de celle, plus ancienne,
de moyenne mobile. Pour estimer g(x), où x est un niveau donné de la variable
explicative, à partir des réalisations (x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ), on prend la
moyenne des valeurs des yi pour l’ensemble des observations dont les niveaux

Chapitre 11. Régressions linéaire, logistique et non paramétrique

315

sont situés dans un voisinage (ou fenêtre) [x − h, x + h] autour de x, soit8 :
n


gn (x) =

yi I[x−h,x+h] (xi )

i=1
n


I[x−h,x+h] (xi )

i=1

où I[x−h,x+h] (xi ) est égal à 1 si xi ∈ [x − h, x + h] et 0 sinon. En introduisant
le noyau de Rosenblatt (voir section 8.5.2) : K(u) = 1/2 si −1 ≤ u ≤ 1 et 0
sinon, on peut écrire :
n


gn (x) =

i
yi K( x−x
h )

i=1
n


.
i
K( x−x
h )

i=1

Cette forme se prête à la généralisation à un noyau quelconque K introduisant
une moyenne pondérée des yi . Rappelons qu’une fonction K est un noyau si
elle est paire et si son intégration sur R donne 1. Dans le cas de la densité qui
reste positive ou nulle on a imposé également que K ne soit pas négative. Cette
condition est moins cruciale dans le cas de la régression (les moyennes mobiles
utilisées pour le lissage des séries chronologiques comprennent d’ailleurs des
coeﬃcients négatifs aﬁn de réduire le biais). Les propriétés de continuité et
dérivabilité se transférant à la fonction estimée on aura avantage, comme pour
la densité, à choisir un noyau de type biweight, par exemple, qui soit dérivable
aux bornes du support.

11.4.3

Biais et variance

Comme en régression linéaire nous distinguerons deux cas, selon que les xi
sont déterminés par un plan d’expérience ou qu’ils sont réalisations de variables
aléatoires Xi .

Cas des xi ﬁxés
L’estimateur gn (x) en un point x donné étant une fonction linéaire des Yi ,
i
le biais et la variance se calculent aisément. Posant wi = K( x−x
h ) on a :
8 La plupart des propriétés que nous expliciterons sont de nature asymptotique. C’est pourquoi nous indiçons l’estimation gn (x) par n comme pour la densité. De plus nous utiliserons
la même notation pour estimation et estimateur.

316

Statistique − La théorie et ses applications
n


E(
gn (x)) − g(x) =

wi (g(xi ) − g(x))

i=1

n


wi

i=1
n


wi2

i=1

V (
gn (x)) = σ 2
(

n


wi

.
)2

i=1

Pour ﬁxer les idées prenons le cas de valeurs xi espacées régulièrement sur
l’intervalle d’intérêt [a, b]. Si n est assez grand on peut approcher la somme
ﬁnie par une intégrale pour obtenir (voir Gasser et Müller, 1984) :

E(
gn (x)) − g(x) =
V (
gn (x)) =

1

−1

K(u)[g(x + uh) − g(x)]du + o(

b−a 2
σ
nh



1
−1

[K(u)]2 du + O(

1
)
nh

1
).
n2

Ces formules sont très semblables à celles de la densité et conduisent aux
mêmes conditions nécessaires pour la convergence en moyenne quadratique,
à savoir n → ∞, h → 0, nh → ∞. Pour préciser le comportement du biais
prenons le développement de Taylor :
1
g(x + uh) = g(x) + uhg  (x) + (uh)2 g  (x) + O(h3 )
2
d’où :
E(
gn (x)) − g(x) =

h2 
g (x)
2



1
−1

u2 K(u)du + O(h3 ) + o(

1
).
nh

A partir de ces formules asymptotiques il est possible d’étudier la vitesse de
convergence de l’e.q.m. pour la valeur de h optimale en x ﬁxé. Les développements sont analogues à ceux de la densité et l’on trouve également une vitesse
optimale de l’ordre de n−4/5 avec h de l’ordre de n−1/5 .
Toutefois ces formules ne sont valables que si la fenêtre [x − h, x + h] est
intégralement contenue dans l’intervalle [a, b]. Si, par exemple, x − h < a avec

Chapitre 11. Régressions linéaire, logistique et non paramétrique

317

x − qh = a où 0 ≤ q < 1, le terme uhg  (x) du développement de Taylor fournit
1
un premier terme de biais hg  (x) −q K(u)du généralement non nul. Supposons
que la fonction de régression g soit pratiquement linéaire au voisinage de x. Si
la fenêtre est à l’intérieur de [a, b] le biais est nul par compensation de part et
d’autre de x, mais si x − h < a la partie gauche de la fenêtre contient moins de
points ce qui introduit le biais.
Notons aussi que si g admet un extremum en x alors le biais est du signe de
g  (x) ce qui entraı̂ne un phénomène d’écrêtement déjà rencontré pour l’estimation de densité. Ici aussi l’on peut introduire des noyaux d’ordre 4 contenant
nécessairement une plage négative (voir les remarques diverses en ﬁn de section
8.5.2) pour remédier à ce problème. C’est d’ailleurs pour cette même raison que
les moyennes mobiles utilisent des poids négatifs sur les extrémités.
Cas des Xi aléatoires
On supposera que la densité conjointe fX,Y du couple (X, Y ) est continue dans R2 . On peut alors montrer que gn (x) converge en probabilité vers
g(x) en tout point x tel que fX (x) = 0. Collomb (1977) a établi les formules
asymptotiques du biais et de la variance suivantes (sous certaines conditions
de régularité de fX,Y et de K) :


fX
(x) 1 
u K(u)du g (x)
E(
gn (x)) − g(x) = h
+ g (x) + o(h2 )
fX (x) 2
−1
 1
σ2 1
1
V (
gn (x)) =
[K(u)]2 du + o( ) .
nh fX (x) −1
nh


2

1



2



Ces formules d’approximation asymptotique restent très théoriques car en pratique les valeurs de h convenables sont loin d’être faibles même avec des grands
échantillons. De plus, elles n’intègrent pas les eﬀets de bord. Néanmoins elles
reﬂètent bien les écueils des estimateurs à noyaux. Dans le cas où X suit une loi
continue uniforme sur [a, b] on retrouve les formules et les problèmes précédents.
Si la loi de X n’est pas uniforme il s’introduit un terme de biais supplémentaire

1
fX
(x)
h2 −1 u2 K(u)du .g  (x) fX
(x) , même si g est linéaire, dû au gradient de densité

autour de x mis en évidence par fX
(x), lequel déséquilibre la symétrie.
La régression par noyau présente, nous venons de le voir, des inconvénients
majeurs : eﬀets de bord (importants vu les largeurs de fenêtre nécessaires à un
lissage satisfaisant), écrêtement des extrema, présence de biais même pour une
fonction de régression linéaire si la densité de X n’est pas uniforme ou, dans
le cas d’un plan d’expérience, si la répartition des xi n’est pas régulière. La
méthode qui suit va y remédier.

318

11.4.4

Statistique − La théorie et ses applications

La régression polynomiale locale

Cette méthode (RPL) est une généralisation de la méthode Local Weighted
Regression ou LOWESS de Cleveland (1979) proposée par Lejeune9 (1983) dans
le cadre de l’estimation par noyau. Elle consiste, pour estimer g(x), à ajuster
une fonction polynomiale de degré s choisi, sur les couples de points (xi , yi ) dont
les xi sont situés dans le voisinage (fenêtre) [x − h, x + h] de x. L’ajustement
s’entend au sens classique des moindres carrés des résidus yi − yi (voir section
11.2) et est donc un cas particulier de régression linéaire multiple, la fonction
polynomiale étant linéaire par rapport aux paramètres inconnus. Il se
résout matriciellement comme indiqué en section 11.2.7. Alors g(x) est estimé
par la valeur ajustée au point x que nous noterons g'n (x).
Soit P (u) = a0 + a1 u + · · · + as us un polynôme de degré s. Cet ajustement
s’opère avec les valeurs de a0 , a1 , . . . , as telles que l’expression :
n

i=1

2

[yi − (a0 + a1 xi + · · · + as xsi )] K(

x − xi
)
h

soit minimale, où K est la fonction indicatrice de l’appartenance de la valeur
xi à la fenêtre (K(u) = 1 si |u| ≤ 1, 0 sinon). Soit 
a0 , 
a1 , . . . , 
as les valeurs
permettant d’atteindre le minimum, g(x) est alors estimé par g'n (x) = 
a0 +
as xs .

a1 x + · · · + 
Comme pour l’estimateur à noyau les propriétés de la fonction K se transfèrent à la fonction g'n et l’on aura avantage à substituer à la fonction indicatrice
une fonction de pondération dérivable partout, ce qui conduit à une solution
des moindres carrés pondérés. On montre aisément que la solution matricielle
de la section 11.2.7 pour le vecteur des paramètres devient (Xt WX)−1 Xt WY
i
où W est la n × n−matrice diagonale des poids K( x−x
h ) aﬀectés aux n observations. On pourra choisir pour K le noyau le plus simple possédant les
qualités requises, à savoir le biweight de Tukey. Notons d’ailleurs que l’estimation par
noyau correspond aui cas particulier s = 0, car la valeur de a0 qui
minimise ni=1 [yi − a0 ]2 K( x−x
h ) est la moyenne pondérée des yi avec les poids
i
).
K( x−x
h
Intuitivement on peut voir les avantages de la méthode pour autant que s
soit supérieur à 0. En eﬀet on perçoit bien qu’avec un simple ajustement local
linéaire (s = 1) on doit pouvoir prendre en compte les problèmes résultant du
diﬀérentiel de densité de points de part et d’autre de la valeur x, en particulier le
problème des eﬀets de bord. De même, mais avec s = 2, il est possible d’obtenir
un meilleur ajustement pour les zones à forte courbure, notamment en ce qui
concerne le problème de l’écrêtement des extrema. Nous allons vériﬁer cela sur
les propriétés de la RPL.
9 A l’origine la méthode a été également appelée «régression polynomiale mobile» par
référence à la moyenne mobile analogue dans son esprit à l’estimateur à noyau.

Chapitre 11. Régressions linéaire, logistique et non paramétrique

319

Cas des xi ﬁxés
On démontre (voir Lejeune, 1984, 1985) que la RPL au degré s :
1. produit, en tout x ﬁxé, un biais en O(hs+1 ) quelle que soit la répartition
des xi et quelle que soit la fonction de poids utilisée,
2. est, avec des pondérations uniformes et pour une largeur de fenêtre ﬁxée,
l’estimateur de variance minimale de g(x) parmi les estimateurs linéaires
(en fonction des Yi ) dont le biais est en O(hs+1 ).
La première propriété montre le caractère adaptatif de la RPL pour ce
qui concerne le problème du biais. Quant à l’optimalité exprimée dans la seconde propriété elle doit être quelque peu sacriﬁée si l’on veut bénéﬁcier de la
dérivabilité de la fonction g'n . Toutefois l’incidence est faible car la fonction de
poids n’inﬂue pas de façon très sensible sur la variance.

Cas des Xi aléatoires
Le biais et la variance asymptotiques ont été établis par Fan (1993). En fait
la variance ne dépend pas du degré de la RPL et reste égale à celle indiquée
plus haut pour l’estimateur à noyau correspondant au cas s = 0. Pour le biais
on obtient pour s ≥ 1 :
E('
gn (x)) − g(x) =

hs+1 (s+1)
g
(x)
(s + 1)!



1

us+1 K(u)du + o(hs+1 ).

−1

Par rapport à l’estimateur à noyau classique on constate qu’avec s = 1 le
terme dû au gradient de densité autour de x (mis en évidence par le facteur

(x)/fX (x) ) disparaı̂t.
g  (x)fX
En pratique le choix s = 1, proposé à l’origine par Cleveland et repris par
divers auteurs, n’est cependant pas satisfaisant car il ne traite pas le problème
de l’écrêtement des extrema (ou, plus généralement, du biais dans les zones
à forte courbure). Pour cela on peut considérer qu’un ajustement parabolique
(s = 2) suﬃra, d’autant qu’à n ﬁni la variance augmente avec l’ordre du biais.
La ﬁgure 11.3 illustre la bonne qualité d’un tel ajustement. Notons que la
RPL n’évite pas le problème du choix de la largeur de fenêtre. Néanmoins on
constate que l’estimation de g(x) est moins sensible à ce paramètre qu’avec un
estimateur à noyau.
Pour approfondir la régression non paramétrique on pourra consulter les
ouvrages de Haerdle (1990) et de Simonoﬀ (1996).

320

Statistique − La théorie et ses applications

Figure 11.3 - Estimation par régression polynomiale locale de degré 2 avec
pondérations biweight et h = 0, 4 : échantillon de 100 observations simulées
par un modèle sinusoı̈dal à erreurs N (0 ; 4) avec abcisses U[0, 1]. Reproduction
autorisée de la Revue de Statistique Appliquée, vol. XXXIII, n ◦ 3, page 62, 1985.

11.5

Exercices

Exercice 11.1 Soit (X, Y ) un couple aléatoire gaussien de paramètres μX , μY,
2
σX
, σY2 et ρ.
Partant de l’expression matricielle générale de la section 3.9 développer l’expression analytique de la densité conjointe du couple.

Chapitre 11. Régressions linéaire, logistique et non paramétrique

321

Montrer que la loi conditionnelle de Y sachant X = x est gaussienne de
Y
(x − μX ) et de variance σY2 (1 − ρ2 ).
moyenne μY + ρ σσX
Aide : on utilisera le résultat de la section 3.2 pour la densité conditionnelle :
f
(x,y)
fY |X=x (y) = X,Y
fX (x) .
Exercice 11.2 * Soit (X, Y ) un couple aléatoire non nécessairement gaussien
2
, σY2 et ρ. Montrer
de moyennes, variances et corrélation linéaire μX , μY, σX
que si E(Y |X = x) est une fonction linéaire de x alors cette fonction est
Y
(x − μX ).
μY + ρ σσX
Aide : Soit ϕ(x) = E(Y |X = x). Calculer E(ϕ(X)) d’une part de façon
générale en passant par la formule pour
la densité conditionnelle :

E(Y |X = x) =

R

y

fX,Y (x, y)
dy
fX (x)

et d’autre part par l’expression linéaire E(Y |X = x) = β0 + β1 x pour obtenir
une première équation en β0 et β1 , puis calculer de même E(Xϕ(X)) pour
obtenir une deuxième équation.
Montrer que si, de plus, la variance conditionnelle V (Y |X = x) ne dépend pas
de x alors elle est égale à σY2 (1 − ρ2 ).
Aide : Soit ψ(x) = V (Y |X = x). Calculer E(ψ(X)) d’une part en décentrant
V (Y |X = x) et d’autre part en tenant compte de la propriété sur V (Y |X = x).
Exercice 11.3 Démontrer la formule de décomposition de la somme des carrés
totale pour la régression linéaire simple.
yi − y) et montrer, en remplaçant yi
Aide : partir de yi − y = (yi − yi ) + (
n

par y + β1 (xi − x), que i=1 (yi − yi )(
yi − y) = 0.
Exercice 11.4 (intervalle de prédiction) Dans le cadre de la régression linéaire
gaussienne simple on cherche à prévoir une observation Y0 pour le niveau x0
de la variable explicative.
Montrer que Y0 − (β0 + β1 x0 ) suit une loi de Gauss de moyenne 0 et de va−x)2
(aide : Y0 est indépendante des Yi sur lesquels
riance σ 2 1 + n1 +  n(x0(x
2
i −x)
i=1
reposent β0 et β1 ).
En déduire que :

)
1
(n−2)
P β0 + β1 x0 − t0,975 S 1 + +
n

2
 n(x0 −x) 2
(x
−x)
i
i=1

)
(n−2)
β0 + β1 x0 + t0,975 S

1+

1
+
n

< Y0 <

2
 n(x0 −x) 2
i=1 (xi −x)

= 0, 95.

Les réalisations des bornes d’encadrement de Y0 (qui sont aléatoires) constituent un «intervalle de prédiction à 95%» pour Y0 .

322

Statistique − La théorie et ses applications

Exercice 11.5 * Démontrer le théorème 11.1 pour β1 (cet exercice nécessite
la connaissance de la méthode du multiplicateur de Lagrange).
Aide
:n établir les deux contraintes sur les ai pour qu’un estimateur de la
forme i=1 ai Yi soit sans biais pour β1 . Puis minimiser la variance d’un tel
estimateur sous ces contraintes.
Exercice 11.6 Pour la régression simple, déterminer la statistique λ du test
du rapport de vraisemblance généralisé pour l’hypothèse H0 : β1 = 0 et montrer
que c’est une fonction décroissante de la statistique F.

Corrigés des exercices

Corrigés des exercices

325

Chapitre 1 : Variables aléatoires
Exercice 1.1
Clairement, on a (avec la convention A0 = ∅) :
n

n

j=1

j=1

∪ Aj = An = ∪ (Aj ∩ Aj−1 )

et :

∞

∞

n=1

n=1

∪ An = ∪ (An ∩ An−1 ) .

La suite {An ∩ An−1 } étant une suite d’événements incompatibles, on a :
∞

P ( ∪ An ) =
n=1

∞


P (An ∩ An−1 )

n=1

= lim

n→∞

n


P (Aj ∩ Aj−1 )

j=1



= lim P
n→∞



n

∪ (Aj ∩ Aj−1 )

j=1

= lim P (An ) .
n→∞

Note : Soit X une v.a. de fonction de répartition FX , montrons que
P (X < x) = FX (x− ). Considérons l’événement An =] − ∞, x − n1 ] – soit,
∞

avec la convention de notation usuelle (X ≤ x − n1 ). Comme ∪ An =] − ∞, x[,
n=1

on a : P (X < x) = lim P (X ≤ x − n1 ) = lim FX (x − n1 ). Or, en raison de la
n→∞

n→∞

non-décroissance de FX , lim FX (x − n1 ) = lim FX (x − ε) = FX (x− ).
n→∞

ε→0

Exercice 1.2
Utilisons le fait que {B n } est une suite croissante. On a :
∞

∞

P ( ∩ Bn ) = 1 − P ( ∪ B n )
n=1

n=1

= 1 − lim P (B n )
n→∞

= 1 − lim [1 − P (Bn )] = lim P (Bn ).
n→∞

n→∞

326

Statistique - La théorie et ses applications

Note : Dans le même contexte d’application que celui de la note précédente,
prenons Bn =]−∞, x+ n1 ]. On a ainsi lim P (Bn ) = lim FX (x+ n1 ) = FX (x+ ).
n→∞

∞

n→∞

Comme ∩ Bn =]−∞, x] il découle du résultat précédent que FX (x+ ) = FX (x),
n=1
ce qui établit la continuité à droite de la fonction de répartition.

Exercice 1.3
Considérons la suite croissante d’événéments {An } avec An =] − ∞, n].
∞
Comme ∪ An = R, on a :
n=1

∞

P ( ∪ An ) = 1 = lim P (An ) = lim FX (n).
n→∞

n=1

n→∞

En raison de la non décroissance de FX , on a lim FX (n) = lim FX (x) =
n→∞

x→∞

FX (+∞) = 1.
∞
De même avec An =] − n, +∞], on a P ( ∪ An ) = 1 = lim P (An ) =
n=1

lim [1 − FX (−n)], d’où lim [FX (−n)] = 0 soit FX (−∞) = 0.

n→∞

n→∞

n→∞

Exercice 1.4
Considérons la suite décroissante d’événéments {Bn } avec Bn =]x − n1 , x].
Notons que pour tout n, Bn est un bien événement puisque :
]x −

1
1
, x] =] − ∞, x] ∩ ] − ∞, x − ] .
n
n

∞

Or ∩ Bn = {x}, événement dont la probabilité se note conventionnellement
n=1

P (X = x). D’où, d’après le résultat de l’exercice 1.2 :
1
P (X = x) = lim P (Bn ) = lim P (x − < X ≤ x)
n→∞
n→∞
n


1
= lim FX (x) − FX (x − )
n→∞
n


1
= FX (x) − lim FX (x − ) = FX (x) − FX (x− ).
n→∞
n
Note : Dans la note de l’exercice 1.1, on a établi que P (X < x) = FX (x− ).
Comme (X ≤ x) = (X < x) ∪ (X = x) le résultat ci-dessus est alors immédiat.

Corrigés des exercices

327

Exercice 1.5
La fonction de probabilité pX de X est déﬁnie sur N∗ = {1, 2, 3, · · · }.
 2
pX (1) = 16 , pX (2) = 56 16 , pX (3) = 56 61 et plus généralement :
pX (k) =
On a bien :


k∈N∗

 k−1
5
1
6
6

pX (k) =

1
6

+
1+

P (1 < X ≤ 3) = pX (2)+ pX (3) =

5
6

51
66

pour k ∈ N∗ .

+

 5 2

+

6

 5 2
6

,
+ ··· =
1
6

1
6

+

1
1− 56

,
= 1.

 0, 255 .

Pour k ∈ N∗ on a :

  
 2
 k 
k
1
5
5
5
5
1+ +
+ ··· =
.
P (X > k) =
6
6
6
6
6

Donc FX (k) = 1 −

 5 k
6

et plus généralement :

"
FX (x) =

1−

0
 5 k
6

pour x < 1
pour x ∈ [k, k + 1[

Son graphe est une fonction en escalier avec sauts des marches aux valeurs
1, 2, 3, · · · . Il y a continuité à droite (voir ﬁgure 1.1).
Note : Il s’agit d’une variante de la loi géométrique (section 4.1.4)

Exercice 1.6
On doit avoir d’abord f (x) ≥ 0 pour x ∈ [0, 1] soit c > 0 puisque x(1−x) ≥ 0
sur [0, 1]. Puis :


1



1

cx(1 − x)dx = c

0


(x − x2) dx = c

0

x2
x3
−
2
3

1
=c
0

1
6

doit être égal à 1. +D’où c =,6. La fonction de répartition vaut, pour x ∈ [0, 1] :
x
2
3
6 0 (x − x2) dx=6 x2 − x3 . Plus généralement :
⎧
⎪
⎪
⎨
F (x) =

⎪
⎪
⎩

+
6

x2
2

0 ,pour x ≤ 0
3
pour x ∈ [0, 1]
− x3
1 pour x ≥ 1

328

Statistique - La théorie et ses applications

On vériﬁe que F est continue partout. La médiane est la valeur de x telle
que F (x) = 12 . L’équation n’est pas simple à résoudre mais l’on peut constater
que le graphe de f (x) est symétrique par rapport à x = 12 qui est donc la
médiane (et la moyenne).
Note : Il s’agit de la loi bêta Beta(1, 1) (section 4.2.9).

Exercice 1.7
La fonction F (x) est non décroissante, elle part de 0 et tend vers 1 quand
x tend vers +∞. Comme elle est continue partout, elle caractérise une v.a.
continue. Elle est strictement croissante sur le support [0, +∞[ de la loi et les
x0,25
quantiles sont donc uniques. Le premier quartile x0,25 vériﬁe 1−e− 2 = 0, 25,
d’où x0,25 = −2 ln(1 − 0, 25)  0, 575. De même x0,75 = −2 ln(1 − 0, 75)  2, 77.
1

2

P (1 < X ≤ 2) = F (2) − F (1) = e− 2 − e− 2  0, 239.
Note : Il s’agit de la loi exponentielle E( 12 ) (section 4.2.2).

Exercice 1.8
Posons Y = 1/X. Au support [0, 1] de X correspond le support [1, +∞[ de
Y. Alors, pour y ∈ [1, +∞[, on a :
FY (y) = P (Y ≤ y) = P (
Or, pour t ∈ [0, 1], FX (t) =

t
0

1
1
1
≤ y) = P (X ≥ ) = 1 − FX ( ).
X
y
y

2xdx = t2 . D’où :
"

0 pour y ≤ 1
.
1 − y12 pour y ≥ 1

FY (y) =

On vériﬁe la continuité de FY au point y = 1.
Posons Z = ln(1/X). Au support [0, 1] de X correspond le support [0, +∞[
de Z. Alors, pour z ∈ [0, +∞[, on a :
FZ (z) = P (Z ≤ z) = P (ln(
"

D’où :
FZ (z) =

1
1
1
) ≤ z) = P (X ≥ z ) = 1 − FX ( z ).
X
e
e

0 pour z ≤ 0
.
1 − e−2z pour z ≥ 0

Corrigés des exercices

329

On vériﬁe la continuité de FZ au point z = 0. Il s’agit de la loi exponentielle
E(2) (section 4.2.2).
Exercice 1.9
Sur [0, 1] on a FX (x) = x. Au support [0, 1] de X correspond le support
[0, +∞[ de Y. Alors, pour y ∈ [0, +∞[, on a :
FY (y) = P (Y ≤ y) = P (−θ ln(1 − X) ≤ y) = P (ln(1 − X) ≥ − yθ )
y
y
FY (y) = P (X ≤ 1 − e− θ ) = 1 − e− θ .
Pour y ≤ 0 FY (y) vaut 0. On vériﬁe la continuité de FY au point y = 0.
Note : Il s’agit de la loi exponentielle E( 1θ ) (section 4.2.2). La transformation
de nombres au hasard par la fonction −θ ln(1 − x) permet donc de simuler des
observations d’une loi exponentielle E( 1θ ) (voir section 4.3).

330

Statistique - La théorie et ses applications

Chapitre 2 : Espérance mathématique
et moments
Exercice 2.1
E(Y ) = (02 − 1) × 0, 7 + (12 − 1) × 0, 2 + (22 − 1) × 0, 1 = −0, 4.

Exercice 2.2
Suivons la méthode de la section 1.6.
FZ (z) = P (Z ≤ z) = P (g(X) ≤ z).
Comme g est croissante, l’événement (g(X) ≤ z) est identique à l’événement
(X ≤ g −1 (z)), donc :
FZ (z) = P (X ≤ g −1 (z)) = FX (g −1 (z)).
Par dérivation, on obtient :

(g −1 (z))(g−1 (z)) = fX (g−1 (z))(g −1 (z))
fZ (z) = FZ (z) = FX

ce qui peut s’écrire, puisque g −1 est également croissante et (g −1 ) est positive :
fZ (z) = fX (g −1 (z))|(g −1 (z)) | .
Si g est décroissante, l’événement (g(X) ≤ z) est identique à (X ≥ g −1 (z)),
donc :
FZ (z) = P (X ≥ g −1 (z)) = 1 − FX (g −1 (z)).

Par dérivation, fZ (z) = FZ (z) = −FX
(g −1 (z))(g−1 (z)) = −fX (g −1 (z))(g−1 (z)) .
Comme g −1 est décroissante, (g −1 ) est négative et fZ (z) = fX (g −1 (z))|(g −1 (z)) |
également.

Soit maintenant g strictement croissante, variant de a à b quand x croı̂t de
−∞ à +∞ (où a et b peuvent, respectivement, être −∞ ou +∞). Alors :


b

E(Z) =
a

z fX (g −1 (z))(g −1 (z)) dz.

Corrigés des exercices

331

Eﬀectuons le changement de variable x = g −1 (z) avec dx = (g −1 (z)) dz et
z = g(x), d’où immédiatement :


+∞

E(Z) =
−∞

g(x) fX (x) dx.

Soit g strictement décroissante. Supposons que g(x) décroı̂t de b à a (a < b)
quand x croı̂t de −∞ à +∞ (où a et b peuvent, respectivement, être −∞ ou
+∞). Alors :
 b
−z fX (g −1 (z))(g−1 (z)) dz
E(Z) =
a

et par le changement de variable x = g −1 (z) on obtient :


−∞

E(Z) =


−g(x) fX (x) dx =

+∞

−∞

g(x) fX (x) dx.
+∞

Exercice 2.3
Soit X de densité f (x), alors :


+∞

1
etx e−|x| dx
2
−∞
 0
 +∞
1
1
=
etx ex dx +
etx e−x dx
2
2
−∞
0
 0
 +∞
1
1
e(t+1)x dx +
e(t−1)x dx
=
2 −∞
2 0
,0
,+∞
+
+
1
1
=
+
e(t+1)x
e(t−1)x
2(t + 1)
2(t − 1)
−∞
0




1
1
(t+1)x
(t−1)x
+
1 − lim e
lim e
=
−1
x→−∞
2(t + 1)
2(t − 1) x→+∞

Ψ(t) = E(etX ) =

La première limite est 0 si t + 1 > 1, soit t > −1, et la deuxième est 0 si t < 1.
Donc Ψ(t) est déﬁnie au voisinage de 0 et :
Ψ(t) =

1
(t − 1) − (t + 1)
1
1
−
=
=
pour |t| < 1.
2(t + 1) 2(t − 1)
2(t2 − 1)
1 − t2

En développant Ψ(t) en série entière (voir Note 2.3) :
Ψ(t) = 1 + t2 + t4 + · · · ,

332

Statistique - La théorie et ses applications

on accède directement aux moments, d’où : μ2 = 2! = 2, μ4 = 4! = 24. Ce sont
aussi les moments centrés, donc le coeﬃcient d’aplatissement vaut :
24
μ4
−3=
−3=3
σ4
4
ce qui indique un pic plus pointu en la moyenne que pour la loi de Gauss.

Exercice 2.4
On a :



+∞

E(X) =
a

θ
x
a

θ+1

a
x


dx = θa

+∞

θ
a

1
dx.
xθ

L’intégrale ci-dessus ne convergeant que si θ > 1, la moyenne n’existe qu’à cette
condition et vaut alors :
 −θ+1 +∞
x
θ
E(X) = θa
−θ + 1 a
 −θ+1 
a
aθ
=
.
= θaθ
θ−1
θ−1
Puis :

E(X 2 ) =
a

+∞

x2

θ
a

a
x

θ+1



+∞

dx = θaθ
a

1
dx.
xθ−1

L’intégrale ci-dessus ne convergeant que si θ > 2, E(X 2 ) et la variance n’existent
qu’à cette condition. Alors :


+∞
x−θ+2
−θ + 2 a
 −θ+2 
θa2
a
=
.
= θaθ
θ−2
θ−2

E(X 2 ) = θaθ

Ainsi :
θa2
θ 2 a2
−
θ − 2 (θ − 1)2
θa2 (θ − 1)2 − θ2 a2 (θ − 2)
θa2 [θ2 − 2θ + 1 − θ(θ − 2)]
=
=
2
(θ − 2)(θ − 1)
(θ − 2)(θ − 1)2
θa2
=
(θ − 2)(θ − 1)2
2

V (X) = E(X 2 ) − [E(X)] =

Corrigés des exercices

333

Généralisons à μr = E(X r ) avec r > 2, r entier :
 +∞
 +∞
θ a θ+1
1
xr
dx = θaθ
dx.
E(X r ) =
θ−r+1
a
x
x
a
a
L’intégrale ne converge que si θ − r + 1 > 1, soit θ > r. Notons qu’il en va de

même pour μr = E((X − μ)r ) qui s’exprime en fonction de μr , μr−1 , · · · , μ2 , μ.
Or si μr existe, les moments d’ordres inférieurs existent. Pour θ > r on a donc :

E(X r ) = θaθ

x−θ+r
−θ + r



+∞
= θaθ
a

θar
.
=
θ−r

a−θ+r
θ−r



La condition θ > r laisse entendre que la fonction génératrice − permettant
pour θ ﬁxé d’obtenir tous les moments − ne peut exister, ce que nous vériﬁons
directement en calculant :
 +∞
 +∞ tx
θ a θ+1
e
etx
dx = θaθ
dx.
Ψ(t) = E(etX ) =
θ+1
a
x
x
a
a
Or, quel que soit θ ∈ R,

tx
lim eθ+1
x→+∞ x

→ +∞ si t > 0 et l’intégrale ne peut

converger. Donc Ψ n’est déﬁnie dans aucun voisinage de 0, condition nécessaire

pour la déﬁnition de la fonction génératrice des moments aﬁn que Ψ (0), Ψ (0),
etc., puissent exister.

Exercice 2.5
1.
E(

1
)=
X



1
0

1 2
3x dx = 3
x





1

x dx = 3
0

2. X prend ses valeurs sur [0, 1] donc Y =
Ainsi FY (y) = 0 si y ≤ 1. Soit y ≥ 1, alors :

1
X

x2
2

1
=
0

3
.
2

prend ses valeurs sur [1, +∞[.

1
≤ y)
X
1
1
= P ( ≤ X) car > 0 et X ne prend que des valeurs positives,
y
y
1
= 1 − FX ( )
y

FY (y) = P (

334

Statistique - La théorie et ses applications


Or :

x

FX (x) =

3t2 dt = x3 pour x ∈ [0, 1] ,

0

donc :

1
pour y ≥ 1 (et 0 sinon).
y3

FY (y) = 1 −

On vériﬁe que la continuité est assurée pour y = 1 car l’expression ci-dessus
vaut 0 pour y = 1. Par dérivation on a :
fY (y) =

3
y4

pour y ≥ 1 (et 0 sinon),

d’où :




+∞

y

E(Y ) =
1



=3

−2

y
−2

3
y4



+∞
1

+∞

=
1


dy = 3
3
.
2

1
dy
y3

Corrigés des exercices

335

Chapitre 3 : Couples et n-uplets
de variables aléatoires
Exercice 3.1
On a :
E(X) = 1 × 0, 35 + 2 × 0, 45 + 3 × 0, 20 = 1, 85
E(Y ) = 1 × 0, 48 + 2 × 0, 33 + 3 × 0, 19 = 1, 71
E(X 2 ) = 12 × 0, 35 + 22 × 0, 45 + 32 × 0, 20 = 3, 95
E(Y 2 ) = 12 × 0, 48 + 22 × 0, 33 + 32 × 0, 19 = 3, 51
V (X) = 3, 95 − (1, 85)2 = 0, 5275

V (Y ) = 3, 51 − (1, 71)2 = 0, 5859

E(XY ) = 1 × 1 × 0, 22 + 1 × 2 × 0, 11 + · · · + 3 × 3 × 0, 07 = 3, 33
cov(X, Y ) = 3, 33 − 1, 85 × 1, 71 = 0, 1665
et ﬁnalement :
corr(X, Y ) = √

0, 1665
= 0, 2995.
0, 5275 × 0, 5859

Exercice 3.2
Soit X et Y les deux variables aléatoires. On a, quelles qu’elles soient :
cov(X + Y, X − Y ) = cov(X + Y, X) − cov(X + Y, Y )
= cov(X, X) + cov(Y, X) − cov(X, Y ) − cov(Y, Y )
= V (X) − V (Y ).
Il suﬃt que les deux variables soient de même loi pour que la covariance soit
nulle. L’indépendance n’est pas nécessaire.

Exercice 3.3
L’événement (X + Y = 0) équivaut à (X = 0, Y = 0) − c’est-à-dire
(X = 0)∩(Y = 0) − dont, par l’indépendance, la probabilité vaut (1−p)(1−p).

336

Statistique - La théorie et ses applications

De même (X + Y = 0) équivaut à : (X = 0, Y = 1) ou (X = 1, Y = 0) soit une
probabilité totale de 2 × p(1 − p), etc.
valeurs possibles
0
1
2
La loi de X +Y est déﬁnie par :
2
2p(1 − p) p2
avec probabilités (1 − p)
On établit de la même façon la loi de X − Y :
valeurs possibles
avec probabilités

-1
p(1 − p)

0
p + (1 − p)2
2

1
p(1 − p)

L’événement (X + Y = 0, X − Y = 0) ne se réalise que si et seulement si
(X = 0, Y = 0) (puisque le système des deux équations x + y = 0 et x − y = 0
n’a qu’une seule solution : x = 0 et y = 0). Par conséquent :
P (X + Y = 0, X − Y = 0) = P (X = 0, Y = 0) = (1 − p)2 ,
alors que P (X +Y = 0)P ( X −Y = 0) = (1−p)2 [p2 +(1−p)2 ], ce qui est diﬀérent
dans les cas non dégénérés (i.e. p = 0 et p = 1). On en déduit donc que X + Y
et X − Y ne sont pas deux v.a. indépendantes. Toutefois cov(X + Y, X − Y ) = 0
comme il a été montré dans l’exercice 3.2 ci-dessus, ce qui illustre le fait qu’une
corrélation linéaire nulle n’implique pas nécessairement l’indépendance.

Exercice 3.4
Calculons P (Z ≤ z|X = x).
P (Z ≤ z|X = x) = P (X + Y ≤ z|X = x) = P (x + Y ≤ z|X = x)
= P (Y ≤ z − x|X = x)
= P (Y ≤ z − x) car X et Y sont indépendantes.
Donc FZ|X=x (z) = FY (z − x) et, en dérivant par rapport à z, fZ|X=x (z) =
fY (z − x).
On a montré en ﬁn de section 3.2 que :
fZ|X=x (z) =

fX,Z (x, z)
,
fX (x)

d’où :
fX,Z (x, z) = fY (z − x)fX (x)

Corrigés des exercices
et (voir aussi section 3.2) :
 +∞

fX,Z (x, z) dx =
fZ (z) =
−∞

+∞

−∞

337

fY (z − x)fX (x) dx .

Notons que par le changement de variable y = z − x cette densité s’écrit
 +∞
également −∞ fY (y)fX (z − y) dy
Pour T = X − Y on se ramène au cas précédent en posant U = −Y , d’où
T = X + U. La loi de U est donnée par :
FU (y) = P (U ≤ y) = P (−Y ≤ y) = P (−y ≤ Y ) = 1 − FY (−y) ,
d’où fU (y) = fY (−y). En appliquant la formule plus haut, on obtient :
 +∞
 +∞
fU (t − x)fX (x) dx =
fY (x − t)fX (x) dx ,
fT (t) =
−∞

−∞

ce qui s’écrit encore, par changement de variable y = x − t :
 +∞
f (y)fX (t + y) dy .
−∞ Y
Exercice 3.5
Comme fX (x) = 1 pour 0 ≤ x ≤ 1 et fY (y) = 1 pour 0 ≤ y ≤ 1, par
l’indépendance on a fX,Y (x, y) = 1 dans le carré du plan {(x, y) | 0 ≤ x ≤ 1,
0 ≤ y ≤ 1} et 0 ailleurs. Ainsi la probabilité de toute région à l’intérieur
du carré est égale à son aire. Posons Z = X + Y et calculons sa fonction de
répartition P (Z ≤ z). On voit sur la ﬁgure 3.1 qu’il faut distinguer le cas z ≤ 1
du cas z > 1.
1) Pour z ∈ [0, 1]
L’événement (X + Y ≤ z) correspond alors aux réalisations (x, y) du couple
(X, Y ) appartenant au domaine A indiqué sur la ﬁgure. Son aire étant égale à
z 2 /2 on a P (Z ≤ z) = z 2 /2 si 0 ≤ z ≤ 1.
2) Pour z ∈ [1, 2]
L’événement (X + Y ≤ z) correspond alors aux réalisations (x, y) du couple
(X, Y ) appartenant au carré, à l’exclusion du domaine B indiqué sur la ﬁgure
dont l’aire est (2 − z)2 /2. D’où P (Z ≤ z) = 1 − (2 − z)2 /2 si 1 < z ≤ 2. Donc :
⎧
si z ≤ 0
⎪
⎪ 0
⎪
⎨ z 2 /2
si 0 ≤ z ≤ 1
.
FX+Y (z) =
2
⎪ (2 − z) /2
si 1 ≤ z ≤ 2
⎪
⎪
⎩
1
si 2 ≤ z

338

Statistique - La théorie et ses applications

On vériﬁe qu’il y a continuité entre les diﬀérents morceaux de la fonction de
répartition.
Dérivons pour décrire la densité :
⎧
⎪
⎨ z
fX+Y (z) =
2−z
⎪
⎩
0

si 0 ≤ z ≤ 1
si 1 ≤ z ≤ 2
sinon

dont le graphe fait apparaı̂tre un triangle dont la base est le segment [0, 2] sur
l’axe des abcisses. Cette loi est naturellement appelée loi triangulaire.
Note : Pour le calcul de la densité on aurait pu appliquer directement le résultat
de l’exercice 3.4 mais en étant attentif aux bornes de l’intégrale. En premier
 +∞
z
lieu −∞ fY (z − x)fX (x) dx se ramène à z−1 fX (x) dx puisque fY (z − x) = 1
quand 0 < z − x < 1, soit z − 1 < x < z. Pour calculer cette dernière intégrale,
z
il faut distinguer le cas 0 < z < 1 où elle vaut 0 1 dx = z, et le cas 1 < z < 2
1
où elle vaut z−1 1 dx = 2 − z.
Exercice 3.6
La surface étant XY, on a E(XY ) = E(X)E(Y ) = μX μY (voir la proposition 3.7).
De plus, V (XY ) = E((XY )2 ) − [E(XY )]2 . Or E((XY )2 ) = E(X 2 Y 2 ) =
E(X 2 )E(Y 2 ). En eﬀet, comme X et Y sont indépendantes, X 2 et Y 2 le sont
2
+ μ2X et E(Y 2 ) = σY2 + μ2Y , on
aussi (proposition 3.4). Puisque E(X 2 ) = σX
a:
2
2 2
2
V (XY ) = (σX
+ μ2X )(σY2 + μ2Y ) − (μX μY )2 = σX
σY + μ2X σY2 + μ2Y σX
.
Exercice 3.7
La v.a. X ayant pour fonction de probabilité :
"
p
si x = 1
p(x) =
,
1 − p si x = −1
on a :
E(X) = p + (−1)(1 − p) = 2p − 1
E(X 2 ) = 12 p + (−1)2 (1 − p) = 1
V (X) = E(X 2 ) − [E(X)]2 = 1 − (2p − 1)2 = 4p(1 − p).

Corrigés des exercices

339

Soit maintenant n étapes successives de déplacements X1 , X2 , · · · , Xn , ces
v.a. étant indépendantes et chacune suivant la loi ci-dessus. La position résultante
n
étant Y = i=1 Xi , on a :
E(Y ) = nE(X) = n(2p − 1)
V (Y ) = nV (X) = 4np(1 − p).
De toute évidence, si p > 1/2 alors E(Y ) > 0 et, inversement, si p < 1/2
alors E(Y ) < 0.

Exercice 3.8
Appliquons avec p = 2 la formule générale de la densité gaussienne d’un
p−vecteur gaussien :
!
1
1
t
−1
fX (x1 , x2 , · · · , xp ) =
exp − (x − μ) Σ (x − μ) .
2
(2π)p/2 (det Σ)1/2
La matrice des variances-covariances est :

2
ρσX σY
σX
Σ=
ρσX σY
σY2

L’inverse d’une 2 × 2−matrice inversible A =
−1

A

1
=
det A



d
−c

−b
a

2 2
on a det Σ = σX
σY (1 − ρ2 ) et :

Σ−1

1
= 2 2
σX σY (1 − ρ2 )



.
a
c

b
d

étant :

où det A = ad − bc ,

σY2
−ρσX σY

−ρσX σY
2
σX

.


L’expression dans l’exponentielle se réduit ici à A = − 12 (x, y) Σ−1


a c
x
Comme (x, y)
= ax2 + 2cxy + by 2 on a :
c b
y
A=−
=−

1
2 2
2 2
2 σ 2 (1 − ρ2 ) (σY x − 2ρσX σY xy + σX y )
2σX
Y
x2
xy
y2
1
(
−
2ρ
+
)
2
2(1 − ρ2 ) σX
σX σY
σY2

Par ailleurs, la constante devant l’exponentielle est bien

1

1

2π(det Σ) 2

.

x
y

.

340

Statistique - La théorie et ses applications

Chapitre 4 : Les lois de probabilités
usuelles
Exercice 4.1
La marche aléatoire est décrite dans l’exercice 3.7.
Notons que lorsque X prend respectivement les valeurs 1 et −1, Y =
+ 1) prend les valeurs 1 et 0. X prenant la valeur 1 avec probabilité
p, Y suit une loi de Bernoulli B(p).
1
(X
2

La v.a. de la marche aléatoire après n pas est T =
Comme Xi = 2Yi − 1, avec Yi ; B(p) , on a :
T =2
où S =

n

n


n
i=1

Xi .

Yi − n = 2S − n

i=1

Yi suit une loi binomiale B(n, p) :
 
n k
p (1 − p)n−k pour k = 0, 1, 2, · · · , n.
P (S = k) =
k

i=1

Comme l’événement (S = k) équivaut à (T = 2k − n) on a, en posant
t = 2k − n, la loi suivante pour T :


n+t
n−t
n
P (T = t) = n+t p 2 (1 − p) 2 pour t = −n, −n + 2, · · · , n − 2, n.
2

Exercice 4.2
Soit X qui suit une loi binomiale négative BN (r, p), d’où (voir section
4.1.4) :

r+x−1 r
p (1 − p)x pour x ∈ N.
x
Sa fonction génératrice des moments est (section 2.5) :
 r + x − 1
etx
ΨX (t) = E(etX ) =
pr (1 − p)x
x
x∈N
 r + x − 1
[(1 − p)et ]x .
= pr
x


P (X = x) =

x∈N

Corrigés des exercices

341

Pour calculer la somme ci-dessus, posons 1 − q = (1 − p)et et notons qu’en
vertu de la fonction de probabilité d’une loi BN (r, q) on a :
 r + x − 1
q r (1 − q)x = 1
x
x∈N

 r + x − 1
1
(1 − q)x = r .
x
q

et :

x∈N

Notons que ceci est vrai si 0 < 1 − q < 1 et que, de toute façon, les sommes
ci-dessus sont divergentes si 1 − q > 1 puisque (1 − q)x → +∞ quand x → +∞.
1
1
ou t < ln 1−p
ou
Il est donc nécessaire que 0 < (1 − p)et < 1 soit 0 < et < 1−p
t < − ln(1 − p)
Finalement, en revenant à la dernière écriture deΨX (t), on a :
ΨX (t) = pr
=

1
qr

pr
[1 − (1 − p)et ]r

qui est bien l’expression donnée en section 4.1.4.
Pour obtenir E(X), dérivons par rapport à p chaque terme de l’égalité :
 r + x − 1
pr (1 − p)x = 1
x
x∈N

ce qui donne :
 r + x − 1
 r + x − 1
r−1
x
p (1 − p) −
r
pr (1 − p)x−1 = 0,
x
x
x
x∈N

x∈N

soit en multipliant par p :
r−

d’où E(X) =

r(1−p)
.
p



r+x−1 r
p 
p (1 − p)x = 0
x
x
1−p
x∈N
p
r−
E(X) = 0
1−p


On peut évidemment arriver à ce résultat en calculant ΨX (0) comme indiqué
en proposition 2.2.

342

Statistique - La théorie et ses applications

Exercice 4.3
On posera q = 1 − p pour simpliﬁer les écritures.
Notons que, par interchangeabilité des notions de succès et échec, n−x → ∞
pour les valeurs d’intérêt de n − x.
On a :
n!
px q n−x
P (X = x) =
x!(n − x)!
En appliquant la formule de Stirling pour n! , x! et (n − x)! on obtient :

1/2
1
nx nn−x
n
P (X = x) ∼ √
px q n−x
x
x (n − x)n−x
2π x(n − x)

1/2  x 
n−x −1
x
n−x
1
1
P (X = x) ∼ √
,
x
x
np
nq
2π n n (1 − n )
soit, en posant u = x − np (donc
1
P (X = x) ∼ √
2πnpq

u
n



→ 0 puisque
u
1+
np

→p):

x
n

u+np 

u
1−
nq

u−nq −1

Le logarithme de l’expression entre parenthèses est égal à :
(u + np) ln(1 +

u
u
) + (u − nq) ln(1 −
)
np
nq

et équivalent à :

(u + np)




u
u2
u2
u2
u
− 2 2 + (u − nq) −
− 2 2 =
np 2n p
nq
2n q
2npq

donc :
u2
1
exp −
P (X = x) ∼ √
2npq
2πnpq

!

(x − np)2
1
exp −
=√
2npq
2πnpq

qui est la fonction de la densité de U en x.
Montrons maintenant que cette expression est équivalente à :
P (x −

1
1
1
1
< U < x + ) = FU (x + ) − FU (x − ).
2
2
2
2

Par la formule des accroissements ﬁnis on a :
1
1
FU (x + ) − FU (x − ) = fU (x + h)
2
2

!

Corrigés des exercices

343

où h ∈ (− 12 , + 12 ) et fU est la fonction de densité de U , donc :
!
1
(x + h − np)2
.
exp −
fU (x + h) = √
2npq
2πnpq
Mais fU (x + h) ∼ fU (x) car :
fU (x + h)
= exp
fU (x)

−(x + h − np)2 + (x − np)2
2npq

!
= exp −


!
h
x
h
2( − p) +
2pq
n
n

tend vers 1 quand n → ∞, ce qui montre que P (X = x) ∼ P (x − 12 < U <
x + 12 ). Cela constitue une approximation d’une loi binomiale par une loi de
Gauss quand n est grand (voir section 5.8.3).
Annexe :
Justiﬁons sommairement le fait que les valeurs utiles de x tendent vers l’inﬁni
en recourant à l’inégalité de Tchebichev introduite dans l’exercice 5.9. Selon
cette inégalité, pour toute v.a. X ayant une variance et pour tout k > 0, on a :
P (|X − μ| < kσ) ≥ 1 −
soit, ici, en choisissant k =

√
np :

1
k2

1
√
P (|X − np| < np q) ≥ 1 −
np
1
√
√
P (np(1 − q) < X < np(1 + q)) ≥ 1 −
np
√
et donc l’ensemble des valeurs inférieures à np(1 − q), qui tend vers l’inﬁni,
reçoit une probabilité tendant vers 0.
Par ailleurs, le fait que pour les valeurs d’intérêt de x on ait nx → p découle
de la loi des grands nombres (section 5.8.2).

Exercice 4.4
Cet exercice demande une démonstration directe de la propriété établie par
la fonction génératrice en section 4.1.7.
P (X = x) =

(np)x
n!
px (1 − p)n−x ∼
(1 − p)n−x
x!(n − x)!
x!

n!
puisque (n−x)!
∼ nx . En posant np = λ (constant quand n → ∞), on a
n−x
= (1− nλ )n−x , lequel tend vers e−λ quand n → ∞, et donc P (X = x)
(1−p)
x −λ
e
tend vers λ x!
qui est la probabilité de la loi P(λ) en x.

344

Statistique - La théorie et ses applications

Exercice 4.5
Pour cette loi hypergéométrique, on a :
M N −M 
P (X = x) =

x

.
Nn−x

n

que M → ∞ et N − M → ∞.
et notons que N → ∞ et M
N → p = 0 implique
 
u
s!
∼ su , ou us ∼ su! , quand s tend vers l’inﬁni et
En utilisant le fait que (s−u)!
u est ﬁxé ( s et u entiers positifs), on peut écrire :
M x (N − M )n−x n!
x!
(n − x)! N n
   x 
n−x
M
n
M
∼
1−
x
N
N

P (X = x) ∼

et, ainsi, P (X = x) tend vers

n
x

px (1 − p)n−x .

Exercice 4.6
Calculons :
P ((X1 = k) ∩ (X2 = n − k))
P (X1 + X2 = n)
k 
n−k
 
λ2
λ1
n
=
λ1 + λ2
λ1 + λ2
k

P (X1 = k | X1 + X2 = n) =
=

−λ2 n−k
λ2
e−λ1 λk
1 e
k!
(n−k)!

e−(λ1 +λ2 ) (λ1 +λ2 )n
n!

qui est la probabilité pour k de la loi binomiale de paramètres n et

λ1
.
λ1 +λ2

Exercice 4.7
Reprenant la déﬁnition de la loi G(p), on a :
P (X > n) =



p(1 − p)x = p(1 − p)n+1 1 + (1 − p) + (1 − p)2 + · · ·

x≥n+1

= p(1 − p)n+1

1
= (1 − p)n+1 .
1 − (1 − p)

Corrigés des exercices

P(X > n + k | X > n) =

345

P ((X > n + k) ∩ (X > n))
P (X > n)

P (X > n + k)
P (X > n)
(1 − p)n+k+1 .
=
= (1 − p)k
(1 − p)n+1 .

=

qui est indépendant de n et égal à P (X ≥ k). Dans la modélisation en temps
discret d’une durée de vie cela signiﬁe que la probabilité de vivre encore k
unités de temps au-delà d’un temps donné reste constante.

Exercice 4.8
Reprenant la fonction de répartition de la loi U[0, 1] : P (X ≤ u) = u si
u ∈ [0, 1] et vaut 0 si u < 0 ou 1 si u > 1.
P (Y ≤ y) = P ((b − a)X + a ≤ y)
y−a
= P (X ≤
)
b−a
y−a
y−a
qui vaut y−a
b−a si b−a ∈ [0, 1], soit a ≤ y ≤ b, 0 si b−a < 0, soit y ≤ a, et 1 si
y−a
b−a > 1, soit y > b. Cela correspond bien à la fonction de répartition de la loi
U[a, b] déﬁnie en section 4.2.1.

Exercice 4.9
En section 4.2.3 la loi Γ(r, λ) a été déﬁnie pour r entier (r > 0). Pour r,
réel positif, on vériﬁe aisément (par changement de variable u = λx) que la
fonction de x :
λr r−1 −λx
x e
Γ(r)
est bien une fonction de densité, de par la déﬁnition même de Γ(r).
Si elle existe, la fonction génératrice de X ; Γ(r, λ) est :
 +∞
λr r−1 −λx
Ψ(t) =
x e
etx
dx
Γ(r)
0
 +∞
λr
(λ − t)r r−1 −(λ−t)x
x e
dx si t < λ.
=
(λ − t)r 0
Γ(r)

346

Statistique - La théorie et ses applications

Posons u = (λ − t)x, alors :
r  +∞

λ
1 r−1 −u
u e du.
Ψ(t) =
λ−t
Γ(r)
0
r

λ
Comme la fonction intégrée est la densité de la loi Γ(r, 1), on a Ψ(t) = λ−t
.
Si t > λ on pose u = (t − λ) pour constater que la fonction à intégrer
contient eu en lieu et place de e−u et l’intégrale est divergente.

Pour calculer E(X), dérivons Ψ(t) :
Ψ (t) =
puis :

rλr
(λ − t)r+1

E(X) = Ψ (0) =

=⇒

r
,
λ

r(r + 1)λr
r(r + 1)
=⇒ E(X 2 ) = Ψ (0) =
(λ − t)r+2
λ2
r(r+1)  r 2
et V (X) = λ2 − λ = λr2 . Notons que les résultats obtenus sont identiques
à ceux de la section 4.2.3.
Ψ (t) =

Exercice 4.10
Soit la v.a. T dénotant le temps entre une occurrence et la r-ième suivante dans un processus de Poisson d’intensité λ. Alors T ; Γ(r, λ) (voir
section 4.2.3). L’évènement (T ≥ x) est identique au fait qu’il y ait moins de r
occurrences dans un intervalle de temps de longueur x et, donc :
P (T ≥ x) =
FT (x) = 1 −

r−1 −λx

e
(λx)k

k=0
r−1 −λx


e

k=0

k!

(λx)k
k!

où FT (x) est la fonction de répartition de T. En dérivant, on obtient sa fonction
de densité :
fT (x) = λ
=λ

r−1 −λx

e
(λx)k
k=0
r−1

k=0

k!

−λ

e−λx (λx)k
−λ
k!

λr e−λx xr−1
=
(r − 1)!

r−1 −λx

e
(λx)k−1
k=1
r−2

k=0

(x > 0)

qui est bien l’expression donnée en section 4.2.3.

(k − 1)!
e−λx (λx)k
k!

Corrigés des exercices

347

Exercice 4.11
Soit X ; Γ(r, λ), alors, avec des notations évidentes :
x
x
) = FX ( ),
r
r
λ
λ
r − λx
r−1
( r )r e− r x xr−1
1λ e r x
1
x
frX (x) = fX ( ) =
=
r
r
r (r − 1)!rr−1
(r − 1)!

FrX (x) = P (rX ≤ x) = P (X ≤

qui est la densité de la loi Γ(r, λr ). De la même façon, FλX (x) = FX ( λx ),
λr e−x xr−1
e−x xr−1
fλX (x) = λ1 fX ( λx ) = λ1 (r−1)!λ
r−1 =
(r−1)! , densité de la loi Γ(r, 1).

Exercice 4.12
Pour la v.a. X de loi de Pareto de seuil a = 1, on a FX (x) = 1 − x−θ pour
x ≥ 1 et 0 pour x < 1, où θ > 0. Pour Y = ln(X), on a :
FY (y) = P (Y ≤ y) = P (eY ≤ ey ) = P (X ≤ ey ) = 1 − e−θy .
Comme FX (x) est nulle pour x ≤ 1, FY (y) vaut 0 pour y ≤ 0, ce qui restitue
la loi E(θ).

Exercice 4.13
On a un processus de Poisson avec un nombre moyen d’arrivée par seconde
λ = 1/30. Soit X le temps écoulé entre le départ du guichet de la 1-ère personne
et celui de la sixième, alors X ; Γ(5, λ). On cherche à calculer P (X ≤ 30), soit
P (X ≤ λ1 ) ou P (λX ≤ 1). Comme λX ; Γ(5, 1) selon le résultat de l’exercice
 1 1 4 −x
1
4.11, cette probabilité vaut 0 4!
x e dx. Posons In = 0 xn e−x dx, alors, en
intégrant par parties :


1

In = −

n

x d(e
0

−x

)=

−[xn e−x ]10



1

+n

xn−1 e−x dx.

0

1
D’où la relation de récurrence In = nIn−1 −e−1 . En partant de I0 = 0 e−x dx =
1 − e−1 , on trouve I1 = 1 − 2e−1 , I2 = 2 − 5e−1 , I3 = 6 − 16e−1 et I4 =
24 − 65e−1 . Finalement, en divisant I4 par 4 ! on obtient une probabilité d’environ 0,0366. On peut accéder directement au résultat avec EXCEL en évaluant
LOI.GAM M A(30 ; 5 ; 30 ; V RAI).

348

Statistique - La théorie et ses applications

Exercice 4.14
La v.a. Y = X − 1 prend, respectivement, les valeurs 0 et 1 avec probabilités 0,7 et 0,3 et suit donc une loi de Bernoulli B(0, 3). Soit Y1 , Y2 , · · · , Y20
20
des variables aléatoires i.i.d. de cette même loi, alors i=1 Yi suit une loi binomiale B(20 ; 0, 3). Le parking aura une capacité suﬃsante si l’événement
20
20
( i=1 Yi + 20 ≤ 29), soit ( i=1 Yi ≤ 9), se réalise. Par EXCEL on obtient une
probabilité 0,952 pour l’expression LOI.BIN OM IALE(9 ; 20 ; 0, 3 ; V RAI).
Alternativement, par l’approximation gaussienne introduite en section 5.8.3,
on calcule P (S ≤ 9, 5) pour S ; N (20 × 0, 3 ; 20 × 0, 3 × 0, 7), soit P (Z ≤
9,5−6
√
)  P (Z ≤ 1, 71) pour Z ; N (0 ; 1), ce qui vaut 0,956. On notera que
4,2
l’approximation est satisfaisante dans la mesure où 20×0, 3 ≥ 5 et 20×0, 7 ≥ 5.

Exercice 4.15
Selon la déﬁnition donnée en section 4.2.5, X ; LN (μ , σ 2 ) si Y = ln X ;
N (μ , σ 2 ). Ainsi μ et σ 2 sont la moyenne et la variance non pas de X mais de
son logarithme. Pour trouver μ et σ 2 , sachant que E(X) = a et V (X) = b,
il faut résoudre les équations suivantes obtenues à partir des expression s de
E(X) et de V (X) établies en section 4.2.5 :
"
"
"
1 2
σ 2 = ln(1 + ab2 )
μ + 12 σ 2 = ln a
eμ+ 2 σ = a
⇔
⇔
.
2
2
2
e2μ+σ (eσ − 1) = b
eσ − 1 = ab2
μ = ln a − 12 ln(1 + ab2 )
Ici a = 70 et b = (12)2 ce qui donne μ = 4, 234 et σ 2 = 0, 02896.
Dès lors, on peut calculer des probabilités selon le modèle retenu, par
) = P (Z ≤ 0, 870), où
exemple P (X ≤ 80) = P (Y ≤ ln 80) = P (Z ≤ ln√80−4,234
0,02896
Z ; N (0 ; 1), soit une probabilité de 0,808.

Corrigés des exercices

349

Chapitre 5 : Lois fondamentales
de l’échantillonnage
Exercice 5.1
n
La v.a. T = i=1 Xi est de loi Γ(n, λ) (voir section 4.2.3). Soit FX (x) la
fonction de répartition de X :
FX (x) = P (X ≤ x) = P (T ≤ nx) = FT (nx).
Passons aux fonctions de densité :
fX (x) = nfT (nx) =

nλn
(nλ)n n−1 −(λn)x
e
(nx)n−1 e−λ(nx) =
x
(n − 1)!
(n − 1)!

qui est la densité de la loi Γ(n, nλ). Note : On peut aussi utiliser les fonctions
génératrices sur le modèle de l’exercice suivant.

Exercice 5.2
n
n
Pour T = i=1 Xi , on a la fonction génératrice ΨT (t) = [ΨX (t)] où ΨX (t)
est la fonction génératrice de la loi Γ(r, λ) (voir proposition 3.12). D’où ΨT (t) =
λ
λ−t

nr

. Or :

T
t
t
ΨX (t) = Ψ T (t) = E(e n t ) = E(eT n ) = ΨT ( ) =
n
n



λ
λ−

nr
t
n


=

nλ
nλ − t

nr

qui est la fonction génératrice de la loi Γ(nr, nλ).

Exercice 5.3
Calculons la fonction caractéristique (voir note 2.4) de la v.a. X qui suit la
loi de Cauchy déﬁnie lors de l’exemple 2.1. :
ΦX (t) = E(eitX ) = E(cos tX) + iE(sin tX)



1 +∞ sin tx
2 +∞ cos tx
1 +∞ cos tx
dx
+
i
dx
=
dx = e−|t| ,
=
π −∞ 1 + x2
π −∞ 1 + x2
π 0
1 + x2

350

Statistique - La théorie et ses applications


*n
*n
|t|
1
d’où ΦX (t) = E e n Xi . t = i=1 ΦXi ( nt ) = i=1 e− n = e−|t| . La loi de
Cauchy n’ayant pas de moyenne (ni a fortiori de variance), la loi des grands
nombres ne s’applique pas.

Exercice 5.4
n
La statistique (n−1)S 2 = i=1 (Xi −X)2 peut s’écrire (voir la démonstration
n
n
2
2
de la proposition 5.2)
i=1 (Xi − μ) − n(X − μ) ou encore
i=1 (Xi −

2
n
μ)2 − n1 [ i=1 (Xi − μ)] . Soit Zi = Xi − μ. On a E(Zi ) = 0, E(Zi2 ) = σ 2
et E(Zi4 ) = μ4 , où σ 2 et μ4 sont, respectivement, les deuxième et quatrième
moments centrés de la loi mère de l’échantillon aléatoire considéré. Les Zi sont
i.i.d. Par substitution et développement du carré d’une somme, on obtient :
2

(n − 1)S =

n


Zi2

i=1


 n
n

1  2
n−1  2 2
−
Zi + 2
Zi Zj =
Zi −
Zi Zj
n i=1
n i=1
n i<j
i<j


2

2
n
n−1  2 2
Zi −
Zi Zj
n i=1
n i<j
2 

n
 2 2
n−1
E
Zk4 + 2
Zi Zj
=
n
i<j
k=1
 n



2

 2 
4(n − 1)
4
E
Zk
Zi Zj
Zi Zj
+ 2E
−
n2
n
i<j
i<j

(n − 1)2 E( S 2 ) = E

k=1

2

Le premier terme vaut n−1
(nμ4 + n(n − 1)σ 4 ), le deuxième est nul car
n
il contient soit des termes en E(Zi Zj Zk2 ) avec i = j = k, soit des termes en
E(Zj Zk3 ) avec j = k. Pour le dernier terme les produits croisés sont nuls car ils
contiennent soit des termes en E(Zi2 Zj Zk ) avec i = j = k, soit en E(Zi Zj Zk Zl )
 2 2
avec i = j = k = l. Il vaut donc n42 E(
Zi Zj ) = 2(n−1)
σ 4 . Finalement :
n
i<j

(n − 1)2 
(n − 1)3 4
(n − 1) 4
μ4 +
σ +2
σ − (n − 1)2 E(S 2 )
n
n
n
(n − 1)2 
(n − 1)(3 − n) 4
μ4 +
σ
=
n
n

(n − 1)2 V (S 2 ) =

V (S 2 ) =
Pour la loi de Gauss, μ4 =

2σ 4
n−1 .

2

1 
n−3 4
(μ −
σ ).
n 4 n−1

4!
4
4×2! σ

1
n−3 4
4
n (3σ − n−1 σ ) =
2
(n−1)S
suit une loi du
σ2

= 3σ 4 , donc V (S 2 ) =

Note : On peut retrouver ce résultat sachant que

Corrigés des exercices

351

khi-deux à n − 1 degrés de liberté dont la variance est égale à 2(n − 1). D’où
(n−1)2 V (S 2 )
2σ 4
= 2(n − 1) et V (S 2 ) = n−1
.
σ4

Exercice 5.5
Quel que soit i les valeurs possibles pour Xi sont a1 , a2 , . . . , aN et sa loi
marginale est P (Xi = aj ) = N1 pour j = 1, 2, . . . , N.
La loi conjointe de Xi et Xk , i = k, est P (Xi = aj , Xk = al ) = N (N1−1) , car
il y a N (N −1) arrangements pour les (aj , al ), tous équiprobables par symétrie.
Il résulte de cela que :
E(Xi ) =

N
N
n
1 
1 
1
aj = μ, V (Xi ) =
(aj − μ)2 = σ 2 , E(X) =
μ = μ.
N
N
n
j=1

j=1

i=1

⎡
⎤



1
1
⎣(
E(Xi Xk ) =
aj al =
aj )2 −
a2j ⎦ ,
N (N − 1)
N (N − 1)
j
j
j =l

d’où E(Xi Xk ) =
2

N 2 μ2 − (N σ 2 + N μ2 ) et cov(Xi , Xk ) = E(Xi Xk )−

1
N (N −1)

μ2 = − Nσ−1 .
n
n

Puis V ( i=1 Xi ) =
i=1 V (Xi ) + 2
i<k cov(Xi , Xk ) et, comme il y a
n(n−1)
termes de covariance ici,
2
V(

n


Xi ) = nσ 2 − 2

i=1

d’où V (X) =

1
n2 V

n

(

i=1

Xi ) =

n(n − 1) σ 2
N −n
= nσ 2
,
2
N −1
N −1

σ2 N −n
n N −1 .

Exercice 5.6
Soit Zi = Xi − μ, alors X̄ − μ = Z, E(Z) = 0 et
n
n
2
2
2
i=1 (Zi − Z) =
i=1 Zi − nZ . Donc :

n

i=1 (Xi

− X̄)2 =




 n
 2
1
2
cov(X̄, S ) = cov(X̄ − μ, S ) = cov Z,
Zi − nZ
n − 1 i=1
   n


  n
 2
 2
1
1
2
=
E Z
=
Zi − nZ
Zi
E Z
n−1
n−1
i=1
i=1
2

2


3

− nE(Z ) ,

352

Statistique - La théorie et ses applications

 n

n
n
n

or E Z( i=1 Zi2 ) = n1 E(( j=1 Zj )( i=1 Zi2 )) = n1 E( i=1 Zi3 ) = μ3
,
+
n
n
n
n
3
1
1 
3
et E(Z ) = n13 E([ i=1 Zi ]
j=1 Zj [
k=1 Zk ]) = n3 E(
i=1 Zi ) = n2 μ3 .
D’où :




μ3
1
1 
2
=
μ
μ
.
−
cov(X̄, S ) =
n−1 3 n 3
n

Exercice 5.7
Soit Z ; N (0 ; 1) et, donc, Z 2 ; χ2 (1) de fonction de répartition FZ 2 et de
densité fZ 2 Alors :
√
√
√
FZ 2 (x) = P (Z 2 ≤ x) = P (− x < Z < x) = 2Φ( x) − 1 si x > 0 (0 sinon).
En dérivant :
√
x
2
1
fZ 2 (x) = √ Φ ( x) = √ √ e− 2 si x > 0 (0 sinon).
2 x
x 2π

Exercice 5.8
La fonction génératrice de la loi χ2 (ν) est Ψ(t) = (1 − 2t)− 2 (voir sec− ν −1
tion 5.3). Sa moyenne est donc μ = Ψ (0). Comme Ψ (t) = ν(1 − 2t) 2
on a μ = ν. Le moment simple d’ordre 2 est μ2 = Ψ (0) avec Ψ (t) =
ν
ν(ν + 2)(1 − 2t)− 2 −2 , soit μ2 = ν(ν + 2). La variance est le moment centré
d’ordre 2 : μ2 = μ2 − μ2 = 2ν.
ν

Exercice 5.9

1.

∞



g(x)fX (x)dx ≥ A g(x)fX (x)dx > k A fX (x)dx.

Donc E(g(X)) > k A fX (x)dx.

Mais A fX (x)dx = P (X ∈ A) = P (g(X) > k). D’où :
−∞

E(g(X)) ≥ kP (g(X) > k).
2. En prenant g(x) = (x − μ)2 et en posant ε2 = k, ε > 0, on a :
σ 2 = E((X − μ)2 ) ≥ ε2 P ((X − μ)2 ) ≥ ε2 ) = ε2 P (|X − μ| > ε)
ce qui démontre l’inégalité de Tchebichev.

Corrigés des exercices

353

3. Posons X = Yn − Y. Alors E((Yn − Y )2 ) = E(X 2 ) ≥ ε2 P (|X| > ε) =
ε2 P (|Yn − Y | > ε) pour tout ε > 0 donné.
m.q.

Yn −−−→ Y équivaut, par déﬁnition, à lim E((Yn − Y )2 ) → 0. Donc
n→∞

lim P (|Yn − Y | > ε) → 0 ou lim P (|Yn − Y | < ε) → 1 ce qui déﬁnit

n→∞
p
Yn →

n→∞

Y.

Exercice 5.10
Soit X1 , · · · , Xn de loi mère de moyenne μ et de variance σ 2 . Appliquons
2
l’inégalité de Tchebichev à X n de moyenne μ et variance σn :
σ2
pour tout ε > 0 ﬁxé.
nε2
Donc lim P (|X n − μ| > ε) → 0 ⇐⇒
lim P (|X n − μ| < ε) → 1
P (|X n − μ| > ε) ≤

n→∞

n→∞

p

soit, par déﬁnition, X n → μ (convergence faible).

Exercice 5.11
La médiane de la loi mère est le nombre M = F −1 ( 12 ) où F est sa fonction
de répartition. La fonction de répartition du maximum de l’échantillon X(n) est
[F (x)]n en x (voir section 5.6), d’où :


1
1
1
−1 1
P X(n) ≤ F ( ) = [F (F −1 ( ))]n = n =⇒ P (X(n) > M ) = 1 − n .
2
2
2
2


De même P X(n) ≤ F −1 ( 34 ) = ( 34 )n et la probabilité que le maximum dépasse
le troisième quartile de la loi mère est donc 1 − ( 34 )n . Notons que dans les deux
cas la probabilité tend vers 1 quand la taille de l’échantillon croı̂t vers l’inﬁni.

Exercice 5.12
Pour la loi U[0 , 1] on a F (x) = x pour x ∈ [0 , 1]. Pour le minimum X(1) de
l’échantillon de taille n la fonction de répartition est :
FX(1) (x) = 1 − [1 − F (x)]n
= 1 − (1 − x)

n

(voir section 5.6)
pour x ∈ [0 , 1]

354

Statistique - La théorie et ses applications

et la fonction de densité est, par dérivation, fX(1) (x) = n(1 − x)n−1 pour
x ∈ [0 , 1] et 0 sinon. D’où :


1

x(1 − x)n−1 dx et avec x(1 − x)n−1 = (1 − x)n−1 − (1 − x)n ,
0

 1
 1
n−1
n
(1 − x)
dx −
(1 − x) dx
soit en posant t = 1 − x,
=n
0
0 

 n+1 1
1
1
1
tn
t
1
)=
.
=n
−
= n( −
n 0
n+1 0
n n+1
n+1

E(X(1) ) = n

Exercice 5.13
Reprenons le corrigé de l’exercice 3.7 qui donne E(X) = 2p − 1, V (X) =
n
4p(1 − p), E(Y ) = n(2p − 1), V (Y ) = 4np(1 − p) où Y = i=1 Xi . Pour n
grand, Y suit approximativement une loi normale N (n(2p − 1) ; 4np(1 − p)),
donc :
P (Y > |x|) = P (−x < Y < x)

−x − n(2p − 1)
x − n(2p − 1)
%
P
<Z< %
2 np(1 − p)
2 np(1 − p)


x − n(2p − 1)
−x − n(2p − 1)
%
%
=Φ
−Φ
2 np(1 − p)
2 np(1 − p)

où Z ; N (0 ; 1)

où Φ est la fonction de répartition de la loi normale centrée-réduite donnée en
section 4.2.4.

Exercices appliqués
Exercice 5.14
Soit X le niveau de bruit d’une machine prise au hasard et X 10 la moyenne d’un
échantillon aléatoire de taille 10. On suppose que l’approximation gaussienne
s’applique avec n = 10 (ce qui est réaliste s’agissant d’une mesure physique
dans un processus de fabrication). On a :
X 10

;

approx

N (44 ;

52
),
10

Corrigés des exercices

355

48−44
√ ) = P (Z > 2, 53) = 1 − 0, 9943 = 0, 0057, où
d’où P (X 10 > 48)  P (Z > 5/
10
Z ; N (0 ; 1). On note que cette probabilité est très faible alors que pour une
) soit environ 0, 21.
seule machine elle serait P (Z > 48−44
5

Exercice 5.15
t=

6, 42 − 6, 30
√
= 2, 99.
0, 22/ 30

Pour T, v.a. de Student à 29 degrés de liberté, P (T < 2, 99) = 0, 9972, valeur
obtenue dans EXCEL par 1 − LOI.ST U DEN T (2, 99 ; 29 ; 1). On peut vériﬁer
grossièrement l’ordre de grandeur dans la table fournie dans cet ouvrage. La valeur observée correspond au quantile 0,997 ce qui est extrême sur la distribution
des valeurs observables et rend l’indication du constructeur peu plausible.

Exercice 5.16
Soit X le poids d’une personne prise au hasard et X 100 la moyenne d’un
échantillon aléatoire de taille 100. L’approximation gaussienne s’applique sans
problème. On a :


(15, 6)2
X 100 ; N 66, 3 ;
,
approx
100
000
)  P (Z >
d’où P (X 100 > 7100
0, 0089, où Z ; N (0 ; 1).

70−66,3
√
)
15,6/ 100

= P (Z > 2, 37) = 1 − 0, 9911 =

Exercice 5.17
On suppose que les 1000 personnes sont choisies au hasard dans la population française. Le taux de sondage étant très faible (voir section 3.7), on peut
assimiler ce sondage à un sondage avec remise. À chaque tirage la probabilité
d’obtenir une personne favorable est 0,44 et, donc, on a :
Sn ; B(1 000 ; 0, 44)
d’où P (Sn ≤ 420)  P (U ≤ 420, 5) où U ; N (440 ; 246, 4).
√
Soit P (Sn ≤ 420)  P (Z ≤ 420,5−440
) = P (Z ≤ −1, 24) = 1 − 0, 8925  0, 11
246,4
où Z ; N (0 ; 1).

356

Statistique - La théorie et ses applications

Exercice 5.18
Soit Sn le nombre de pièces défectueuses parmi 120 pièces sélectionnées.
On a :
Sn ; B(120 ; 0, 09)
d’où P (Sn ≤ 22)  P (U ≤ 22, 5) où U ; N (10, 8 ; 9, 828). Soit
√
) = P (Z ≤ 3, 73), où Z ; N (0 ; 1). Cette
P (Sn ≤ 22)  P (Z ≤ 22,5−10,8
9,828
probabilité est supérieure à 0,9995 comme on peut le voir dans la table (plus
exactement elle est égale à 0,99990 selon EXCEL). Cette valeur est très extrême
sur la distribution théorique et incite à conclure avec quasi certitude que le
fonctionnement est anormal.

Exercice 5.19
Cet exercice, comme d’ailleurs d’autres ci-dessus, préﬁgure la démarche d’un
test statistique. En arrière-plan de l’énoncé, on peut supposer que l’on a observé un écart-type de 0,077 sur cinq mesures faites sur le même échantillon
de sang − soit une valeur de précision correspondante de 0,154 mg/l − et l’on
souhaite savoir si cette valeur observée est plausible au regard de la distribution
théorique de S considérant ce que l’on sait de la méthode de mesure.
Soit donc S l’écart-type d’un échantillon gaussien de taille 5. On s’intéresse
4S 2
à l’événement (S > 0, 077) qui équivaut à (S 2 > 0, 005929) ou ( (0,05)
2 > 9, 49).
2

2

(n−1)S
4S
qui, selon le théorème 5.1, suit
La statistique (0,05)
2 correspond ici à
σ2
une loi χ2 (4). On lit dans EXCEL (LOI.KHIDEUX(9,49 ; 4)) une probabilité
de 0,05 ce qui n’est pas très plausible, sans plus (quantile 0,95). Notons au
passage qu’EXCEL n’a pas de cohérence pour les probabilités restituées par les
lois et qu’il y a lieu de vériﬁer avec un exemple de calcul à quoi correspond la
probabilité donnée pour telle loi en fonction de x (plus grand que, plus petit
que, autre ?).

Exercice 5.20
On suppose que les 10 000 ménages ont des comportements indépendants
ce qui donne lieu à une suite de variables aléatoires i.i.d. de loi P(4). Leur total

Corrigés des exercices

357

T suit une loi P(40 000) dont la moyenne est E(T ) = 40 000 et V (T ) = 40 000.
Cette loi peut être précisément approchée par une loi N (40 000 ; 40 000).
Comme P (−1, 96 < Z < 1, 96) = 0, 95 où Z ; N (0 ; 1), de même P (40 000−
√
√
1, 96 40 000 < U < 40 000 + 1, 96 40 000) = 0, 95 où U ; N (40 000 ; 40 000),
soit P (39 608 < U < 40 392) = 0, 95. Donc, en négligeant toute correction de
continuité et en arrondissant, un intervalle de probabilité 0,95 de [39 600, 40 400].

Exercice 5.21
Pour une seule opération l’erreur d’arrondi est une variable aléatoire
1
X ; U[− 12 , + 12 ] d’où E(X) = 0 et V (X) = 12
. Pour T, l’erreur d’arrondi to000
000
et T ; N (0 ; 1 12
).
tale sur 1 000 opérations, on a E(T ) = 0, V (T ) = 1 12
approx

Comme P (−1, 96 < Z < 1, 96) = 0, 95 où Z ; N (0 ; 1), de même :
)
)
1 000
1 000
< T < 0 + 1, 96
)  0, 95
P (0 − 1, 96
12
12
soit approximativement un intervalle [−18, +18] en centimes d’euros.

Exercice 5.22
Rappelons que le paramètre λ de la loi E(λ) est l’inverse de sa moyenne.
Pour un accumulateur quelconque, on a ainsi une durée de vie X ; E( 12 ) dont
1
la fonction de répartition en x est FX (x) = 1 − e− 2 x pour x ≥ 0. L’appareil fonctionne si et seulement si, avec des notations évidentes, l’événement
(X(1) ≥ x) est réalisé, où X(1) = min{X1 , X2 , X3 }. Selon la loi générale du minimum d’un échantillon explicitée en section 5.6, la fonction de répartition de
1
3
X(1) est, au point x ≥ 0, FX(1) (x) = 1−(e− 2 x )3 = 1−e− 2 x laquelle correspond
à la loi E( 32 ).
Donc E(X(1) ) =

2
3

3

et P (X(1) > 1) = e− 2  0, 22.

Exercice 5.23
On doit envisager un processus de Bernoulli avec probabilité de succès 0,9,
la variable d’intérêt Y étant le nombre d’essais pour arriver à 100 succès.

358

Statistique - La théorie et ses applications

La loi de Y est la loi binomiale négative sous la deuxième forme déﬁnie en
ﬁn de section 4.1.4, avec paramètres r = 100 et p = 0, 9. Alors Y prend ses
 12, 35.
valeurs dans {100, 101, · · · }, E(Y ) = pr  111, 1 et V (Y ) = r(1−p)
p2
Comme on a vu dans la dite section qu’une loi BN (r, p) peut être envisagée
comme une somme de r variables aléatoires i.i.d. de loi G(p), le théorème central
limite s’applique et la loi binomiale négative peut être approchée par une loi
de Gauss pourvu que r soit suﬃsamment grand, ce qui est le cas ici car r est
largement supérieur à 30. Ainsi, en utilisant la correction de continuité :
P (Y > 111)  P (U > 111, 5) où U ; N (111, 1 ; 12, 35)
111, 5 − 111, 1
√
 P (Z >
) = P (Z > 0, 11) = 1 − 0, 5438  0, 46.
12, 35
Pour être sûr de fabriquer 100 pièces bonnes, il faudrait prévoir de fabriquer
une inﬁnité de pièces !
Pour atteindre 100 bonnes pièces avec probabilité 0, 99 il faut fabriquer n
pièces où n est tel que P (Y ≤ n) = 0, 99 ou plus correctement P (Y ≤ n) ≥ 0, 99
puisque nous sommes sur une loi discrète. Or P (Z < 2, 33) = 0, 99, donc
√
P (U < 111, 1 + 2, 33 12, 35) = 0, 99, soit P (U < 119, 29) = 0, 99. Comme
119, 29 < 119, 5 on prendra n = 120 pièces.

Corrigés des exercices

359

Chapitre 6 : Théorie de l’estimation
paramétrique ponctuelle
Exercice 6.1
Il s’agit de voir si ces lois à un paramètre (inconnu), disons θ, ont une fonction de probabilité, respectivement fonction de densité de probabilité, pouvant
se mettre sous la forme (voir section 6.3) :
p(x; θ) = a(θ)b(x) exp{c(θ)d(x)}
où a, b, c, d sont des fonctions.
Loi BN (r, p) avec r connu
Fonction de probabilité :

r+x−1 r
p (1 − p)x , x ∈ N
p(x; p) =
x


r+x−1 r
p exp{x ln(1 − p)}
=
x


forme classe exponentielle avec a(p) = pr , b(x) = r+x−1
, c(p) = ln(1 − p) et
x
d(x) = x.


Loi P (λ)
Fonction de probabilité :
p(x; λ) =

e−λ λx
= e−λ
x!



1
x!


exp{x ln λ}

forme classe exponentielle avec a(λ) = e−λ , b(x) =

1
x! ,

c(λ) = ln λ et d(x) = x.

Loi E(λ)
Fonction de densité de probabilité :
f (x; λ) = λe−λx = λ exp{−λx}
forme classe exponentielle avec a(λ) = λ, b(x) = 1, c(λ) = −λ et d(x) = x.

360

Statistique - La théorie et ses applications

Loi Γ(r, λ) avec r connu
Fonction de densité de probabilité :
λr r−1 −λx
x e
=
f (x; λ) =
Γ(r)



forme classe exponentielle avec a(λ) =
d(x) = x.

λr
Γ(r)
λr
Γ(r)


(xr−1 ) exp{−λx}
, b(x) = xr−1 , c(λ) = −λ et

Exercice 6.2
f (x; θ) = θaθ x−(θ+1) I[a,+∞[ (x) = (θaθ )I[a,+∞[ (x) exp{−(θ + 1) ln x},
de forme classe exponentielle avec a(θ) = θaθ , b(x) = I[a,+∞[ (x), c(θ) =
−(θ + 1) et d(x) = ln x.

Donc ni=1 ln Xi est statistique exhaustive minimale.
θa
(voir section 4.2.6), l’estimateur des moments θM est tel
Comme μ = θ−1
M

que X = θ a , soit θM = X . On constate qu’il n’est pas fonction de la
θM −1

X−a

statistique exhaustive minimale ci-dessus (et en ce sens il ne saurait être un
des plus pertinents).

Exercice 6.3
Γ(α + β + 2)
xα (1 − x)β I[0,1] (x)
Γ(α + 1)Γ(β + 2)
Γ(α + β + 2)
I]0,1[ (x) exp{α ln x + β ln(1 − x)},
=
Γ(α + 1)Γ(β + 1)

f (x; α, β) =

de forme classe exponentielle avec d1 (x) = ln x et d2 (x) = ln(1 − x). Donc le
n
n
couple ( i=1 ln Xi , i=1 ln(1 − Xi )) est statistique exhaustive minimale.
Pour l’estimateur des moments du couple (α, β), on utilise le fait que (voir
(α+1)(β+1)
(α+1)2
α+1
section 4.2.8) E(X) = α+β+2
et E(X 2 ) = (α+β+2)
2 (α+β+3) + (α+β+2)2 =
(α+1)(α+2)
(α+β+2)(α+β+3) .

"

Il s’agit donc de résoudre en (α, β) le système :

α+1
α+β+2
(α+1)(α+2)
(α+β+2)(α+β+3)

=X
n
⇔
= n1 i=1 Xi2

"

α+1
α+β+2 = X
n
α+2
1
i=1
α+β+3 X = n

Xi2

,

Corrigés des exercices

361

soit deux équations linéaires en α et β, dont la solution in ﬁne est :
X (1 − X) − S'2 (1 + X)
S'2
2

α
M =

X − (2 − X)(S'2 + X )
βM =
.
S'2
2

et

On constate qu’elle n’est pas fonction de la statistique exhaustive minimale
ci-dessus (et en ce sens elle ne saurait être une des plus pertinentes).
Exercice 6.4
La fonction de répartition de S en s est P (S ≤ s) = P (S 2 ≤ s2 ) =
2
2
2
P (n−1)S
= Fχ2 (n−1) (n−1)s
(selon le théorème 5.1) où Fχ2 (n−1)
≤ (n−1)s
σ2
σ2
σ2
est la fonction de répartition d’une loi du khi-deux à (n − 1) degrés de li2
berté. La fonction de densité de S est donc : 2(n−1)s
que
fχ2 (n−1) (n−1)s
σ2
σ2
l’on peut écrire explicitement en substituant (proposition 5.5) fχ2 (n−1) (x) =
−1 (n−3)/2 −x/2
[2(n−1)/2 Γ( n−1
x
e
, avec x > 0. On peut calculer directement
2 )]
E(S) par :
σ
E
E(S) = √
n−1



(n − 1)S 2
σ2

 12

= √

1
σ
)
n − 1 2(n−1)/2 Γ( n−1
2



+∞
0

1

x2 x

n−3
2

x

e− 2 dx

où l’intégrale n’est autre que la densité de la loi du khi-deux à n degrés de
liberté au facteur [2n/2 Γ( n2 )]−1 près. D’où :
)

E(S) = σ

Γ( n2 )
2
.
n − 1 Γ( n−1
)
2

(
Γ( n−1
2 )
S.
Pour éliminer le biais, il suﬃt de prendre, comme estimateur de σ, n−1
2
Γ( n )
2

Exercice 6.5
1. Soit n = 2k − 1, la médiane est la statistique d’ordre k, notée X(k) . Sa
fonction de répartition (voir proposition 5.12) est :
n  
G(x; θ) = j=k nj [F (x − θ)]j [1 − F (x − θ)]n−j . Sa densité est donc :
 

g(x; θ) = nj=k nj f (x − θ){ j[F (x − θ)]j−1 [1 − F (x − θ)]n−j
−(n − j)[F (x − θ)]j [1 − F (x − θ)]n−j−1 }
n  
g(x; θ) = j=k nj f (x − θ)[F (x − θ)]j−1 [1 − F (x − θ)]n−j−1 [j − nF (x − θ)].
2. Notons que f (−t) = f (t) équivaut à F (−t) = 1 − F (t). Soit x1 = θ + h et
x2 = θ − h. Il suﬃt de montrer que G(x2 ; θ) = 1 − G(x1 ; θ). Posons F (h) = a.

362

Statistique - La théorie et ses applications

Ainsi :
G(x1 ; θ) =

n  

n
j=k

G(x2 ; θ) =

n  

n
j=k

=

aj (1 − a)n−j

j

j

[F (−h)] [1 − F (−h)]

n−k

s=0

j

n−j



n
(1 − a)n−s as =
n−s

=

n  

n
j=k

k−1


j

(1 − a)j an−j



n s
a (1 − a)n−s
s

s=0

en posant s = n − j (et comme n = 2k − 1).
 

Donc G(x1 ; θ) + G(x2 ; θ) = nt=1 nt at (1 − a)n−t = 1.

 +∞
3. Soit M la médiane empirique, alors E(M − θ) = −∞ (x − θ)g(x; θ)dx =
 +∞
tg(t + θ; θ)dt = 0 car g(−t + θ; θ) = g(t + θ; θ). Notons qu’il est toutefois
−∞
 +∞
nécesssaire que ces intégrales existent, ce qui revient à ce que −∞ xf (x − θ)dx
existe, soit que la loi mère ait une espérance mathématique.

Exercice 6.6
X est sans biais pour λ1 puisque E(X)= λ1 ; V (X) = nλ1 2 ; eqm λ1 (X) = nλ1 2 .
n
1
n
n 1
1 1
T = n+1
i=1 Xi = n+1 X, donc E(T ) = n+1 λ et T a un biais − n+1 λ ;
n
1
V (T ) = (n+1)2 λ2 .
1
1
n
1
1
D’où eqm λ1 (T ) = (n+1)
2 λ2 + (n+1)2 λ2 = (n+1)2 λ2 < eqm 1 (X). En erreur
λ
quadratique moyenne T est meilleur, le gain de variance étant supérieur à la
perte due au biais.

Exercice 6.7


4
Comme vu à l’exercice 5.4, V (S 2 ) = n1 (μ4 − n−3
n−1 σ ) et est sans biais donc son
e.q.m. est V (S 2 ).

(n−1)2
(n−1)2
σ2
n−3 4
2
2
'2
S'2 = n−1
σ )
n S et a un biais − n ; V (S ) =
n2 + V (S ) =
n3 (μ4 − n−1
,
2


(n−1)
1
n−3
n−3
σ4
σ 4 )−
(μ4 −
σ 4 ) + 2 > 0,
S'2 domine S 2 en e.q.m. si (μ4 −
3

soit


2n−1
n (μ4

−

n−3 4
n−1 σ )


>

n
σ4
n2 ou,

n−1

n



ﬁnalement, μ4 >

n−1
3n2 −8n+3
(2n−1)(n−1) σ4.

n

Pour la loi de Gauss μ4 = 3σ4 et l’on peut aisément vériﬁer que cette condition
est remplie pour tout n > 1. Si l’on s’en tient au critère de l’e.q.m., on doit
préférer S'2 .

Corrigés des exercices

363

Exercice 6.8
Soit pour X ; U[0, θ] la fonction de répartition FX (x; θ) = xθ si x ∈ [0, θ]. Donc
 n
pour un échantillon de taille n, FX(n) (x; θ) = [FX (x; θ)]n = xθ
si x ∈ [0, θ].


 θ−ε n


qui tend vers
Pour tout ε > 0, P ( X(n) − θ > ε) = P (X(n) < θ − ε) = θ


0 quand n → ∞. Ainsi pour tout ε > 0, P (X(n) − θ < ε) → 1 quand n → ∞
ce qui déﬁnit la convergence en probabilité de X(n) vers θ (déﬁnition 5.14).

Exercice 6.9
L’estimateur UMVUE est T =  n−1
(voir exemple 6.11). Posons Tn =
n
i=1 Xi
n
1
λ
i=1 Xi . Suivant le même argument que pour démontrer E( Tn ) = n−1 dans
l’exemple 6.11, on a, à condition que n > 2 :
 +∞
1
1
λ
E( 2 ) =
(λt)n−1 e−λt dt
2 (n − 1)!
Tn
t
0
 +∞
λ2
λ
λ2
=
(λt)n−3 e−λt dt =
.
(n − 1)(n − 2) 0
(n − 3)!
(n − 1)(n − 2)
2

2

λ
λ
− (n−1)
Donc V ( T1n ) = (n−1)(n−2)
2 =
eqmλ (T ) pour l’estimateur UMVUE.

L’estimateur des moments est
biais

λ
;
n−1

V

eqmλ (

( X1 )

=

n
n−1

2

λ2
(n−2)

1
X

=

λ2
,
(n−1)2 (n−2)

n
n−1 T,

d’où V (T ) =

1
d ’où E( X
) =

n
n−1 λ

λ2
(n−2)

=

avec un

et :

1
1
n2
n2 + n − 2
λ2 =
λ2 .
)=
λ2 +
2
2
(n − 1)
(n − 1) (n − 2)
(n − 1)2 (n − 2)
X
2

+n−2
2
2
= 3(n − 1). Non
Or, pour tout n > 2 , n(n−1)
2 > 1 car n + n − 2 − (n − 1)
seulement l’estimateur des moments est dominé en e.q.m., mais en plus il est
biaisé.

Exercice 6.10
De toute évidence, la famille est dans la classe exponentielle, avec d(x) = x2 .
n
n
Ainsi i=1 d(Xi ) = i=1 Xi2 est statistique exhaustive minimale.
 +∞ x3 − x2
 +∞ −t
x2
2
2θ dx, soit, en posant t =
te dt.
E(X 2 ) = 0
θ e
2θ , E(X ) = 2θ 0
Or l’intégrale ci-dessus est la moyenne de la loi E(1) et vaut donc 1. Donc

364

Statistique - La théorie et ses applications

n
n
1
1
2
2
E(X 2 ) = 2θ et E( 2n
i=1 Xi ) = θ. En vertu de la proposition 6.7, 2n
i=1 Xi
n
− sans biais et fonction linéaire de i=1 d(Xi ) − est eﬃcace. On peut dire aussi
que cet estimateur de θ est UMVUE.

Exercice 6.11


 a θ
x
1. P (ln X
si aex ≥ a, soit x ≥ 0 (voir fonca ) < x = P (X < ae ) = 1 − aex


)<x =
tion de répartition de la loi de Pareto en section 4.2.6). Ainsi P (ln X
a
1 − e−θx pour x ≥ 0, qui est la fonction de répartition de la loi E(θ).
2. La densité de la loi de Pareto est :
f (x; θ) = θaθ x−(θ+1) I[a,+∞[ (x) = θaθ I[a,+∞[ (x) exp{−(θ + 1) ln x},
qui met en évidence la forme de la classe exponentielle avec d(x) = ln x.
n
Posons Yi = ln( Xai ), alors i=1 Yi ; Γ(n, θ) puisque Yi ; E(θ).
Selon l’exemple 6.11, E

 n1

i=1

Yi

=

θ
n−1 ,

d’où E

n−1
n
i=1 Yi

= θ. La statistique

n−1
n−1
= n
n
Xi
i=1 ln Xi − n ln a
i=1 ln( a )
n
est une fonction de i=1 ln Xi sans biais pour θ. D’après la proposition 6.6
c’est la statistique UMVUE pour estimer θ.
n
3. Selon la proposition 6.7, seul i=1 d(Xi ) est eﬃcace pour estimer h(θ) =
n
1
Eθ ( i=1 d(Xi )) et ceci à une fonction linéaire près. Or E(ln( X
a )) = θ , la


n
n
moyenne de la loi E(θ). Ainsi E( n1 i=1 ln( Xai )) = 1θ et n1 i=1 ln( Xai ) est sans
1
biais et eﬃcace pour estimer θ .

Exercice 6.12
1. La densité est :
f (x; θ) = θaθ x−(θ+1) I[a,+∞[ (x) qui ne peut être mis sous la forme de la
classe exponentielle en raison de I[a,+∞[ (x).
2. L’estimateur des moments pour a est solution de
X.
4.2.6, si θ > 1), soit 
aM = θ−1
θ

aθ
θ−1

= X (voir section

3. Utilisons le théorème 6.1 :
n
$
i=1


n nθ

f (xi ; a) = θ a

n
$
i=1

xi

−(θ+1) n
$
i=1

I[a,+∞[ (xi ),

Corrigés des exercices
or

*n

i=1 I[a,+∞[ (xi )
n
$

=

*n

i=1 I[a,+∞[ (x(1) )

nθ

f (xi ; a) = a

i=1

n
$

365

où x(1) = min{x1 , · · · , xn }. Donc :


I[a,+∞[ (x(1) )

i=1

n
$

−(θ+1)

θn

xi

i=1

et d’après le théorème X(1) est exhaustive. Elle est minimale du fait qu’elle est
de dimension 1.
 nθ
4. La fonction de répartition de X(1) est 1 − [1 − F (x; a)]n = 1 − xa
si x ≥ a.
Donc X(1) suit une loi de Pareto de paramètre de seuil a et de paramètre de




nθ
1
forme nθ, d’où E X(1) = nθ−1
a, E nθ−1
nθ X(1) = a et (1 − nθ )X(1) est un
estimateur sans biais pour a. Notons qu’il n’est pas pour autant UMVUE car
on n’est pas dans la classe exponentielle.

Exercice 6.13
1. Pour la famille des lois N (μ , 1), μ ∈ R, on a la densité :
f (x; μ) =

√1
2π

2

exp{− 12 (x − μ) }. Calculons :

ln f (x; μ) = − 12 ln(2π) −
ϑ2
ϑμ2

2

ϑ
(x − μ) , ϑμ
ln f (x; μ) = x − μ,
+
,
ϑ2
ln f (x; μ) = −1, I(μ) = E − ϑμ2 ln f (X; μ) = 1.
1
2

1
nI(μ)

La borne de Cramer-Rao est
que V (X) = n1 , X est eﬃcace.

=

1
.
n

Comme X est sans biais pour μ et

2. Pour la famille des lois N (0 , σ 2 ), σ 2 > 0, posons v = σ 2 pour simpliﬁer les
écritures.
f (x; v) =

√1
2πv

2

exp{− 12 xv }, ln f (x; v) = − 12 ln(2π) −
2

2

1
2

ln v −

2

1
x
ϑ
1
x
ln f (x; v) = − 2v
+ 2v
2 , ϑv 2 ln f (x; v) = 2v 2 − v 3 ,
,
+
2
)
ϑ2
= − 2v12 + E(X
= − 2v12 +
I(v) = E − ϑv
2 ln f (X; v)
v3
ϑ
ϑv

car E(X 2 ) = σ 2 = v, donc I(v) =
1
2σ 4
.
Rao est nI(σ
2) =
n
n

X2

1 x2
,
2 v

1
2v 2

ou I(σ 2 ) =

1
2σ 4

1
v2 ,

et la borne de Cramern

X2

i
i
i=1
Notons que i=1
est sans biais pour σ 2 et que V
= n1 V (X 2 ) =
n
n
+  
,
  2


4
1
4
= n1 3σ 4 − σ 4 = 2σn . Donc cet estimateur est eﬃ− E X2
n E X
cace.

366

Statistique - La théorie et ses applications

Exercice 6.14
Comme vu en section 6.6.3, pour estimer une fonction h(p) =


2

[h (p)]
nI(p)

p
1−p

du paramètre

1
nI(p)(1−p)4 .

p, la borne de Cramer-Rao est
=
Pour la loi de Bernoulli
f (x; p) = px (1 − p)1−x , ln f (x; p) = x ln p + (1 − x) ln(1 − p),
2

1−x
ln f (x; p) = xp − 1−x
, ϑ ln f (x; p) = − px2 − (1−p)
2,
1−p ϑp2
,
+
2
ϑ
1
= E(X)
+ 1−E(X)
= p1 + 1−p
=
I(p) = E − ϑp
2 ln f (X; p)
p2
(1−p)2
p
Cramer-Rao est donc, pour estimer h(p), n(1−p)3 .
ϑ
ϑp

1
.
p(1−p)

La borne de

Exercice 6.15
−2(x−θ)
ϑ
On a : ln f (x; θ) = − ln π − ln[1 + (x − θ)2 ], ϑθ
ln f (x; θ) = 1+(x−θ)
2,
,
+
,
+


2
2
2
+∞
(x−θ)
4(X−θ)
ϑ
= E [1+(X−θ)2 ]2 = π4 −∞ [1+(x−θ)2 ]3 dx.
ln f (X; θ)
I(θ) = E ϑθ

Posons t = x − θ :
 +∞ t2
I(θ) = π4 −∞ (1+t
2 )3 dt =

4
8π

+
arctan t +

,+∞
t(t2 −1)
(t2 +1)2 −∞

=

1
π
2π

= 12 .

La borne de Cramer-Rao pour θ est donc n2 .
√
2
La variance asymptotique de n(Mn − θ) est π4  2, 47 > 2, donc Mn
n’est pas asymptotiquement eﬃcace et n’est pas un estimateur BAN (voir la
déﬁnition en proposition 6.11).

Exercice 6.16
Estimateur des moments
Il vériﬁe θ2 = X et c’est donc θM = 2X. Il est sans biais et eqm(θM ) =
θ2
θ2
= 3n
.
V (θM ) = 4V (X) = n4 V (X) = n4 12
Estimateur du MV
On a vu que θM V = X(n) (exemple 6.21) et que E(X(n) ) =
6.4). Son biais est

n
n+1 θ − θ

θ
= − n+1
. Calculons V (X(n) ) =

n
n+1 θ (exemple
n2
2
2
E(X(n)
) − (n+1)
2θ .

n−1

La densité de X(n) étant n xθn si x ∈ [0, θ] et 0 sinon (voir exemple 6.4), on
a:
 θ n+1
n
2
) = n 0 xθn dx = n+2
θ2 ; V (X(n) ) = (n+1)n2 (n+2) θ 2 . D’où :
E(X(n)
θ
eqm(X(n) ) = − n+1

2

+

n
2
(n+1)2 (n+2) θ

=

2
2
(n+1)(n+2) θ .

Corrigés des exercices
Estimateur n+1
n X(n) (UMVUE)
Calculons sa variance d’après celle de l’EMV.
 n+1 2
n+1
V (X(n) ) =
eqm( n+1
n X(n) ) = V ( n X(n) ) =
n

367

1
2
n(n+2) θ .

X(n) ) < eqm(X(n) ) < eqm(θM ).
Pour n ≥ 3, on vériﬁe aisément que eqm( n+1
n
Pour n = 2, la deuxième inégalité devient une égalité.
2
2
Notons que pour n grand, eqm( n+1
X(n) ) ∼ nθ 2 , eqm(X(n) ) ∼ 2θ
, eqm(θM ) ∼
n
n2
2
θ
3n . L’estimateur des moments est largement dominé par les deux autres (il
est peu pertinent car il n’est pas une fonction de X(n) , statistique exhaustive
minimale).

Exercice 6.17
La fonction de probabilité de cette loi au point k s’obtient par :
P (X=k)
1
e−λ λx
f (x; λ) = P (X = k| X = 0) = 1−P
(X=0) = 1−e−λ
x! . Alors :
−λ
ln f (x; λ) = −λ + x ln λ − ln x! − ln(1 − e ).
n
ϑ
e−λ
ϑ
ne−λ
i=1 xi
ln f (x; λ) = −1 + λx − 1−e
− 1−e
−λ , ϑλ ln L(λ) = −n +
−λ .
ϑλ
λ
L’estimation
du MV est la valeur de λ (si unique) telle que :
n
ne−λ
λ
i=1 xi
−n +
− 1−e
−λ = 0 ou 1−e−λ = x.
λ
M V  2, 82.
Pour x = 3, la solution donne, par approximations successives, λ

Exercice 6.18
La densité de la loi de Pareto est :
 θ+1
pour x ≥ a et 0 sinon.
f (x; θ) = aθ xa
θ
ϑ
ln f (x; θ) = ln a + (θ + 1) ln xa ; ϑθ
ln f (x; θ) = 1θ + ln xa ;

n
ϑ
ln L(θ) = nθ + i=1 ln xai .
ϑθ
L’estimation du MV est telle que :
n
n
+ i=1 ln xai = 0, soit pour l’estimateur θM V =  n nln
θ
i=1

Dans la solution de l’exercice 6.11 on a vu que E
M V

E(θ

)=

n
n−1 θ.

Son biais est

n
n−1 θ

−θ =

Pour la borne de Cramer-Rao calculons :
2
et la borne est θn .

a
Xi

 nn−1
i=1 ln

.

a
Xi

= θ, donc

1
n−1 θ.

ϑ2
ϑθ2

ln f (x; θ) = − θ12 , d’où I(θ) =

1
θ2

Pour la variance de l’EMV notons que selon l’exercice 6.11, Yi = ln( Xai ) et
n
n2
2
M V ) = V ( n ) =
Yi ; E(θ). Posons T =
i=1 Yi , alors V (θ
T
(n−2)(n−1)2 θ

368

Statistique - La théorie et ses applications

comme il a été établi lors de la solution de l’exercice 6.9 pour V ( T1 ). Comme
n2
θ2
2
M V est asymptotiquement eﬃcace.
(n−2)(n−1)2 θ ∼ n quand n → ∞, θ
Exercice 6.19
Calculons :
ln f (x; ρ) = ln ρ + ln(ρ + 1) + ln x + (ρ − 1) ln(1 − x),
ϑ
1
1
ϑρ ln f (x; ρ) = ρ + ρ+1 + ln(1 − x).
n
n
MV
(estimation du MV) est solution de nρ + ρ+1
+ i=1 ln(1 − xi ) =
Donc ρ
n
n
2ρ+1
0 ou ρ(ρ+1)
= − n1 i=1 ln(1 − xi ). Posons a = − n1 i=1 ln(1 − xi ). Il faut
√

2

résoudre en ρ l’équation 2ρ + 1 = aρ(ρ + 1). Les solutions sont 2−a±2a 4+a .
Comme a ne√prend que des valeurs positives, la seule solution dans R+ est
2
ρM V = 2−a+2a4+a .

Exercice 6.20
Loi binomiale négative BN (r, p) avec r connu

 r
fonction de probabilité f (x; p) = r+x−1
p (1 − p)x , x ∈ N.
x
r+x−1
+ r ln p + x ln(1 − p)
ln f (x; p) = ln
x
ϑ
r
x
x
ln
f
(x;
p)
=
−
M V (estimation) solution de pr − 1−p
= 0, d’où
ϑp
p
1−p ; p
r
MV
= X+r (estimateur). La solution est intuitivement réaliste puisqu’elle
p
consiste à faire le rapport du nombre de succès sur le nombre d’essais.
Loi binomiale B(n, p) avec n connu
 
fonction de probabilité f (x; p) = nx px (1 − p)n−x , x = 0, 1, · · · , n.
 
ln f (x; p) = ln nx + x ln p + (n − x) ln(1 − p)
ϑ
ln f (x; p) = xp − n−x
; pM V (estimation) solution de xp − n−x
= 0,
ϑp
1−p
1−p
X
MV
d’où p
= n (estimateur), nombre de succès sur nombre d’essais.

Exercice 6.21
La fonction de probabilité non nulle pour x ∈ {1, 2, · · · , N − M + 1} est :
f (x; N ) = P (X = x) =
ln f (x; N ) =

x−2
k=0

N −M N −M −1
N − M − (x − 2)
M
···
.
N
N −1
N − (x − 2) N − (x − 1)

ln (N − M − k) −

x−1
k=0

ln (N − k) + ln M

Corrigés des exercices
x−2
ln f (x; N ) = k=0
annulant cette expression.
ϑ
ϑN

1
N −M −k

−

x−1

1
k=0 N −k ;

369

 M V (estimation) solution
N

Application
1
1
+ N −101
− ( N1 + N 1−1 + N 1−2 ) = 0.
M = 100, x = 3, résoudre en N : N −100
 M V = 300.
La solution donnée par un logiciel mathématique est N
On notera que c’est la même solution que dans un échantillonnage sans
1
remise où Y = X − 1 ; G(p) avec p = M
M V = y+1
= x1 = 13 .
N , car p

Exercice 6.22
r

λ
La fonction de probabilité est f (x; λ) = Γ(r)
xr−1 e−λx , x > 0.
ln f (x; λ) = r ln λ − ln Γ(r) + (r − 1) ln x − λx
n
ϑ
r
ϑ
nr
M V (estimation) solution
i=1 xi . λ
ϑλ ln f (x; λ) = λ −x; ϑλ ln L(λ) = λ −
r
M
V

annulant cette expression, donc λ
= X (estimateur).

M V
Pour n grand λ
I(λ) :
ϑ2
ϑλ2

;

approx

1
N (λ ; nI(λ)
) selon la proposition 6.11. Calculons

+
,
ϑ2
ln f (x; λ) = − λr2 ; I(λ) = E − ϑλ
=
2 ln f (X; λ)

M V
d’où λ

;

approx

r
λ2 ,

2

λ
N (λ ; nr
). Ce résultat sera utile pour établir un intervalle de

conﬁance pour λ (voir chapitre 7).

Exercice 6.23
La densité de probabilité a posteriori de p sachant (X1 , · · · , Xn ) = (x1 , · · · , xn )
n
− ou en bref X = x − est, en posant s = i=1 xi :
%
1
1
ps (1 − p)n−s p(1 − p)
ps+ 2 (1 − p)n−s+ 2
πp|X=x (p) =  1
= 1
%
1
1
ps (1 − p)n−s p(1 − p)dp
ps+ 2 (1 − p)n−s+ 2 dp
0
0
qui est la densité de la loi Beta(s + 12 , n − s + 12 ). Pour l’estimation bayésienne
de p on prend sa moyenne

Γ(s+ 52 ) Γ(n+3)
Γ(n+4) . Γ(s+ 32 )

=

s+ 32
n+3 .

Pour une loi a priori Beta(α, β), on a une densité proportionnelle à pα (1 − p)β ,
avec α > −1, β > −1, d’où une loi a posteriori Beta(s + α, n − s + β), de
s+α+1
moyenne n+2+α+β
.

370

Statistique - La théorie et ses applications

Chapitre 7 : Estimation paramétrique
par intervalle de conﬁance
Exercice 7.1
Pour μ la largeur est 2 × 1, 96 √sn .
Pour σ on a l’IC (voir note 7.5) :
⎡%

(n − 1)s

%

⎤
(n − 1)s ⎦

,(
IC0,95 (σ) = ⎣ (
2 (n−1)
2 (n−1)
χ0,975
χ0,025

.

Pour n grand la loi χ2 (n − 1), de moyenne n − 1 et variance 2(n − 1), est
approchée par la loi N (n − 1 ; 2(n − 1)) (voir la remarque de la section 5.8.3).
Donc :
+
,
(
%
2 (n−1)
2
χ0,975  (n − 1) + 1, 96 2(n − 1) = (n − 1) 1 + 1, 96 (n−1)
( ,
,− 12
+
,− 12
+
+
(
1
1
2 (n−1)
2
1
 (n−1)− 2 1 + 1, 96 (n−1)
 (n−1)− 2 1 − 1, 96 2n
χ0,975
et de même par simples changements de signes :
( ,
+
,− 12
+
1
2 (n−1)
1
χ0,025
.
 (n − 1)− 2 1 + 1, 96 2n
(
(
+
,
1
1
) , s(1 + 1, 96 2n
)
dont la largeur est
D’où IC0,95 (σ)  s(1 − 1, 96 2n
√
s
2 × 1, 96 √2n soit 2 fois plus petite que celle de l’IC sur μ.

Exercice 7.2
n

X2

i
; χ2 (n).
Pour un échantillon (X1 , X2 , · · · , Xn ) de la loi N (0 ; σ 2 ), on a i=1
σ2
Donc :
n
n
n
Xi2
Xi2
Xi2
2 (n)
2 (n)
P (χ α
< i=1
< χ1− α ) = 1 − α, P ( i=1
< σ2 < i=1
2
2 ( n)
2 (n) ) = 1 − α,
σ
2
2
χ1− α
χα
2
2



d’où IC1−α (σ 2 ) =

n

2
i=1 xi
2 (n)
χ1− α
2

,

n

2
i=1 xi
2 (n)

χα
2

.

Corrigés des exercices

371

Exercice 7.3
Pour X ; U[0, θ] on a la fonction de répartition FX (x; θ) = xθ si x ∈ [0, θ]. Donc
 n
n
pour un échantillon de taille n, FX(n) (x; θ) = [FX (x; θ)] = xθ
si x ∈ [0, θ].
 xα n
1
= α ou xα = θα n . Ainsi :
Le quantile xα de X(n) est tel que θ
1

1

P θ(0, 025) n < X(n) < θ(0, 975) n

= 0, 95

1

1

P X(n) (0, 975)− n < θ < X(n) (0, 025)− n = 0, 95
+
,
1
1
d’où IC0,95 (θ) = x(n) (0, 975)− n , x(n) (0, 025)− n .

Exercice 7.4
1
n−1 − x
e 2
2n (n−1)! x

La densité de la loi χ2 (2n) est, au point x,
fonction de répartition est :

Fχ2 (x; 2n) =

x

0

1
t
tn−1 e− 2 dt =
n
2 (n − 1)!



x
2

0

(x > 0) et sa

1
tn−1 e−t dt = Jn−1 .
(n − 1)!

Intégrons par partie Jk pour établir une relation de récurrence :


x
2

Jk = −
0

 x k − x
 k
 x  x k−1 −t
2 t
e 2
t −t 2
tk
e
−t
+
d(e ) = −
e
dt = − 2
+ Jk−1 ,
k!
k!
k!
0 (k − 1)!
0

avec J0 = 1 − e− 2 .
x

D’où Jn−1 = 1 − e− 2 −
x

2

x

e− 2
2!

( x2 )

− ···

x

e− 2
(n−1)!

( x2 )

n−1

=1−

n−1
k=0

x

k
e− 2 ( x
2)
.
k!

Remplaçons x par 2λ et 2x + 2, x entier, par 2n pour obtenir Fχ2 (2λ; 2x + 2) =
x
−λ k
1 − k=0 e k!λ = 1 − FP (x; λ). Trouver λ tel que, pour x donné, FP (x; λ) = α
équivaut à prendre le quantile d’ordre 1 − α de la loi χ2 (2x + 2) puis à le diviser
par 2.
Exemple 7.6 : T ; P (7λ). Pour x = 18 et α = 0, 025 on prend pour 7λ le
quantile 0, 975 de la loi χ2 (38), soit 56,9. Cela donne la borne supérieure de
l’IC sur 7λ : 7λ2 = 56,9
2 = 28, 4. Pour la borne inférieure, on obtient x = 17,
α = 0, 975, χ20,025 (36) = 21, 34 et 7λ1  10, 7. Finalement, on a trouve bien le
même intervalle pour λ : [1, 53 ; 4, 06] .

372

Statistique - La théorie et ses applications

Exercice 7.5
20
On a T = i=1 Xi ; B(20 ; p). Il faut résoudre en p les deux équations (voir
section 7.5, cas d’une loi discrète) :
7 20 x
20−x
= 0, 975
x=0 x p (1 − p)
8 20 x
20−x
= 0, 025
x=0 x p (1 − p)
Un logiciel de résolution d’équations donne pour la première p1 = 0, 19 et la
deuxième p2 = 0, 64, qui sont les bornes de
(l’intervalle de conﬁance à 95%. L’in-

p)
de la section 7.4.5 donnerait,
tervalle asymptotique classique p ± 1, 96 p(1−
n
avec ici p = 8/20 : [0, 185; 0, 615]. Cet intervalle est toutefois assez approximatif du fait que la règle de validité donnée n
p(1 − p) > 12 n’est pas vériﬁée (ici
n
p(1 − p) = 4, 8).

Exercice 7.6
On sait que X est statistique exhaustive minimale. On a remarqué en section
6.8 que la loi a posteriori du paramètre inconnu sachant les valeurs prises
par l’échantillon ne dépend que de cette statistique. On conditionnera donc
2
simplement sur l’événement (X = x). Comme X ; N (μ , σn ), sa densité est
2
fX (x; μ) = √ 1 2 exp{− 12 (x−μ)
σ2 /n }. La densité de la loi a priori de μ étant
2πσ /n

π(μ) =

√1 2
2πσ0

2

0)
exp{− 12 (μ−μ
}, on a la loi a posteriori :
σ2
0


!
(μ − μ0 )2
1 (x − μ)2
+
πμ|X=x (μ) = c exp −
2
σ 2 /n
σ02

où c est la constante qui normalise à une densité. L’expression entre crochets
s’écrit :




x2
n
nx μ0
μ20
1
2
−
2μ
+
+
+
+
μ
2
2
σ2
σ0
σ2
σ0
σ 2 /n σ02
μ0 2


2

nx
σ02 x̄ + σ 2 μ0 /n
n
1
1
σ 2 + σ02
te
μ
−
μ
−
=
+
+
C
=
+ C te ,
n
1
2 + σ 2 /n
σ02 σ2 /n
σ2
σ02
σ
+
2
2
0
σ
σ
2
2
0

σ0 +σ /n

expression qui met en évidence la moyenne et la variance de la loi a posteriori
de μ, comme indiqué dans l’énoncé. On en déduit l’intervalle de probabilité
0, 95 pour μ (ou, en d’autres termes, son IC bayésien à 95%) :

σ02 σ 2 /n
σ02 x̄ + σ 2 μ0 /n
±
1,
96
.
σ02 + σ 2 /n
σ02 + σ 2 /n

Corrigés des exercices

373

Quand n → ∞, on obtient x pour le premier terme et, pour l’expression sous
2
2
le radical, n+σσ2 /σ2 ∼ σn , soit l’IC classique.
0
Dans l’approche bayésienne, le centre de l’intervalle est une pondération entre
2
2
x et μ0 avec(poids
( respectifs σ0 et σ /n. La demi-largeur est plus petite car
égale à 1, 96

σ2
n

σ02
.
σ02 +σ 2 /n

Exercice 7.7

(

n )
P
tend
Dans les notations de la section 7.4.5, il suﬃt de montrer que Pn (1−
n
Sn

vers 0 en probabilité, où Pn = n . Or selon la loi des grands nombres, Pn
converge presque sûrement (
vers p (voir section 5.8.2) et a fortiori Pn converge
en probabilité vers p. Ainsi Pn (1 − Pn ), en tant que fonction continue de Pn ,
%
converge en probabilité vers p(1 − p), ce qui prouve le résultat.

Exercices appliqués
Exercice 7.8
(11)

On calcule x = 8, 10, s2 = 0, 1018, s = 0, 319 et on lit t0,975 = 2, 201. D’où :
0, 319
= 8, 10 ± 0, 20.
IC0,95 (μ) = 8, 10 ± 2, 201 √
12
Exercice 7.9
(499)

On a x = 5, 4, s = 3, 1 et on lit t0,95  z0,95 = 1, 645. D’où :
3, 1
IC0,90 (μ) = 5, 4 ± 1, 645 √
= 5, 4 ± 0, 23 jours.
500
Pour le coût moyen, on obtient un IC à 90% en multipliant les bornes du
précédent par 200, soit 1080 ± 46 euros.

Exercice 7.10
Pour les 200 sinistres sélectionnés, on a une valeur moyenne de 9 944 euros avec
(199)
un écart-type de 1901. Avec t0,975  1, 97 on obtient, pour la valeur moyenne
1 901
des sinistres en cours, l’IC à 95% : 9 944 ± 1, 97 √
ou 9 944 ± 264, 8 euros.
200
Pour la valeur totale des sinistres en cours on a un IC à 95% de :
11 210 × 9 944 ± 11 210 × 264, 8  111, 5 ± 3, 0 millions d’euros.

374

Statistique - La théorie et ses applications

Exercice 7.11
(19)

Soit μ le nombre moyen de mots par page du livre, on a, avec t0,975 = 2, 093 :
26
IC0,95 (μ) = 614 ± 2, 093 √ = 614 ± 12, 2.
20
Pour le nombre total de mots, on obtient l’IC à 95% en multipliant par le
nombre de pages, soit 97 012 ± 1 923 ou environ [95 100, 98 900] .

Exercice 7.12
Soit μ1 et μ2 les consommations moyennes avec carburant traditionnel et carburant nouveau respectivement. On cherche un IC sur μ1 −μ2 . Sur les échantillons
on a n1 = 10, x1 = 10, 8, s1 = 0, 21, n2 = 10, x2 = 10, 3, s2 = 0, 18.
On peut appliquer la formule sous l’hypothèse de variances égales au vu des
tailles d’échantillons et des écarts-types observés (voir section 7.4.3). On calcule
1
d’abord la variance empirique pondérée : s2p = 18
(9 × (0, 21)2 + 9 × (0, 18)2 ) =
0, 03825 , d’où sp = 0, 196.
(18)

Avec t0,95 = 1, 734 on obtient :

)

IC0,95 (μ1 − μ2 ) = (10, 8 − 10, 3) ± 1, 734 × 0, 196

1
1
+
 0, 5 ± 0, 15.
10 10

Le gain peut être estimé entre 0,35 et 0,65 litres.

Exercice 7.13
On a p = 45/400 = 0, 1125. Comme n
p(1 − p) = 39, 9 > 12 nous appliquons
l’approximation gaussienne pour la proportion de pièces défectueuses. Avec
z0,995 = 2, 57 on a :
)
0, 1125 × 0, 8875
= 0, 1125 ± 0, 0406,
IC0,99 (p) = 0, 1125 ± 2, 57
400
ce qui donne pour le nombre total de pièces défectueuses dans le stock :
1 125±406 soit entre 720 et 1 530 pièces environ.

Corrigés des exercices

375

Exercice 7.14
On a p = 0, 20. Comme n
p(1 − p) = 240 > 12, nous appliquons l’approximation
gaussienne pour la proportion de personnes prévoyant d’acheter une voiture
dans les douze prochains mois. Avec z0,975 = 1, 96 on a :
)
0, 20 × 0, 80
IC0,95 (p) = 0, 20 ± 1, 96
= 0, 20 ± 0, 032,
1500
soit entre 17% et 23% environ.

Exercice 7.15
Soit p1 et p2 les proportions de pièces défectueuses produites par le premier
et le deuxième procédé respectivement. On cherche un IC sur p1 − p2 . Sur les
échantillons on a n1 = 1000, p1 = 86/1000 = 0, 086, n2 = 800, p2 = 92/800 =
p2 (1 − p2 ) = 81, 4 > 12 nous
0, 115. Comme n
p1 (1 − p1 ) = 78, 6 > 12 et n
appliquons l’approximation gaussienne de la section 7.4.6. Avec z0,975 = 1, 96
on a :
)
0, 086 × 0, 914 0, 115 × 0, 885
+
IC0,95 (p1 − p2 ) = (0, 086 − 0, 115) ± 1, 96
1000
800
= −0, 029 ± 0, 028,
soit entre −0, 057 et −0, 001. Cet intervalle semble indiquer que p1 est inférieur
à p2 .

Exercice 7.16
50
Pour les 50 jours, on a observé un nombre total d’accidents égal à i=1 xi =
0 × 21 + 1 × 18 + 2 × 7 + 3 × 3 + 4 × 1 = 45 d’où une moyenne d’accident par
jour observée de x = 0, 90. On applique l’approximation gaussienne développée
dans l’exemple 7.3. Avec z0,975 = 1, 96 on a :
)
0, 90
= 0, 90 ± 0, 26,
IC0,95 (λ) = 0, 90 ± 1, 96
50
soit un nombre moyen d’accident par jour entre 0,64 et 1,16.
Notons que cet IC tient compte des spéciﬁcités du modèle de Poisson, à savoir
que moyenne et variance sont égales à λ.

376

Statistique - La théorie et ses applications

Exercice 7.17
On calcule s2 = 0, 00461, s = 0, 0679. Calculons d’abord l’IC pour la variance
2 (9)
2 (9)
σ 2 du taux (voir section 7.4.2), avec χ0,025 = 2, 700 et χ0,975 = 19, 023 :
IC0,95 (σ 2 ) = [

9 × 0, 00461 9 × 0, 00461
,
] = [0, 00281 , 0, 01537]
19, 023
2, 700

soit IC0,95 (σ) = [0, 047 , 0, 124] ou, environ, [0, 05 , 0, 12].

Corrigés des exercices

377

Chapitre 8 : Estimation
non paramétrique et estimation
fonctionnelle
Exercice 8.1
Dans EXCEL on génère en première colonne 200 nombres au hasard dans [0,1]
avec la fonction ALEA(). En deuxième colonne on applique à la première colonne la fonction EXP(LOI.NORMALE.STANDARD.INVERSE(-)) ou directement LOI.LOGNORMALE.INVERSE(- ; 0 ; 1), pour obtenir 200 observations
issues de la loi LN (0 ; 1) selon le principe déﬁni en section 4.3.
Nous avons eﬀectué cette opération et trouvé une médiane des 200 observations
égale à 1,073 qui est une estimation ponctuelle de la vraie médiane μ
' laquelle
est ici égale à e0 = 1. En eﬀet, si ζq est le quantile d’ordre q de la loi N (0 ; 1), eζq
est le quantile d’ordre q de la loi LN (0 ; 1) puisque P (Z ≤ q) = P (eZ ≤ eq ) = q
où Z ; N (0 ; 1) et eZ ; LN (0 ; 1).
Comme vu en section 8.2 on obtient un IC à 95% pour μ
' avec les l1 -ième et
l2 + 1-ième statistiques d’ordre réalisées dans l’échantillon, où l1 est tel que
' ; B(200 ; 0, 5). En calculant les
' ≥ l1 ) ≥ 0, 975 et l2 = n − l1 , avec N
P (N
probabilités de cette loi dans EXCEL pour l’ensemble des valeurs possibles on
' ) = 0, 980 et P (87 ≤ N
' ) = 0, 972 donc l1 = 86 et l2 = 114.
trouve que P (86 ≤ N
Dans notre échantillon, les 86-ième et 115-ième statistiques d’ordre ont pris,
respectivement, les valeurs 0,923 et 1,230 d’où IC0,95 ('
μ) = [0,923;1,230]. Cet
intervalle couvre bien la vraie valeur.
' ∗ , nombre
Pour x0,90 , le quantile d’ordre 0,90, on recourt de même à la v.a. N
d’observations inférieures ou égales à x0,90 qui suit une loi B(200 ; 0, 90). On
' ∗ ≤ l∗ ) ≥ 0, 95. On trouve, en caldoit chercher l1∗ et l2∗ tels que P (l1∗ ≤ N
2
' ∗ ) = 0, 984 et
culant les probabilités de cette loi dans EXCEL, P (171 ≤ N
' ∗ ) = 0, 973 donc on prendra l∗ = 171. Comme P (N
' ∗ ≤ 187) =
P (172 ≤ N
1
∗
' ≤ 187) = 0, 968 − 0, 016 = 0, 952. Dans notre
0, 968 on a P (171 ≤ N
échantillon les 171-ième et 188-ième statistiques d’ordre ont pris, respectivement, les valeurs 3,29 et 4,48 d’où IC0,95 ('
μ) = [3,29;4,48]. Cet intervalle couvre
1,28
bien la vraie valeur égale à e
= 3,60. L’estimation ponctuelle était donnée
par le quantile empirique d’ordre 0,90 soit 3,94 pour notre échantillon.

378

Statistique - La théorie et ses applications

Exercice 8.2
Pour les 12 valeurs données, (
on obtient x = 1, 10 et s = 1, 32.
12
1
2
On calcule les valeurs s−i = 10
j=1,j =i (xi − x−i ) et on obtient :
1,34 1,34 1,34 1,34 1,36 1,37 1,38 1,38 1,38 1,36 1,36 0,71.
Puis on calcule les pseudo-valeurs s∗i = 12s − 11s−i et on obtient :
1,05 1,05 1,05 1,05 0,82 0,77 0,62 0,61 0,63 0,89 0,89 8,05.
La moyenne des pseudo-valeurs est égale à 1,46 ce qui donne l’estimation par
jackknife de σ. On note qu’elle est assez diﬀérente de l’estimation usuelle par
s =1,32 ce qui s’explique par l’incidence forte de la dernière valeur pour un
nombre faible d’observations.
Comme l’écart-type des pseudo-valeurs est sJK =2,08 on obtient :
2,08
2,08
11
√
√
IC0,95 (σ) = [1,46−t11
0,975 12 , 1, 46 + t0,975 12 ]

soit, avec t11
0,975 =2,201, l’intervalle [0,14 ; 2,78] qui est assez large en raison du
faible nombre d’observations.
Note : Les résultats ont été calculés avec la précision d’EXCEL mais sont
indiqués arrondis à la deuxième décimale.

Exercice 8.3
On a S'n2 =
2
=
S'−i

1
n

n

1
n−1

j=1 (Xj
n


− X)2 et, en omettant l’observation i :

(Xj − X −i )2 =

j=1,j =i

1
n−1

n


(Xj − X)2 − (X −i − X)2

j=1,j =i

soit pour la pseudo-valeur correspondante :
2
2
S'∗i
= nS'n2 − (n − 1)S'−i

=

n


n


(Xj − X)2 −

j=1

(Xj − X)2 + (n − 1)(X −i − X)2

j=1,j =i

= (Xi − X) + (n − 1)(X −i − X)2 .
2

Or :
X −i − X =
=

1
n−1

n

j=1,j =i
n


1
n(n − 1)

j=1

1
Xj =
n j=1
n

Xj −
Xj −



1
1
−
n−1 n


n
j=1

1
1
Xi =
(X − Xi ),
n−1
n−1

Xj −

1
Xi
n−1

Corrigés des exercices
d’où :

2
= (Xi − X)2 +
S'∗i

379

1
n
(X − Xi )2 =
(Xi − X)2 .
n−1
n−1

2
Ainsi l’estimateur du Jackknife qui est la moyenne des S'∗i
donne Sn2 qui est
2
sans biais. Cela corrobore la proposition 8.2 : le biais de S'n2 qui est - σn est
éliminé par la procédure Jackknife.

Exercice 8.4
On a X =

1
n

n
j=1

Xj et X −i =

1
n−1

n
j=1,j =i

X ∗i = nX − (n − 1)X −i =

n

j=1

Xj . Donc :

Xj −

n


Xj = Xi .

j=1,j =i

Les pseudo-valeurs sont ainsi identiques aux valeurs mêmes et l’estimateur du
jackknife reste X.

Exercice 8.5
Considérons la double suite · · · , a0 − 2h, a0 − h, a0 , a0 + h, a0 + 2h, · · · qui
déﬁnit une grille d’intervalles de largeur h pour l’histogramme, pour laquelle
nous considérerons le point a0 comme point de positionnement. Soit fn (x; a0 ) la
valeur de l’histogramme au point x. Nous pouvons écrire fn (x; a0 ) en exprimant
le comptage des xi situés dans le même intervalle que x (pour lever l’ambiguı̈té
aux limites des intervalles, nous prendrons des intervalles ouverts à droite), ce
qui donne la double somme comme suit :
1 
I[a0 +kh,a0 +(k+1)h[ (xi ) I[a0 +kh,a0 +(k+1)h[ (x)
fn (x; a0 ) =
nh i=1
n

=

1
nh

k∈Z
n


i=1 k∈Z

I[0,1[ (

xi − a0
x − a0
− k) I[0,1[ (
− k).
h
h

Nous considérons maintenant fn (x; a0 ) correspondant à la valeur moyenne obtenue en faisant glisser uniformément la grille du positionnement a0 à a0 + h :
1
fn (x; a0 ) =
nh2



n 
a0 +h 
a0

i=1 k∈Z

I[0,1[ (

xi − t
x − t)
− k) I[0,1[ (
− k)dt,
h
h

380

Statistique - La théorie et ses applications

xi −a0
x−a0
0
soit, en posant u = t−a
et en permutant l’intégration et
h , zi =
h ,z =
h
la première sommation :
n 
1  1
I[0,1[ (zi − k − u)I[0,1[ (z − k − u)du.
fn (x; a0 ) =
nh i=1 0
k∈Z

En remarquant que I[0,1[ (zi − k − u) est égal à 1, si et seulement si k = [zi ]
(la partie entière de zi ) quand u ∈ [0, zi − [zi ]] et si et seulement k = [zi ] − 1
quand u ∈ [zi − [zi ], 1] , on a :


 1
n
zi −[zi ]

1
fn (x; a0 ) =
I[0,1[ (z − [zi ] − u)du +
I[0,1[ (z − [zi ] + 1 − u)du .
nh i=1 0
zi −[zi ]
En eﬀectuant le changement de variable v = z − [zi ] − u dans la première
intégrale et v = z − [zi ] + 1 − u, on obtient :
1 
fn (x; a0 ) =
nh i=1
n



z−zi +1

z−zi

I[0,1[ (v)dv.

En examinant les diﬀérentes positions de [0, 1] par rapport à [z − zi , z − zi + 1],
on arrive ﬁnalement à :
1 
[1 − |z − zi |] I[0,1[ (|z − zi |)
fn (x; a0 ) =
nh i=1


n
n
1 
1 
x − xi
=
K(z − zi ) =
K
nh i=1
nh i=1
h
n

où K(u) = (1 − |u|)I[0,1[ (|u|) est le noyau triangulaire.
Comme on s’y attendait, fn (x; a0 ) ne dépend pas de l’origine a0 .

Exercice 8.6
Dans EXCEL, on génère en première colonne 50 nombres au hasard dans [0,1]
avec la fonction ALEA(). En deuxième colonne on applique à la première colonne la fonction LOI.NORMALE.STANDARD.INVERSE(-). Puis on applique
(1 − x2 )2 pour les valeurs x ∈ [−1, 1] et 0
la fonction biweight avec h = 1 soit 15
16
sinon. Alors la moyenne de ces poids est l’estimation pour f (0) = √12π 0,399.
L’espérance mathématique de cet estimateur est (voir section 8.5.2) :

 +1
x2
x−t
15
1
1
)f (t)dt =
(1 − x2 )2 √ e− 2 dx  0,369 ,
K(
h R
h
2π
−1 16

Corrigés des exercices

381

valeur obtenue via un logiciel de calcul mathématique. Le biais est 0,369−0,399
= 0,030. Il est évidemment négatif puisqu’on pondère des observations prises
au voisinage d’un maximum.
Sur une simulation de 50 observations on doit trouver une estimation avec
un écart à la vraie valeur pas trop éloigné de 0,03 (on peut éventuellement
augmenter la taille de l’échantillon pour le vériﬁer plus précisément).

Exercice 8.7
On procède comme à l’exercice précédent à ceci près que la fonction de poids est
15
x 2 2
16 (1 − ( h ) ) pour les x ∈ [−h, h], avec les diﬀérentes valeurs de h proposées.
La valeur de h optimale asymptotiquement en un point x est donnée en
section 8.5.2 :

1/5

f (x) R [K(u)]2 du
n−1/5 .
hopt =

2
[f  (x)]2 R u2 K(u)du
Ici on a x = 0, f (0) =

R

√1
2π

0,399 , f  (0) = −1 et :

2  +1
 2  +1
15
15
(1 − u2 )4 du =
(1 − 4u2 + 6u4 − 4u6 + u8 )du
16
16
−1
−1

 2 
4
15
6
4
1
 0,7143 ,
1− + − +
=2
16
3
5
7
9




15 +1 2
15 1
2
1
u2 K(u)du =
(u − 2u4 + u6 )du = 2
 0,1429.
− +
16 −1
16 3
5
7
R

[K(u)]2 du =



1

d’où ﬁnalement hopt =1,694 n− 5 .
Pour n = 50 on trouve hopt =0,775. Observez-vous l’estimation la plus proche
de √12π 0,399 pour h =0,75? Augmentez éventuellement la taille de l’échantillon
pour vériﬁer empiriquement que la meilleure estimation est celle obtenue avec
la valeur de h la plus proche de hopt . Par exemple, pour n = 500, hopt =
0,49. Notons que la largeur de fenêtre reste assez large même pour une taille
d’échantillon assez élevée.

Exercice 8.8
Rappelons l’expression asymptotique de l’eqim donnée en section 8.5.2.
h4
eqim fn ∼
4



[f  (x)]2 dx
R



2
u2 K(u)du

R

+

1
nh


2

[K(u)] du
R

382

Statistique - La théorie et ses applications

En la dérivant par rapport à h, on obtient une expression qui s’annule pour :




hopt = 
R

1/5

2

[K(u)] du

u2 K(u)du
R

2

 45 

 25 

R

[f (x)]2 dx

n−1/5 .

et en remplaçant dans l’eqim :
eqim fn

opt

5
=
4


2

2

[K(u)] du
R



u K(u)du
R

2

[f (x)] dx

 15

4

n− 5 .

R

Le facteur dépendant uniquement du noyau que l’on peut souhaiter minimiser
+
, 45 
2
est ν(K) = R [K(u)]2 du
u2 K(u)du 5 .
R

2
Pour le noyau biweight on a calculé à l’exercice précédent R [K(u)] du =
 2
0,7143 et R u K(u)du = 0,1429 ce qui donne ν(K) = 0,351. Pour le noyau de
Rosenblatt on trouve de même ν(K) = 0,369 et pour le meilleur noyau, celui
d’Epanechnikov, ν(K) = 0,349. On constate qu’en termes d’approximation
asymptotique les diﬀérences sont faibles. En particulier le noyau biweight est
très proche de celui d’Epanechnikov, avec l’avantage d’être partout dérivable.

Exercice 8.9
On peut simplement appliquer les résultats de l’exercice précédent concernant
⎤ 15
le noyau biweight : ⎡
0,7143
⎦ n− 15  2,78 σn− 15 .
h=⎣
5
3σ
2
√
(0, 1429)
8 π

 15
4
5

2

= ν(K)
[f (x)] dx n− 5
eqim fn
4
opt
R
 5  15
4
4
5
3σ
√
= (0, 351)
n− 5  0, 321 σ −1 n− 5 .
4
8 π
En intégrant eqm fn (x) ∼ h12 [f  (x)] + f (x)/(nh) donnée en section 8.5.2
pour l’histogramme au point x, on obtient :


h2
1
2
eqm fn (x) dx ∼
[f  (x)] dx +
.
eqim(fn ) =
12 R
nh
R
La dérivée par rapport à h de cette expression s’annule pour :

−1/3
2
1/3

[f (x)] dx
n−1/3 ,
hopt = 6
2

2

R

ce qui donne pour la loi N (μ , σ 2 ) :

Corrigés des exercices

383

√ 1
hopt = (24 π) 3 σ n−1/3  3,49 σn−1/3 .
En remplaçant dans l’expression de eqim(fn ) ci-dessus on vériﬁe aisément que
2
eqim(fn )opt  0,430 σ −1 n− 3 .
Pour n = 500, par exemple, cette erreur vaut 6,83×10−3 σ −1 contre 2,22×10−3 σ −1
pour le biweight, soit environ trois fois plus.

Exercice 8.10
Pour Fn (x) =

1
n

n
i=1

H

 x−xi 
h

, on a :

 

 
  +∞ 

n
+
,
1
x − Xi
x−t
x−X
E Fn (x) =
E H
H
=E H
=
f (t)dt,
n i=1
h
h
h
−∞
 +∞
 +∞
H(u) h f (x − uh)du =
H(u) d(−F (x − uh))
=
−∞

= [−H(u)F (x − uh]+∞
−∞ +



−∞

+∞
−∞

K(u) F (x − uh)du.

car H  (u) = K(u). Le premier terme est nul et, pour le deuxième, développons
F (x − uh) au voisinage de x :


,  +∞
+
u2 h2 
2

f (x) + o(h ) du
K(u) F (x) − uhf (x) +
E Fn (x) =
2
−∞
 +∞
h2
u2 K(u)du + o(h2 )
= F (x) + f  (x)
2
−∞
en utilisant la symétrie du noyau. Notons qu’en raison de cette propriété on
peut remplacer o(h2 ) par o(h3 ).

384

Statistique - La théorie et ses applications

Chapitre 9 : Tests d’hypothèses
paramétriques
Exercice 9.1
n
*n
La fonction de vraisemblance est L(λ) = i=1 f (xi ; λ) = λn e−λ i=1 xi . Le
rapport de vraisemblance (RV) est :
 n
 n
n
n
L( 12 )
1
1 
1
1
=
exp{(1 − )
xi } =
exp{
xi }.
L(1)
2
2
2
2
i=1

i=1

L( 12 )

On rejette H0 : λ = 1/2 (vs. H1 : λ = 1) si L(1) < kα , ce qui équivaut à
n
n
1

2
i=1 xi < kα . Sous H0
i=1 Xi ; Γ(n, 2 ) ≡ χ (2n).
n
2 (2n)
1
Comme P ( i=1 Xi < χ0,05 |λ = 2 ) = 0, 05 on rejette H0 au niveau 0,05 si
n
2 (2n)
i=1 xi < χ0,05 .

Exercice 9.2
a) Dans un processus de Bernoulli X1 ; G(p) est le nombre d’échecs avant le
premier succès. Les v.a. X1 et X2 étant indépendantes, X1 + X2 peut être
vue comme le nombre d’échecs avant le deuxième succès et ainsi de suite
X1 + X2 + · · · + Xn peut-être vue comme le nombre d’échecs avant le n-ième
n
succès et ainsi i=1 Xi ; BN (n, p).
n
b) La fonction de vraisemblance de p est L(p) = pn (1−p) i=1 xi , n étant connu.
Pour le test considéré le RV est :
   n
n
L 13
1
2 =
2 i=1 xi
2
L 3
n
< kα équivaut à i=1 xi < kα .
   4
n
c) T =
(4, 1 ) sous H0 . Alors P (T = 0) = 30 13 ( 23 )0 =
i=1 Xi ; BN
   4
4  1 34 2
0,0123 , P (T = 1) = 1 3 ( 3 ) = 0,0329 et P (T = 2) = 52 13 ( 23 )2 =
0,0549. On a donc un test conservateur de niveau 0,05 en rejetant H0 si
n
i=1 xi ≤ 1.
 
d) La puissance du test est donnée par P (T = 1| p = 23 ) = ( 23 )4 + 4( 23 )4 13 =
0,461.
et

L( 13 )
L( 23 )

Corrigés des exercices

385

Exercice 9.3
Le risque de première espèce est α = P (X > 5 + √1n ) avec X ; N (5 ; n1 ). Donc
X−5
√ > 1) = P (Z > 1) où Z ; N (0 ; 1), d’où α = 0, 159.
α = P ( 1/
n

Pour μ quelconque X ; N (μ , n1 ) et :
√
X −μ
1
5−μ
P (X > 5 + √ ) = P ( √ > √ + 1) = P (Z > n(5 − μ) + 1)
n
1/ n
1/ n
√
et la fonction puissance, déﬁnie pour μ > 5, est h(μ) = 1 − Φ( n(5 − μ) + 1) où
Φ est la fonction de répartition de la loi normale centrée-réduite. Cette fonction
croı̂t de 0,159 à 1.

Exercice 9.4
Écrivons la fonction de densité f (x; θ) = θaθ I[a,+∞[ (x) exp{−(θ + 1) ln x}, .soit
la forme exponentielle avec c(θ) = −(θ + 1). Or la proposition 9.5 indique que
le RV est monotone si la fonction c(θ) est monotone. Pour θ < θ , L(θ)/L(θ ) =
n

(θaθ /θ aθ )n exp{(θ − θ) i=1 ln xi } croı̂t en fonction de la statistique exhausn
tive minimale (ou plutôt sa réalisation) i=1 ln xi .
Pour H0 : θ ≥ θ0 vs. H1 : θ < θ0 le test UPP consiste à rejeter H0 si
n
◦
i=1 ln xi > kα (cas N 3 de la section 9.4.2).
Application :
θ
θ
L’existence de la moyenne θ−1
suppose que θ > 1. L’hypothèse θ−1
≤ 2 équivaut
à θ ≥ 2. Or le test UPP pour H0 : θ ≥ 2 vs. H1 : 1 < θ < 2 consiste à rejeter

H0 si ni=1 ln xi > kα . La fonction de répartition de X suivant la loi de Pareto
avec a = 1 étant, au point x, 1 − x−θ pour x ≥ 1, celle de ln X au point x est
P (ln X ≤ x) = P (X ≤ ex ) = 1 − e−θx si ex ≥ 1, soit x > 0. Donc ln X ; E(θ)
n
et i=1 ln Xi ; Γ(n, θ).

Sous H0 ni=1 ln Xi ; Γ(n, 2). Soit γ0,95 le quantile d’ordre 0,95 de cette loi,
n
alors le test UPP consiste à rejeter H0 au niveau α = 0, 05 si i=1 ln xi > γ0,95 .

Exercice 9.5
On a f (x; θ) = 1θ I[0,θ] (x) et :

386

Statistique - La théorie et ses applications

n
1 $
1
L(θ) = n
I[0,θ] (xi ) = n I[0,θ] (x(n) )I[0,+∞] (x(1) ).
θ
θ
i=1

Pour 0 < θ < θ , L(θ)/L(θ  ) est nul pour x(n) > θ et vaut (θ /θ)n > 0 pour
0 ≤ x(n) ≤ θ (le RV n’est pas déﬁni pour x(n) < 0). Donc le RV est non
croissant en fonction de x(n) .
Soit, par exemple, le test H0 : θ ≤ θ0 vs. H1 : θ > θ0 (cas N◦ 2 de la section
9.4.2), alors le test UPP consiste à rejeter H0 si x(n) > k. Pour un niveau α
on choisit k tel que, sous H0 , P (X(n) > k) = α. Or, sous H0 , la fonction de
x
θ0

répartition de X(n) au point x est
résolvant 1 −

k
θ0

n

n

, donc P (X(n) > k) = 1 −

k
θ0

n

. En

= α, on trouve k = θ0 (1 − α)1/n .

Exercice 9.6
On est ici dans une situation où l’on fait une seule observation. X est donc
statistique exhaustive minimale. Pour une réalisation x, on a :

L(M ) =

M
x




N −M
N
/
n−x
n

si M entier dans [n, N ] et 0 sinon (nous envisageons le cas réaliste où M et
N − M sont supérieurs à n sinon il y a lieu de tenir compte de toutes les
situations autres). D’où :
L (M + 1)
=
L (M )
=

M +1 N −M −1
x

n−x

 M  N −M 
x

n−x

(M + 1)(N − M − n + x)
(M + 1)!(M − x)!(N − M − 1)!(N − M − n + x)!
=
(M + 1 − x)!M !(N − M − 1 − n + x)!(N − M )!
(M + 1 − x)(N − M )

qui est une fonction croissante de x. Cela est vrai également pour

L(M +2)
L(M )

=

L(M +2) L(M +1)
L(M +1) L(M )

comme produit de deux fonctions croissantes positives et, de
L (M  )
proche en proche, pour L(M ) avec M  > M. Notons que les résultats établis
en section 9.4.2 sont fondés sur le rapport
décroissant.

L(θ)
L(θ  )

avec θ < θ , lequel est donc ici

Pour tester H0 : M ≥ M0 vs. H1 : M < M0 nous sommes dans le cas 4
décrit en section 9.4.2 et le test UPP consiste à rejeter H0 si x < k, ce qui est
intuitif. Pour un risque de première espèce α choisi, soit il existe − cas très peu
vraisemblable − un entier cα tel que, pour M = M0 , P (X ≤ cα ) = α et on a
un test exactement de niveau α en rejetant si x ≤ cα , soit le quantile d’ordre α

Corrigés des exercices

387

est une interpolation entre un entier c et c + 1, auquel cas on devra rejeter de
façon conservatrice pour x ≤ c, ce qui équivaut encore à x ≤ cα avec la valeur
non entière d’interpolation.

Exercice 9.7
Soit le rejet de H0 pour une observation n’appartenant pas à [c1 , c2 ]. On a pour
un λ donné :
/ [c1 , c2 ]) = Pλ (X < c1 ) + Pλ (X > c2 ) = 1 − e−λc1 + e−λc2 . Il s’agit
Pλ (X ∈
de résoudre en (c1 , c2 ) le système suivant :
"
1 − e−λ0 c1 + e−λ0 c2 = α
c1 e−λ0 c1 − c2 e−λ0 c2 = 0
la deuxième équation correspondant à la condition d’annulation de la dérivée
en λ0 de 1 − e−λc1 + e−λc2 , comme indiqué dans la note 9.3.
Supposons que le risque soit équiréparti sur les deux extrémités. Alors on aurait
1 − e−λ0 c1 = e−λ0 c2 = α2 , ou λ0 c1 = − ln(1 − α2 ) et λ0 c2 = − ln α2 . Ce qui
donnerait par la deuxième équation :
λ0 c1 (1 − α2 ) − λ0 c2 α2 = 0 =⇒ α2 ln α2 − (1 − α2 ) ln(1 − α2 ) = 0.
Or on peut vériﬁer que la fonction x ln x − (1 − x) ln(1 − x) ne s’annule pas sur
]0, 12 [ et l’équirépartition n’est donc pas possible pour α ∈]0, 1[.
Exemple : λ0 = 1, α = 0, 10 donnent la répartition 1 − e−λ0 c1 = 0, 081 et
e−λ0 c2 = 0, 019.

Exercice 9.8
On est là dans un cas particulier de l’exemple 9.8. La fonction de vraisemblance
est maximisée, au dénominateur du RVG, pour μ
M V = x. Pour le numérateur
on doit la maximiser, pour μ dans [μ1 , μ2 ], ce qui revient à minimiser μ(μ−2x).
Si x ∈ [μ1 , μ2 ], le minimum est atteint pour μ = x, le RVG vaut 1 et on accepte
toujours.
Si x < μ1 , le minimum est atteint pour μ = μ1 et le RVG vaut alors
2
x−μ
1)
√1
exp{− 2σn2 (μ1 − x)2 }. On rejette donc H0 pour (x−μ
σ2 /n > k1 , soit σ/ n < −k1 .
De même si x > μ2 on rejette si

x−μ
√2
σ/ n

> k2 .

388

Statistique - La théorie et ses applications

Par symétrie on prend k1 = k2 = k avec k tel que :




X − μ1
X − μ2
√
√
< −k + Pμ
>k ≤α
pour tout μ ∈ [μ1 , μ2 ], Pμ
σ/ n
σ/ n




μ1 − μ
μ2 − μ
X −μ
X −μ
√ <
√ − k + Pμ
√ >
√ + k ≤ α.
ou Pμ
σ/ n
σ/ n
σ/ n
σ/ n
On peut aisément vériﬁer que la somme de ces probabilités est identique pour
μ = μ1 et pour μ = μ2 . En admettant que le maximum est atteint pour ces
valeurs, on doit trouver k tel que :
2 −μ
√ 1 + k) = α ⇐⇒ Φ( μ2 −μ
√ 1 + k) − Φ(−k) = 1 − α, où Φ
Φ(−k) + 1 − Φ( μσ/
n
σ/ n
est la fonction de répartition de la loi N (0 ; 1).

Probabilité de rejet

Application :
√
Il faut trouver k tel que Φ( n + k) − Φ(−k) = 0, 95. En prenant k =1,645 on a
√
Φ(−k) = 0, 05 et pour n ≥ 2, Φ( n + k)  1. On donne ci-après, pour n = 50,
le graphe de :




√
√
h(μ) = Pμ Z < n(4 − μ) − 1, 645 + Pμ Z > n(5 − μ) + 1, 645
√
√
= Φ( n(4 − μ) − 1, 645) + 1 − Φ( n(5 − μ) + 1, 645).

Figure 9.4 - Fonction de puissance et de risque de première espèce
Pour tester H0 : μ = μ0 vs. H1 : μ = μ0 appliquons les résultats précédents en
prenant μ1 = μ2 = μ0 .

Corrigés des exercices

389

x−μ
√ 0 < −k (cas où x < μ0 ) ou x−μ
√ 0 > k (cas où x > μ0 ) où
On rejette pour σ/
n
σ/ n
k est tel que Φ(−k) + 1 − Φ(k) = α, soit encore 1 − Φ(k) = α2 ou Φ(k) = 1 − α2 ,
d’où k = z1− α2 . Cela est le test classique donné en section 9.7.1 et fondé sur le

fait que, sous H0 ,

X−μ
√0
σ/ n

; N (0 ; 1).

On a vu ci-dessus que Λn = exp{− 2σn2 (μ0 − X)2 }, donc −2 ln Λn =

X−μ
√0
σ/ n

2

.

Cette variable aléatoire suit une loi χ2 (1) comme carré d’une gaussienne centréeréduite. Le théorème asymptotique 9.2 est en fait vériﬁé pour tout n ici.

Exercice 9.9
M V = 1 , soit L(λ
M V ) =
On a L(λ) = λn e−nλx , prenant son maximum pour λ
x
M V ) =
(x)−n e−n . Pour tester H0 : λ = λ0 vs. H1 : λ = λ0 le RVG est L(λ0 )/L(λ
n −n(λ0 x−1)
(λ0 x) e
. Appliquant le théorème 9.2, on rejettera H0 au niveau α si


2 (1)
2 (1)
n −n(λ0 x−1)
> χ1−α ⇐⇒ 2n[λ0 x − 1 − ln(λ0 x)] > χ1−α .
−2 ln (λ0 x) e
Application :
Pour λ0 = 1/4 et n = 30, la règle de rejet avec α = 0, 05 est x4 − 1 − ln x4 > 3,84
60 .
Or la fonction g(u) = u − 1 − ln u décroı̂t de +∞ à 0 quand x varie de 0 à 1 et
croı̂t ensuite de 0 à +∞. Donc g(u) = c admet deux solutions pour c > 0. Pour
, on trouve, par approximations successives, les solutions u1  0, 68
g(u) = 3,84
60
et u2  1, 40. On sera ainsi amené à rejeter H0 si x <2,72 ou x >5,60.

Exercice 9.10
On a L(a) = 2n an
de a :

n
i=1

ln L(a) = n ln a − a
qui s’annule pour 
aM V

xi exp{−a

n

n
i=1

x2 + ln(2n
i n
= n/ i=1 x2i .
i=1

x2i }. Déterminons l’estimateur du MV

n
i=1

xi ) ,

ϑ
ϑa

ln L(a) =

n
a

−

n
i=1

x2i ,

Le RVG pour tester H0 : a = 1 vs. H1 : a = 1 est :
n
n

 n
n
2 n

2n i=1 xi exp{− i=1 x2i }
L(1)
i=1 xi


=
=
exp{n−
x2i }.
n
n
L(
aM V )
n
2n (n/ i=1 x2i )n i=1 xi exp{−n}
i=1
Pour la statistique RVG, on remplace xi par Xi dans cette expression.

390

Statistique - La théorie et ses applications

Exercice 9.11
 n *n 2 θ+1
si x > a. Déterminons l’estimateur du MV de
On a L(θ) = θ2
i=1 xi
θ:
 
n
n
ϑ
ln L(θ) = n ln θ2 − (θ + 1) i=1 ln( x2i ) , ϑθ
ln L(θ) = nθ − i=1 ln( x2i ),
 

qui s’annule pour θM V = n/ ni=1 ln x2i .
Le logarithme du RVG pour tester H0 : θ = 3 vs. H1 : θ = 3 est :

 
n
n


xi
xi
θM V
3
ln( ) − n ln
ln( )
−4
+ (θM V + 1)
ln RV G = n ln
2
2
2
2
i=1
i=1


n

xi
3
ln( )
+ (θM V − 3)
= n ln
M
V

2
θ
i=1
2 (1)

et on rejette H0 au niveau 0, 05 si −2 ln RV G > χ0,95 = 3, 84.
Application :
30
n
xi
Pour n = 30 et
i=1 ln xi = 31 on a
i=1 ln( 2 ) = 31 − 30 ln 2 =10,206,
θM V =2,9396 et −2 ln RV G = 0, 0125. Donc on accepte H0 .
Note : H0 spéciﬁe une valeur proche de θM V .

Exercice 9.12
*n
M V = x et, pour tester H0 : λ = λ0 vs.
On a L(λ) = ( i=1 xi !)e−nλ λnx , λ
H1 : λ = λ0 :
RV G =

e−nλ0 λ0nx
, −2 ln RV G = 2n[λ0 − x + x(ln x − ln λ0 )].
e−nx xnx

Étudions la fonction g(u) = λ0 − u + u(ln u − ln λ0 )pour u ∈]0, +∞[. On a
g  (u) = ln u − ln λ0 et est négative pour u < λ0 , positive pour u > λ0 . Quand
u → 0, g(u) → λ0 et quand u → +∞, g(u) → +∞. la fonction décroı̂t donc
de λ0 à 0 à gauche de λ0 et croı̂t de 0 à +∞ à droite. L’inéquation g(u) > k
admet donc comme solutions les valeurs de u à l’extérieur de [c1 , c2 ] où
0 < c1 < λ0 < c2 si k ∈]0, λ0 [ et à l’extérieur de [0, c2 ] où λ0 < c2 si k ≥ λ0 .
Étant donné que le rejet de H0 se fait (approximativement) au niveau α quand
2 (1)
1 2 (1)
−2 ln RV G > χ1−α , c’est-à-dire g(x) > 2n
χ1−α , il y a deux cas de ﬁgure. Soit
2
(1)
1
2n χ1−α < λ0 et le rejet est fondé sur un intervalle de la forme [c1 , c2 ], soit
1 2 (1)
2n χ1−α ≥ λ0 et il est fondé sur un intervalle de la forme [0, c2 ]. Ce dernier

Corrigés des exercices

391

cas se présente si λ0 est trop petit compte tenu de la taille d’échantillon et du
niveau α souhaité pour pouvoir rejeter sur des valeurs de x entre 0 et λ0 . Dans
le premier cas, on a la relation −c1 + c1 (ln c1 − ln λ0 ) = −c2 + c2 (ln c2 − ln λ0 )
résultant du fait que g(c1 ) = g(c2 ).
Application :
2 (1)
Pour λ0 = 5, n = 10 et α = 0,05, on a χ0,95 = 3, 84 et on rejette H0 si
g(x) > 0, 192. Par un logiciel mathématique (ou par approximations successives) on trouve pour g(u) = 0, 192, soit 5 − u + u(ln u − ln 5) = 0, 192, les
n
solutions c1 = 3, 68 et c2 = 6, 45. On rejette donc H0 si i=1 xi < 36, 8 ou
n
n
i=1 xi > 64, 5. Comme
i=1 xi est entier, on rejettera de façon conservatrice


n
n
si i=1 xi ≤ 36 ou i=1 xi ≥ 64.
On peut trouver le niveau exact de la règle ci-dessus dans la mesure où
n
i=1 Xi ; P(nλ0 ) sous H0 . On trouve, par exemple via EXCEL :
n
n
P ( i=1 Xi ≤ 36) = 0, 0238 et P ( i=1 Xi ≥ 65) = 0, 0236
d’où le niveau exact de 0,0474.
Pour randomiser, changeons la borne supérieure pour laquelle il faudrait atteindre une probabilité de 0,0262 pour avoir précisément α = 0, 05. Comme

P ( ni=1 Xi ≥ 64) = 0, 0318 on choisit la limite 65 avec probabilité p et 64 avec
probabilité 1 − p où p×0,0236+(1 − p)×0,0318=0,0262, soit p =0,683.

Exercice 9.13
Soit à tester H0 : μ = μ0 vs. H1 : μ = μ0 avec un échantillon issu de la loi
N (μ , σ 2 ), σ 2 étant inconnu. D’une façon générale, on a :
n

− n
1 
L(μ, σ 2 ) = 2πσ 2 2 exp{− 2
(xi − μ)2 }.
2σ i=1

σ 2 )M V = (x, s'2 ) le dénominateur du RVG est :
Comme (μ,
− n
 n
2π i=1 (xi − x)2 2 − n
L(x, s'2 ) =
e 2.
n
Le numérateur est la maximisation de L(μ, σ 2 ) sur Θ0 = {(μ0 , σ 2 ), σ 2 > 0},
n
soit :

− n
1 
(xi − μ0 )2 }
supL(μ, σ 2 ) = sup 2πσ 2 2 exp{− 2
2
2σ
Θ0
σ
i=1
 n
 n
2 −2
2π i=1 (xi − μ0 )
n
=
e− 2 .
n

392

Statistique - La théorie et ses applications

car le sup sur σ 2 est atteint pour σ 2 =
L(x, s'2 )
=
RV G =
supL(μ, σ 2 )
Or

n

Θ0

1
n

n

i=1 (xi

− μ0 )2 . Ainsi :

 n
− n
(xi − μ0 )2 2
i=1
.
n
2
i=1 (xi − x)

n

− μ0 )2 = i=1 (xi − x)2 + n(x − μ0 )2 , d’où :

− n2
− n

(x − μ0 )2 2
n(x − μ0 )2
=
1
+
.
RV G = 1 + n
2
s'2
i=1 (xi − x)

i=1 (xi

Le test du RVG consiste à rejeter H0 si :


− n

 x − μ0 
(x − μ0 )2 2
(x − μ0 )2


1+
<k ⇔
> k ⇔  √  > k 
s'2
s'2
s'/ n
ce qui est la forme du test de Student. En revanche la région de rejet, pour un
α ﬁxé, donnée par le résultat asymptotique sur le RVG ne coı̈ncide pas avec
celle du test de Student.

Exercices appliqués
Exercice 9.14
La question est de savoir si l’on peut admettre l’eﬃcacité de la nouvelle fabrication (μ > 64 000) avec un risque d’erreur contrôlé. On doit donc tester :
H0 : μ ≤ 64 000 vs. H1 : μ > 64 000.
Soit X la moyenne d’un échantillon de taille 10 de pneus de nouvelle fabrication.
000
√
; N (0 ; 1). On doit rejeter H0 au
Sous H0 (μ = 64 000), Z = 8X−64
000/ 10
approx

niveau de risque α = 0, 05 si l’on observe une valeur z de Z supérieure à
z0,95 = 1, 645.
√ 000 =
Dans l’expérience eﬀectuée on a observé x = 67 300, soit z = 678300−64
000/ 10
1, 30 et on accepte H0 . La méthode ne peut être jugée eﬃcace.
Calculons la puissance du test pour une valeur μ = 65 000. Dans cette alterna000
√
tive on a donc 8X−65
; N (0 ; 1) et, selon la règle de décision utilisée, la
000/ 10
approx

probabilité de rejeter H0 est :


P

X − 64 000
√
> 1, 645
8 000/ 10



=P

1 000
X − 65 000
%
√
> 1, 645 −
8 000/ 10
8 000/ 10)

= P (Z > 1, 25) = 0,106.

De façon plus générale, pour toute valeur de μ, la probabilité de rejet est :


h(μ) = P

μ − 64 000
X − 65 000
√
%
> 1, 645 −
8 000/ 10
8 000/ 10)



=1−Φ

1, 645 −

μ − 64 000
%
8 000/ 10)

Corrigés des exercices

393

où Φ est la fonction de répartition de la loi N (0 ; 1). Pour μ = 67 000 on
trouve h(67 000) = 0, 323, pour μ = 69 000 on trouve h(69 000) = 0, 629, pour
μ = 71 000 on trouve h(71 000) = 0, 869.
Le graphe de la fonction est analogue celui de l’exemple 9.5 : la courbe part
de 0,05 pour μ = 64 000 et croı̂t comme l’indiquent les calculs ci-dessus.

Exercice 9.15
On teste H0 : μ = 69 800 vs. H1 : μ = 69 800. Sous H0 , T =

X−5
√
;
S/ 6 approx

750−69
√ 800
t(499)  N (0 ; 1). T a pris la valeur 68
= −2, 27. On a
10 350/ 500
P (T < −2, 27)  0,012 et s’agissant d’un test bilatéral la P-valeur est 0,024
(ceci pour être en cohérence avec la notion de risque α).

Exercice 9.16
On teste H0 : μ ≥ 5 vs. H1 : μ < 5, l’alternative H1 correspondant à l’aﬃrmation (eau non potable) dont le risque doit être contrôlé. Sous H0 ,
X−5
√
T = S/
; t(5) et l’on doit rejeter H0 au niveau de risque α si T prend
6
une

approx
(5)
valeur t < tα

(5)

= −t1−α .

On a observé x = 4, 9567 et s = 0, 1401, soit t =

4,9567−5
√
0,1401/ 6

= −0, 757. Pour

(5)
t0,05

= −2, 105 et il n’y a donc pas lieu de rejeter H0 . En d’autres
α = 0, 05
termes on ne peut aﬃrmer que l’eau n’est pas potable.

Exercice 9.17
L’hypothèse à tester est que le temps moyen est resté identique, soit :
H0 : μ = 42, 5 vs. H1 : μ = 42, 5.
On rejettera H0 au niveau 0,05 si t =
Ici on a t =

39−42,5
√
8,2/ 30

x−42,5
√
s/ 30

(29)

(29)

∈
/ [−t0,975 , t0,975 ] soit [-2,045 ;2,045].

= −2,34 et l’on rejette H0 .
(29)

Pour α = 0, 01 on a t0,995 = 2, 756 et il n’est pas possible de rejeter H0 à ce
niveau de risque plus faible.

394

Statistique - La théorie et ses applications

Exercice 9.18
L’hypothèse à tester est 2σ ≤ 0,1, ce qui équivaut à H0 : σ 2 ≤ 0,0025 vs. H1 :
2 (5)
5s2
> χ0,95 = 11, 1 et
σ 2 > 0,0025. On rejettera H0 au niveau 0,05 si q = 0,0025
2 (5)

au niveau 0,01 si q > χ0,99 = 15, 1 (voir section 9.7.2).
Sur la base des observations on trouve s2 = 0, 009216, soit q = 18, 43 ce qui
nous permet de rejeter H0 au niveau 0,01. Note : 18,43 est le quantile 0,9975
de la loi χ2 (5) − dans EXCEL 1-LOI.KHIDEUX(18,43 ; 5) − et la P-valeur est
donc 0,0025.

Exercice 9.19
On doit tester H0 : p ≤ 0,04 vs. H1 : p > 0,04 où p est la probabilité qu’une
naissance soit prématurée dans la région considérée. On suppose − ce qui est
assez réaliste − que les observations eﬀectuées sont indépendantes. Comme
np0 = 1 243×0, 04 > 5 et aussi n(1−p0 ) > 5 on peut appliquer l’approximation
gaussienne décrite en section 9.7.5. On rejettera H0 au niveau α si :
p − 0, 04
z=(

(0,04)(0,96)
1243

> z1−α,

où z1−α est le quantile d’ordre 1 − α de la loi N (0 ; 1).
On trouve ici, avec p = 72/1243 =0,05792, z = 3, 22 ce qui correspond à une
P-valeur de 0,0006 (voir la table N (0 ; 1)). On peut donc rejeter au moins au
niveau 0,001. On peut donc aﬃrmer avec un risque très faible d’erreur que la
proportion (théorique) de prématurés est plus élevée dans cette région que dans
le nord de l’Italie en général.
Au niveau 0,01 on est amené à rejeter si z > z0,99 = 2,33 ce qui équivaut à une
réalisation p > 0,0530 de Pn , la proportion de prématurés dans un échantillon
aléatoire de taille n (ici n = 1 243) avec probabilité p qu’un nouveau né soit
prématuré à chaque naissance. Pour calculer la puissance pour une valeur p
donnée, supérieure à 0,04, il faut calculer la probabilité d’avoir une réalisation
de Pn supérieure à 0,053. La fonction puissance est donc :
⎛
⎞
0, 053 − p
0, 053 − p ⎠
) = 1 − Φ⎝ (
h(p) = P (Pn > 0, 053) = P (Z > (
p(1−p)
1 243

p(1−p)
1 243

Corrigés des exercices

395

pour p > 0, 04, où Z ; N (0 ; 1) et Φ est la fonction de répartition de cette
loi. Le graphe de h(p) est une courbe croissante partant de 0,01 pour p = 0, 04
(point non inclus) jusqu’à 1 quand p = 1. En fait h(p) se rapproche très vite
de 1 puisqu’elle dépasse déjà 0,999 pour p = 0, 08. Cela est à relier à la taille
d’échantillon élevée.

Exercice 9.20
On doit tester H0 : p ≤ 0,04 vs. H1 : p > 0,04 où p est la proportion de pièces
défectueuses dan le lot. On applique l’approximation gaussienne car np0 =
800 × 0, 04 > 5 et n(1 − p0 ) = 800 × 0, 96 > 5.
On a trouvé p = 40/800 = 0,05, z =

(0,05−0,04

(0,04)(0,96)
800

= 1, 44 ce qui correspond à

une P-valeur de 0,0749 et ne suﬃt pas pour rejeter H0 ne serait-ce qu’au niveau
0,05 (qui aurait nécessité une valeur de z supérieure à 1,645).

Exercice 9.21
Soit μ1 la moyenne de taux de plomb des ﬁlles du primaire dans la ville et μ2
celle des garçons. On se pose la question de savoir si l’on peut, à faible risque
d’erreur, considérer qu’il y a une diﬀérence en moyenne, sans s’intéresser au
sens de cette diﬀérence. On testera donc H0 : μ1 = μ2 vs. H1 : μ1 = μ2 et on
rejettera H0 au niveau α (voir section 9.7.3) si :
+
,
(n1 − 1)s21 + (n2 − 1)s22
x1 − x2
(n +n −2) (n +n −2)
t= (
∈
/ −t1−1α 2 , t1−1α 2
où s2p =
2
2
n1 + n2 − 2
sp n11 + n12
avec n1 = 32 et n2 = 35. Les tailles d’échantillons sont supérieures à 30 et donc
suﬃsantes pour appliquer la procédure, mais nous devons faire l’hypothèse
supplémentaire (au sens de condition à remplir) que σ12 = σ22 . Toutefois cette
condition n’est pas cruciale comme on l’a indiqué en section 9.7.3 dans la mesure
où les tailles d’échantillons sont proches. Qui plus est, les variances empiriques
semblent indiquer que les variances théoriques sont du même ordre. On a :
31 × 3,39 + 34 × 3,94
12,50 − 12,40
(
= 3,678, sp = 1,918 et t =
= 0,213.
s2p =
65
1,918 1 + 1
32

(65) (65)
[−t0,95 , t0,95 ]

35

On ne peut rejeter H0 au niveau 0,05 car
 [−2,00;2,00]. En
d’autres termes, la diﬀérence observée entre les deux moyennes n’est pas signiﬁcative.

396

Statistique - La théorie et ses applications

Exercice 9.22
Soit μ1 la moyenne de l’an dernier pour l’ensemble des appartements de 3 pièces
en ville et μ2 celle de cette année. Prenons l’hypothèse explicitée dans l’énoncé
comme étant l’hypothèse nulle, ce qui revient à se poser la question de savoir
si l’on peut, à faible risque d’erreur, considérer qu’il y a eu une augmentation
en moyenne. On testera donc H0 : μ1 ≥ μ2 vs. H1 : μ1 < μ2 et on rejettera H0
au niveau 0,05 (voir section 9.7.3) si :
(n1 − 1)s21 + (n2 − 1)s22
x 1 − x2
(58)
< −t0,95 = −1, 67 , où s2p =
t= (
1
n1 + n2 − 2
s
+ 1
p

n1

n2

car une valeur de t trop négative correspond à x1 nettement inférieur à x2 ce
qui va dans le sens de H1 .
Les tailles d’échantillons sont (quasi) suﬃsantes pour appliquer la procédure et
nous supposerons pour l’heure que la condition σ12 = σ22 est remplie. On a :
s2p =

28(26)2 + 30(28)2
325 − 338
(
= 732, sp = 27, 1 et t =
= −1,86
58
1
1
27,1 29
+ 31

ce qui nous amène à rejeter H0 au niveau 0,05, sans plus, la P-valeur étant
de 0,034 (obtenue par EXCEL : LOI.STUDENT(1,86 ; 58 ; 1) ; cette fonction
donne la probabilité d’être au-delà d’une valeur − nécessairement positive −
avec le paramètre 1 ; pour 2 la probabilité est doublée, ce qui correspond à une
situation bilatérale) .
La procédure est applicable si les tailles d’échantillon permettent les approximations gaussiennes, ce qui est le cas ici avec des tailles proches de 30. D’autre
part, la condition σ12 = σ22 n’est pas cruciale du fait que n1  n2 , d’autant plus
que les valeurs des variances observées sont assez voisines. Il faut aussi que les
deux échantillons aient été sélectionnés indépendamment.

Exercice 9.23
Soit μ1 la moyenne théorique (population pathologique virtuelle supposée déﬁnie) du rythme cardiaque avant traitement et μ2 celle après traitement. On
se pose la question de savoir si l’on peut, à faible risque d’erreur, considérer
que le traitement est eﬃcace, soit μ2 > μ1 . On testera donc H0 : μ1 ≥ μ2
vs. H1 : μ1 < μ2 . Il s’agit d’un test apparié (voir en ﬁn de section 9.7.3)
car les mesures sont répétées sur le même échantillon. Ce test porte sur la

Corrigés des exercices

397

moyenne δ des diﬀérences, après moins avant par exemple, ce qui se traduit
par H0 : δ ≤ 0 vs. H1 : δ > 0. On rejettera H0 au niveau α (voir les notations
(n−1)
section 9.7.3) si t = sd /d√n > t1−α , car une valeur élevée (positive) résulte
d’une diﬀérence moyenne observée fortement positive, ce qui va dans le sens de
H1 . Les observations après moins avant sont 4 ; 5,5 ; 3,2 ;1 ; −1 et donnent une
moyenne d = 2, 54, un écart-type sd = 2, 561. D’où t = 2,218.
(4)

Pour α = 0,05 on a t0,95 = 2, 132 ce qui nous amène à rejeter H0 à ce niveau
de risque. On ne peut rejeter au niveau 0,01 car la P-valeur est égale à 0,0454
(EXCEL : LOI.STUDENT(2,218 ; 4 ; 1) ). Si l’on est prêt à prendre un risque
d’erreur de 5% on peut admettre que le traitement est eﬃcace.

Exercice 9.24
Comme pour l’exercice précédent, il s’agit d’un test apparié. Soit μ1 la moyenne
théorique du niveau des ventes dans l’ancien régime et μ2 celle relative au
nouveau régime (sur la population de l’ensemble des vendeurs). Si l’on veut
pouvoir aﬃrmer, avec un faible risque d’erreur, qu’il y a baisse de niveau des
ventes (μ2 < μ1 ) il faut mettre cet état de fait en H1 . On testera donc H0 :
μ1 ≤ μ2 vs. H1 : μ1 > μ2 , soit encore en posant δ = μ2 − μ1 (nouveau moins
ancien) H0 : δ ≥ 0 vs. H1 : δ < 0.
(n−1)

On rejettera H0 au niveau α si t = sd /d√n < −t1−α , car une valeur nettement
négative résulte d’une diﬀérence moyenne observée de même nature, ce qui va
dans le sens de H1 . La moyenne des 16 diﬀérences observées nouveau moins
ancien est trouvée égale à d = −2,315, avec un écart-type sd =10,675. D’où
t = −0,8665. De toute évidence, on ne peut faire l’aﬃrmation mentionnée avec
(15)
une telle valeur de t (−t0,95 = −1,753 et P-valeur  0,2).

Exercice 9.25
L’idée est de voir si, sur la base d’observations, on peut considérer que le
vaccin est eﬃcace, soit, avec des notations évidentes, pA < pB . On testera donc
H0 : pA ≥ pB vs. H1 : pA < pB . On suppose que les deux échantillons ont été
sélectionnés indépendamment. On rejettera H0 si la statistique de test prend
une valeur :
z= )

pA − pB
1
1
p(1 − p)(
+
)
nA
nB

398

Statistique - La théorie et ses applications

trop négative (vis-à-vis de la loi N (0 ; 1) ) car cela résulterait d’une valeur pA
nettement inférieure à pB , ce qui va dans le sens de H1 .
Comme nA pA (1 − pA ) = 32 > 12 et nB pB (1 − pB ) = 48 > 12, on est en droit
d’appliquer la procédure approximative décrite en section 9.7.6. On a pA =
0,20−0,40
0, 20, pB = 0, 40 et p = (40+80)/400 =0,3. D’où z = √
=
(0,30)(0,70)(1/200+1/200)

−4, 36. Cette valeur est très éloignée sur la loi de Gauss N (0 ; 1) et a une
P-valeur inférieure à 0,001 car le quantile d’ordre 0,999 est la valeur 3,10 environ
(voir table). Donc, sans aucun doute, le vaccin est eﬃcace.

Exercice 9.26
Le test est H0 : pA = pB vs. H1 : pA = pB . On rejette H0 si la valeur de z
explicitée à l’exercice précédent est trop éloignée de façon bilatérale sur la loi
N (0 ; 1). On a trouvé ici pA = 0, 5204, pB = 0, 4903 et :
510 + 505
0, 5204 − 0, 4903
p =
= 0,505 , z = %
= 1,35.
980 + 1030
(0,505)(0,495)(1/980 + 1/1030)
/ [−1,96; 1,96]. Donc on ne rejette pas.
Au niveau α = 0, 05, on rejette H0 si z ∈
La diﬀérence des estimations fournies par les deux instituts n’est pas signiﬁcative.
Notons que la procédure approximative est applicable : nA pA (1 − pA ) > 12 et
nB pB (1 − pB ) > 12.

Corrigés des exercices

399

Chapitre 10 : Tests pour variables
catégorielles et tests non paramétriques
Exercice 10.1
Prenons les notations du khi-deux introduites en ﬁn de section 10.2, soit :
- pour la loi 1 : n11 succès et n21 échecs parmi n.1 essais, d’où p1 = nn11
.1
- pour la loi 2 : n12 succès et n22 échecs parmi n.2 essais, d’où p2 = nn12
.2
12
et n = n.1 + n.2 , p = n11 +n
= nn1. . Montrons que z 2 = q.
n
Le numérateur de z est p1 − p2 =

n11
n.1

−

n12
n.2

=

n11 n22 −n12 n21
.
n.1 n.2

Au dénominateur on a p(1 − p) = nn1. (1 − nn1. ) = n1.nn2 2. et n1.1 +
Ainsi :
n(n11 n22 − n12 n21 )2
(
p1 − p2 )2
=
.
z2 =
1
1
n1. n2. n.1 n.2
p(1 − p)( n.1 + n.2 )

1
n.2

=

n
n.1 n.2 .

Pour q, la statistique du test du khi-deux, notons que tous les quatre écarts
entre observé et attendu sont égaux en valeur absolue puisque les deux tableaux
ont les mêmes marges. Prenons donc la première case, on a :


n11 − n1. n.1  = 1 |n11 (n11 + n12 + n21 + n22 ) − (n11 + n12 )(n11 + n21 )| =
n
n
1
|n
n
−
n
n
|
.
11 22
12 21
n
Donc q =

1
n2 (n11 n22

− n12 n21 )2 ( n1.nn.1 +

Comme le dernier facteur est égal à
propriété est démontrée.

n
n1. n.2

+

n
n2. n.1

+

n
n2. n.2 )

n2. n.2 +n2. n.1 +n1. n.2 +n1. n.1
n1. n2. n.1 n.2

=

n2
n1. n2. n.1 n.2 ,

la

La statistique de test Z suivant la loi N (0 ; 1), Z 2 ; χ2 (1) et les deux tests sont
donc équivalents (voir avec plus de précision la section10.1.4). Notons toutefois
que ceci n’est vrai que pour le test avec l’alternative bilatérale p1 = p2 .

Exercice 10.2
Rappelons que les deux variables catégorielles X et Y sont dites indépendantes
si tout événement sur l’une est indépendant de tout événement sur l’autre, un
événement étant un sous-ensemble de catégories.
La condition indiquée dans l’énoncé est évidemment nécessaire puisque chaque
catégorie constitue un événement en soi. Il reste à montrer qu’elle est suﬃsante.

400

Statistique - La théorie et ses applications

D’une façon générale, si A est indépendant de B1 d’une part et de B2 d’autre
part, B1 et B2 étant disjoints, alors A est indépendant de B1 ∪ B2 . Clairement,
comme A ∩ B1 et A ∩ B2 sont disjoints on a :
P (A ∩ (B1 ∪ B2 )) = P ((A ∩ B1 ) ∪ (A ∩ B2 )) = P (A ∩ B1 ) + P (A ∩ B2 )
= P (A)P (B1 ) + P (A)P (B2 ) = P (A) [P (B1 ) + P (B2 )]
= P (A)P (B1 ∪ B2 ).

Dans le contexte de l’énoncé, cela implique que {i1 } étant indépendant de {j1 }
et de {j2 } alors {i1 } est indépendant de {j1 , j2 } et, de proche en proche, {i1 }
est indépendant de {j1 , j2 , · · · , jk } quel que soit le sous-ensemble de catégories
de Y. Par symétrie {i1 , i2 } est indépendant de {j1 , j2 , · · · , jk } et, de proche en
proche, {i1 , i2 , · · · , il } est indépendant de {j1 , j2 , · · · , jk }.

Exercice 10.3
Soient les événements A = (N11 = n11 ), B = (N1. = n1. , N2. = n2. , N.1 = n.1 ,
N.2 = n.2 ), on souhaite évaluer P (A|B) = P (A ∩ B)/P (B).
Pour l’événement A ∩ B, N11 = n11 et les marges sont données, alors le tableau 2 × 2 est entièrement déterminé. Ainsi cet événement est identique à
(N11 = n11 , N12 = n12 , N21 = n21 , N22 = n22 ) à condition de prendre n12 =
n1. − n11 , n21 = n.1 − n11 et n22 = n − (n11 + n12 + n21 ) = n + n11 − n1. − n.1 .
La loi conjointe de (N11 , N12 , N21 , N22 ) est une loi multinomiale de paramètres
p1. p.1 , p1. p.2 , p2. p.1 , p2. p.2 sous l’indépendance d’où (voir section 4.1.6) :
P (A ∩ B) = P (N11 = n11 , N12 = n12 , N21 = n21 , N22 = n22 )
=

n!
(p1. p.1 )n11 (p1. p.2 )n12 (p2. p.1 )n21 (p2. p.2 )n22 .
n11 !n12 !n21 !n22 !

L’événement B est identique à (N1. = n1. , N.1 = n.1 ) car N2. et N.2 sont
alors déterminés. On a N1. ; B(n, p1. ), N.1 ; B(n, p.1 ), avec N1. et N.1
indépendants sous l’hypothèse d’indépendance des deux variables catégorielles.
Donc :
n!
n!
P (B) = P (N1. = n1. , N.1 = n.1 ) =
pn1 . pn2.
pn.1 pn.2 .
n1. !n2. ! 1. 2. n.1 !n.2 ! .1 .2
n1 . n2. n.1 n.2
p2. p.1 p.2 et
En notant que (p1. p.1 )n11 (p1. p.2 )n12 (p2. p.1 )n21 (p2. p.2 )n22 = p1.
en faisant le rapport P (A ∩ B)/P (B) on obtient immédiatement le résultat à
démontrer.

Corrigés des exercices

401

Exercice 10.4
1. D’une façon générale, on a la fonction de vraisemblance relative à la loi
multinomiale avec les 4 catégories du tableau, soit :
L(p11 , p12 , p21 , p22 ) =

n!
pn11 pn12 pn21 pn22 .
n11 !n12 !n21 !n22 ! 11 12 21 22

Sous H0 on obtient, en intégrant les contraintes p21 = p12 , p22 = 1 − p11 − 2p12
(et sachant que n22 = n − n11 − n12 − n21 ) :
L(p11 , p12 ) =

n!
p11 n11 pn1212 +n21 (1 − p11 − 2p12 )n22 .
n11 !n12 !n21 !n22 !

2. Maximisons :
ln L(p11 , p12 ) = n11 ln p11 +(n12 +n21 ) ln p12 +n22 ln(1−p11 −2p12 )+Constante
en annulant les deux dérivées partielles par rapport à p11 et p12 :

"

n22
δ
11
L(p11 , p12 ) = np11
− 1−p11
δp11
−2p12
n12 +n21
2n22
δ
L(p
,
p
)
=
−
11 12
δp12
p12
1−p11 −2p12

.

En annulant on obtient :
n11
n22
n12 + n21
n
=
=
=
p11
1 − p11 − 2p12
2p12
1
d’où les estimations du MV sous H0 : p11 =

n11
n ,

p12 =

n12 +n21
2n

= p21 , p22 =

n22
n .

3. Les fréquences attendues sous H0 sont respectivement :
n
p11 = n11 , n
p12 =

n12 +n21
2

= n
p12 , n
p22 = n22 ,

d’où :
21 2
21 2
(n12 − n12 +n
(n21 − n12 +n
)
)
0
0
2
2
+
+
+
n12 +n21
n12 +n21
n11
n22
2
 2

2
2
(n12 − n21 )
(n21 − n11 )
(n12 − n21 )2
2
+
=
=
.
n12 + n21
4
4
n12 + n21

q=

4. Le nombre de catégories est 4 et il y a 2 paramètres à estimer sous H0 , p11 et
p12 , donc selon le théorème 10.1 (Cramer) les degrés de liberté sont 4−1−2 = 1.
Note : Pour le test du RVG, dont on sait qu’il est asymptotiquement équivalent
au test du khi-deux, on aurait un seul paramètre spécifé par H0 et on retrouve
le même degré de liberté.

402

Statistique - La théorie et ses applications

Exercice 10.5
Raisonnons sur des réalisations en supposant l’absence de valeurs identiques. Si
R(1) prend la valeur r1 , cela signiﬁe qu’il y a r1 − 1 valeurs des Yj qui précèdent
X(1) . Si R(2) prend la valeur r2 , il y a r2 − 2 valeurs des Yi qui précèdent X(2)
et, plus généralement, si R(i) prend la valeur ri , il y a ri − i valeurs des Yj qui
précèdent X(i) .
En section 10.5.4, nous avons déﬁni la statistique U de Mann-Whitney en
déterminant pour chaque Yj le nombre de Xi qui lui sont de valeur supérieure
puis en totalisant sur tous les Yj . De façon équivalente, on peut comptabiliser
pour chaque X(i) le nombre de Yj qui lui sont de valeur inférieure et totaliser.
Dans les notations ci-dessus la réalisation u de U vaut :
n1

1
u = r1 − 1 + r2 − 2 + · · · + rn1 − n1 =
ri − n1 (n1 + 1)
2
i=1
n1
ri est la réalisation de la statistique Tn1 de Wilcoxon fondée sur les
où i=1
Xi .
On a donc bien la relation U = Tn1 − 12 n1 (n1 + 1).

Exercice 10.6
La corrélation de Spearman est la corrélation linéaire usuelle (voir section 3.4)
entre la série des rangs Ri des Xi et la série des rangs Si des Yi . Chaque série
étant constituée des nombres 1 à n, sa somme vaut 12 n(n + 1), sa moyenne
1
(n + 1), la somme des carrés 16 n(n + 1)(2n + 1). On a :
2
n

n
− R)(Si − S)
i=1 Ri Si − nRS
R s = (
=(
.


2 n
2
n
n
n
2
2
( i=1 Ri2 − nR )( i=1 Si2 − nS )
i=1 (Ri − R)
i=1 (Si − S)
i=1 (Ri

n
2
1
n(n2 − 1) et de même
Or i=1 Ri2 − nR = 16 n(n + 1)(2n + 1) − 14 n(n + 1)2 = 12
n
2
2
pour i=1 Si − nS .
Finalement en substituant dans l’expression de Rs ci-dessus on obtient immédiatement le résultat recherché.

Corrigés des exercices

403

Exercices appliqués

Les exercices qui suivent concernent des tests de type khi-deux sur des variables catégorielles. Rapportons-nous encore à EXCEL, disponible pour tous.
Ces tests peuvent être eﬀectués par la fonction TEST.KHIDEUX(plage1 ;plage2)
où la plage 1 contient les eﬀectifs observés et est soit un vecteur, soit un tableau, et plage 2 les eﬀectifs attendus correspondants. C’est à l’utilisateur de
calculer les eﬀectifs attendus, le tableur se prêtant aisément à leur calcul. La
fonction renvoie directement la P-valeur sans indiquer la valeur q prise par la
statistique Q (appelée usuellement le khi-deux). On peut toutefois récupérer q
par la fonction KHIDEUX.INVERSE(P-valeur ; d.l.l.).
Pour un tableau I × J, que ce soit lors d’un test de comparaison de lois
multinomiales ou d’indépendance de deux variables catégorielles, les degrés de
liberté (d.d.l.) sont (I − 1)(J − 1), sauf situations très particulières et rares, et
le calcul de q est identique dans les deux types de situations.
Pour un vecteur à I composantes du test d’adéquation à une loi multinomiale, EXCEL applique des degrés de liberté égaux à I − 1, mais ceci peut être
incorrect lorsqu’on a dû estimer un certain nombre de paramètres de la loi de
référence, car il faut encore soustraire ce nombre. On peut toutefois revenir à la
valeur de q par la fonction KHIDEUX.INVERSE avec I − 1 degrés de liberté et
lire la bonne P-valeur via la fonction LOI.KHIDEUX avec le nombre de degrés
de liberté correct. Nous verrons des exemples de cela.
Pour les tableaux les calculs des eﬀectifs attendus sont simples et répétitifs.
Nous donnerons ces calculs sur un premier exemple et les omettrons par la
suite.

Exercice 10.7
A priori on peut penser que l’on a sélectionné les 202 individus au hasard et
donc que toutes les marges sont aléatoires. Il s’agit alors de tester l’indépendance
entre sexe et niveau de gêne. Toutefois, si la sélection a été faite avec des quotas par sexe, les eﬀectifs 120 et 82 ont été ﬁxés par le plan de sondage et il
s’agit stricto sensu du test de comparaison de la distribution de la variable
gêne entre femmes et hommes, ce qui ne change rien à la procédure du test. Le
tableau des eﬀectifs attendus sous H0 (indépendance gêne/sexe ou identité des
distributions par sexe le cas échéant) est le suivant :

404

Statistique - La théorie et ses applications

PP
P

Sexe
Femmes
P
PP
Gêne
P
Aucune
65,3
Faible
30,9
Moyenne
14,9
Forte
8,9
120
PP

Hommes

Tous

44,7
21,1
10,1
6,1
82

110
52
25
15
202

Rappelons qu’il s’obtient par le produit des marges, par exemple 65,3 =
La valeur prise par la statistique du khi-deux est :
q=

110×120
.
202

(75 − 65, 3)2
(12 − 6, 1)2
+ ··· +
= 16,7.
65, 3
6, 1

Sous l’hypothèse H0 , la statistique de test Q suit asymptotiquement une loi
χ2 (3). Comme aucun eﬀectif attendu n’est inférieur à 5, l’approximation asymp2 (3)
totique est satisfaisante. On rejette ici au niveau α = 0, 05 car q > χ0,95 =7,815
(voir table). On rejette même au niveau de risque très faible de 0,001 puisque
2 (3)
χ0,999 =16,26. De fait, la P-valeur fournie par EXCEL est de 0,0008.
On peut aﬃrmer avec un très faible risque d’erreur qu’il y a un eﬀet sexe pour
la gêne. La comparaison des tableaux observé et attendu (sous H0 ) montre que
les femmes se déclarent moins sensibles à la gêne que les hommes.

Exercice 10.8
Il s’agit d’un test d’adéquation à un certain modèle de loi multinomiale. Les 4
catégories correspondent au nombre de garçons dans une famille de 3 enfants.
Sous l’hypothèse H0 que le sexe d’un nouveau né est indépendant au cours des
naissances successives et que la probabilité p d’avoir un garçon à chaque naissance reste constante, le nombre de garçons X pour une famille de 3 enfants
suit une loi B(3, p). Pour les valeurs x = 0, 1, 2, 3 les probabilités sont respectivement (1 − p)3 , p(1 − p)2 , p2 (1 − p), p3 . Mais p est inconnu et doit être estimé
par l’estimateur du MV qui est la proportion p de garçons dans l’échantillon,
soit :
p =

1
3×825 (0

× 71 + 1 × 297 + 2 × 336 + 3 × 121) =0,5382.

Corrigés des exercices

405

Le tableau ci-après donne les eﬀectifs observés en regard des eﬀectifs attendus.
Nombre de garçons
Eﬀectifs observés
Prob. théoriques estimées
Eﬀectifs attendus

d’où q =

(71−81,26)2
81,26

+

x
ni
pi
n
pi

(297−284,08)2
284,08

0
71
0, 0985
81, 26
+

1
297
0, 3443
284, 08

(336−331,06)2
331,06

+

2
336
0, 4013
331, 06

3
121
0, 1559
128, 60

(121−128,60)2
128,60

= 2, 41.

Sous H0 , Q suit approximativement une loi du khi-deux avec 4-1-1=2 d.d.l.
du fait qu’il a fallu estimer un paramètre (les eﬀectifs attendus étant élevés
l’approximation doit être très satisfaisante). La valeur de q trouvée correspond
à une P-valeur de 0,30 et il n’y a pas lieu de rejeter H0 .
Ici nous rencontrons une situation où EXCEL donne une valeur erronée de la
P-valeur. En eﬀet la fonction TEST.KHIDEUX prend un nombre de d.d.l. égal
à 3, ignorant qu’un paramètre a dû être estimé. La P-valeur donnée est en fait
0,4925 laquelle, par KHIDEUX.INVERSE(0,4925 ; 3), redonne la bonne valeur
de q = 2, 41. Il faut ensuite exécuter LOI.KHIDEUX(2,41 ; 2) pour trouver la
bonne P-valeur.

Exercice 10.9
Cet exercice se fait sur le même modèle que le précédent. La distribution multinomiale est donnée avec 5 catégories. Il est nécessaire d’estimer le paramètre
λ de la loi de Poisson pour estimer les probabilités des catégories :
M V = x = 1 (21 × 0 + 18 × 1 + 7 × 2 + 3 × 3 + 1 × 4) = 0, 90 .
λ
50
Les probabilités théoriques du modèle sous H0 sont calculées selon
D’où le tableau :
Nombre d’accidents
Eﬀectifs observés
Prob. théoriques estimées
Eﬀectifs attendus

x
ni
pi
n
pi

0
21
0, 4066
20, 33

1
18
0, 3659
18, 30

2
7
0, 1647
8, 235

e−0,90 (0,90)x
.
x!

3
3
0, 0494
2, 470

4
1
0, 0135
0, 675

On remarque que pour x=3 et x = 4, les eﬀectifs attendus sont inférieurs à
5. Pour éviter cela, il faut regrouper les 3 dernières catégories. Cette nouvelle
catégorie a pour eﬀectifs observés 11 et attendus 11,38.
La valeur prise par la statistique du khi-deux est :
q=

(18 − 18, 30)2
(11 − 11, 38)2
(21 − 20, 33)2
+
+
= 0, 043.
20, 33
18, 30
11, 38

406

Statistique - La théorie et ses applications

Sous H0 , Q suit approximativement une loi du khi-deux avec 3 − 1 − 1 = 1
2 (1)
d.d.l. et on rejette H0 au niveau 0,05 si q > χ0,95 = 3, 84. On accepte donc
H0 . Note : La P-valeur est 0,836 (voir exercice précédent pour son calcul).

Exercice 10.10
On se rapportera à l’exercice 10.7 pour la procédure qui est ici analogue. On
trouve (via EXCEL, par exemple) une valeur de q égale à 2,78 correspondant à
une P-valeur de 0,43 sur la loi χ2 (3). Il n’a donc pas lieu de rejeter l’hypothèse
d’indépendance.
Sur la base de cette enquête on ne peut considérer qu’il y ait un mode de
logement diﬀérent entre étudiantes et étudiants.

Exercice 10.11
On se rapportera également à l’exercice 10.7 pour la procédure.
On est ici dans une situation de comparaison de 4 distributions multinomiales
car on peut considérer qu’on a constitué 4 échantillons de tailles déﬁnies, respectivement 106, 174, 134 et 76. On trouve (via EXCEL, par exemple) une
valeur de q égale à 32,87 correspondant à une P-valeur inférieure à 10−3 sur la
loi χ2 (6). On peut considérer avec un risque quasi nul de se tromper qu’il y a
une relation entre l’âge et le type d’opérateur choisi.

Exercice 10.12
On se rapportera à l’exercice 10.7 pour la procédure.
On est ici dans une situation de comparaison de 2 distributions multinomiales.
2 (3)
On doit rejeter au niveau 0,05 pour q > χ0,95 = 7, 81. On trouve (via EXCEL,
par exemple) une valeur de q égale à 2,81 (P-valeur=0,42). Il n’y a donc pas
lieu de considérer, sur la base de cette enquête, que la répartition des revenus
soit diﬀérente entre les deux pays.
Note : Un des eﬀectifs attendus vaut 4,44 et est donc inférieur à 5. On a
vu toutefois en ﬁn de section 10.2 que, pour des tableaux autres que 2×2, la
condition n’était pas cruciale. De plus, ici, on est suﬃsamment loin de rejeter
pour se soucier du niveau d’approximation.

Corrigés des exercices

407

Exercice 10.13
Le tableau ci-après donne les éléments utiles pour eﬀectuer le test d’adéquation
du khi-deux (colonnes 2 à 4) et celui de Kolmogorv-Smirnov (colonnes 5 à 8).
Mesure
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48

Eﬀectif
Observé
3
18
81
185
420
749
1073
1079
934
658
370
92
50
21
4
1

Proba.
théorique
0,0010
0,0037
0,0126
0,0348
0,0758
0,1303
0,1779
0,1920
0,1643
0,1112
0,0597
0,0253
0,0086
0,0022
0,0005
0,0001

Eﬀectif
attendu
5,7
21,2
72,3
199,7
434,9
747,7
1020,8
1101,7
942,8
638,1
342,6
145,2
49,3
12,6
2,9
0,6

Eﬀ. Obs.
cumulé
3
21
102
287
707
1456
2529
3608
4542
5200
5570
5662
5712
5733
5737
5738

Fn. Rép.
empirique
0,0005
0,0037
0,0178
0,0500
0,1232
0,2537
0,4407
0,6288
0,7916
0,9062
0,9707
0,9868
0,9955
0,9991
0,9998
1,0000

Fn. Rép.
théorique
0,0010
0,0047
0,0173
0,0521
0,1279
0,2582
0,4361
0,6281
0,7924
0,9036
0,9633
0,9886
0,9972
0,9994
0,9999
1,0000

Diﬀérence
test K-S
-0,0005
-0,0010
0,0005
-0,0021
-0,0047
-0,0045
0,0046
0,0007
-0,0008
0,0026
0,0074
-0,0018
-0,0017
-0,0003
-0,0001
0,0000

La moyenne observée est 39,83 et l’écart-type 2,050. Les probabilités théoriques
sous H0 doivent être estimées sur la loi de X ; N (39,83 ; (2,050)2 ).
Pour la première ligne on calcule P (X < 33, 5) = 0,0010, pour la deuxième
P (33, 5 < X < 34, 5) = 0,0037, etc.
Pour le test du khi-deux il faut regrouper les 3 dernières lignes pour avoir
un eﬀectif attendu supérieur à 5, se ramenant ainsi à 14 classes. Sous H0 (la
répartition est gaussienne) la statistique de test Q a une loi asymptotique du
khi-deux avec 14 − 1 − 2 = 11 d.d.l. car il a fallu estimer moyenne et variance de
la loi. On trouve une valeur q = 36,1 pour cette statistique ce qui correspond à
une P-valeur de 0,00016. On peut rejeter l’hypothèse gaussienne avec un risque
d’erreur inﬁme. Ce résultat ne doit pas surprendre car la taille de l’échantillon
étant grande, la puissance du test est élevée et un modèle plus adaptatif doit
être recherché.
Pour le test de Kolmogorov-Smirnov on obtient une diﬀérence maximale entre
fonctions de répartition empirique et théorique dn = 0,0074. Vu la taille d’échantillon on a, pour la statistique de test Dn , une approximation de la valeur
√
critique au niveau 0,05 qui est 1,36/ n = 0,018. On voit que ce test ne rejette pas l’hypothèse gaussienne. En réalité, il est inadapté car beaucoup trop
conservateur et donc peu puissant, d’une part du fait des gros eﬀectifs sur les

408

Statistique - La théorie et ses applications

classes qui ne permettent pas de suivre l’évolution détaillée de la fonction de
répartition empirique, d’autre part du fait que les écarts sont sous-évalués en
prenant comme référence la loi de Gauss dont la moyenne et la variance sont
estimées sur les données.

Exercice 10.14
En fusionnant les deux échantillons, il y a 23 observations en-dessous de 3 et
24 au-dessus de 3. La médiane étant la valeur 3 on est contraint d’évacuer les
13 observations situées sur 3 qui sont inclassables. On raisonne donc sur une
taille globale réduite à 47 pour laquelle il y a 23 valeurs sous la médiane.
'1 d’observations
Sous H0 (pas de diﬀérence d’attitude) la loi du nombre N
sous la médiane pour le premier échantillon (ménages avec enfant(s)) de taille
25, est la loi H(47 ; 23 ; 25) qui peut être approchée par une loi de Gauss de
moyenne et variance :
μ=

25 × 46
= 12, 234;
2 × 47

σ2 =

25 × 22 × 48
= 2, 988 .
4 × (47)2

'1 sort de l’intervalle 12,234±1,96√2, 988,
Donc on rejettera au niveau 0,05 si N
'1 a pris la valeur 15, on ne peut rejeter l’hypothèse
soit [8,85 ;15,62]. Comme N
d’absence de diﬀérence d’attitude.
La probabilité de dépasser 14,5 sur la loi de Gauss étant égale à 0,095, la
P-valeur est approximativement 0,19. Sur la loi H(47 ; 23 ; 25), la probabilité
'1 ≥ 15) vaut 0,092 (calculable par EXCEL) et la P-valeur exacte est donc
P (N
0,184 ce qui montre que l’approximation est très bonne.
Note : Rappelons que le test de la médiane est peu puissant d’une façon générale
et qu’il l’est particulièrement ici au vu de l’étroitesse de l’échelle et de la taille
des échantillons.

Corrigés des exercices

409

Chapitre 11 : Régression linéaire
Exercice 11.1
Pour p = 2 la formule générale de la densité du vecteur (X, Y ) gaussien est :
!
1
1
t
−1
fX,Y (x, y) =
exp − (u − μ) Σ (u − μ) .
2
2π(det Σ)1/2
où u =

x
y

et μ =

μX 
μY

.

La matrice des variances-covariances est :

2
σX
ρσX σY
Σ=
ρσX σY
σY2
2 2
σY (1 − ρ2 ) et :
d’où det Σ = σX

Σ

−1

1
= 2 2
σX σY (1 − ρ2 )



σY2
−ρσX σY

,

−ρσX σY
2
σX

.

L’expression de fX,Y (x, y) est ﬁnalement (pour plus de détails sur le calcul
matriciel voir l’exercice 3.8) :
2πσX σY

1
%

"
(1 − ρ2 )

exp

−

1
2(1 − ρ2 )



(x − μX )(y − μY )
(y − μY )2
(x − μX )2
− 2ρ
+
2
2
σX
σX σY
σY

#
.

7
6
2
X)
et donc fY |X=x (y) a pour constante
exp − 12 (x−μ
2
σX
1
√
devant l’exponentielle √
et pour l’exponentielle :
2
Par ailleurs fX (x) =

1
σX

2πσY



(1−ρ )

(x − μX )2
(x − μX )(y − μY )
(y − μY )2
1
(x − μX )2
− (1 − ρ2 )
− 2ρ
+
2
2
2
2(1 − ρ )
σX
σX
σX σY
σY2


2
2
1
(x − μX )(y − μY )
(y − μY )
(x − μX )
=−
− 2ρ
+
ρ2
2
2(1 − ρ2 )
σX
σX σY
σY2

!2
1
σY
y − μY + ρ
=− 2
(x − μX )
2σY (1 − ρ2 )
σX



−

Y
ce qui met en évidence une loi de Gauss de moyenne μY + ρ σσX
(x − μX ) et de
2
2
variance σY (1 − ρ ).
Cela montre que dans le modèle de régression de Y sur X la droite de régression
Y
a pour pente ρ σσX
et passe par le point de coordonnées (μX , μY ). De plus, la
variance de Y est réduite du facteur 1 − ρ2 , c’est-à-dire d’autant plus que Y
est liée à X.

410

Statistique - La théorie et ses applications

Exercice 11.2
On a :

 
y

E(ϕ(X)) =
R

R

qui est donc égal à
De la même façon :


R



fX,Y (x, y)
dy fX (x)dx =
yfX,Y (x, y)dxdy = μY
fX (x)
R2

(β0 + β1 x) fX (x)dx = β0 + β1 μX .


E(Xϕ(X)) =
qui est donc égal à
en (β0 , β1 ) :


R

R2

xyfX,Y (x, y)dxdy = E(XY )

x(β0 +β1 x) fX (x)dx = β0 μX +β1 E(X 2 ). D’où à résoudre
"

β0 + β1 μX = μY
.
β0 μX + β1 E(X 2 ) = E(XY )

En multipliant la première équation par −μX et lui ajoutant la suivante, on a :
)
Y
soit β1 = cov(X,Y
= ρ σσX
β1 [E(X 2 ) − μ2X ] = E(XY ) − μX μY
2
σX
et en remplaçant dans la première :
Y
Y
μX et ainsi β0 + β1 x = μY + ρ σσX
(x − μX ).
β0 = μY − ρ σσX
Pour la variance conditionnelle :

E(ψ(X)) =
V (Y |X = x)fX (x)dx
R


2 #
  "
fX,Y (x, y)
σY
2
y − μY + ρ
dy fX (x)dx
(x − μX )
=
σX
fX (x)
R
R

2 #
 "
σY
2
(x − μX )
y − μY + ρ
fX,Y (x, y)dxdy
=
σX
R2


2
σY
2
2
2 σY
= E(Y ) − μY + 2μY ρ
E(X − μX ) + ρ 2 V (X)
σX
σX
= E(Y 2 ) − μ2Y − ρ2 σY2 = σY2 (1 − ρ2 ).
Mais d’autre part, comme V (Y |X = x) ne dépend pas de x on peut dire, à
partir de la première équation ci-dessus, que E(ψ(X)) = V (Y |X = x), ce qui
démontre le résultat donné.
Ainsi on peut conclure que, pour tout modèle à espérance conditionnelle linéaire
et à variance conditionnelle constante, les expressions de ces dernières restent
identiques à celles trouvées dans le cas gaussien.

Corrigés des exercices

411

Exercice 11.3
Tout d’abord, reprenons les formules de la section 11.2.2 donnant β0 et β1 en
raisonnant sur les réalisations yi des Yi :
⎧
β0 = y − β1 x
⎪
⎪
⎨
n
(x − x)(yi − y)
1 = i=1
⎪
n i
β
⎪
2
⎩
i=1 (xi − x)
dont on déduit que yi = β0 + β1 xi = y + β1 (xi − x).
yi − y) découle :
De yi − y = (yi − yi ) + (
n


(yi − y)2 =

i=1

n


(yi − yi )2 +

i=1

n


(
yi − y)2 + 2

i=1

n


(yi − yi )(
yi − y).

i=1

Il suﬃt de montrer que le dernier terme à droite est nul pour avoir la décomposition utilisée en section 11.2.4. On a :
n


(yi − yi )(
yi − y) =

i=1

n


yi (yi − yi ) − y

i=1

n


(yi − yi ),

i=1

or, en substituant l’expression
donnée plus haut,pour yi :
n +
n
i ) = i=1 (yi − y) − β1 (xi − x) = 0
i=1 (yi − y
n
n
car i=1 (yi − y) et i=1 (xi − x) sont nuls. Il reste donc le terme :
n

i=1

yi (yi − yi ) =

n +
,

y + β1 (xi − x) (yi − yi )
i=1
n


=y

(yi − yi ) + β1

i=1



= 0 + β1

= β1

n


(xi − x)(yi − yi )

i=1
n


+

,

(xi − x) yi − y − β1 (xi − x)

i=11
n
n


(xi − x)(yi − y) − β1
(xi − x)2
i=1



i=11

et ce terme est bien nul en vertu de l’équation qui donne β1 .



412

Statistique - La théorie et ses applications

Exercice 11.4
La prévision ponctuelle de Y0 est β0 + β1 x0 . Comme E(Y0 ) = β0 + β1 x0 et que
β0 , β1 sont sans biais, E(Y0 − (β0 + β1 x0 )) = 0.
En utilisant les résultats de la section 11.2.2 sur les espérances, variances et
covariance de β0 et β1 on a :
V (β0 + β1 x0 ) = V (β0 ) + x20 V (β1 ) + 2x0 cov(β0 , β1 )


x2
σ2
xσ 2
1
= σ2
− 2x0 n
+ x20 n
+ n
2
2
2
n
i=1 (xi − x)
i=1 (xi − x)
i=1 (xi − x)


(x0 − x)2
1
= σ2
,
+ n
2
n
i=1 (xi − x)

et comme Y0 est de variance σ 2 et indépendante de β0 + β1 x0 :


(x0 − x)2
1
V (Y0 − (β0 + β1 x0 )) = σ 2 1 + + n
.
2
n
i=1 (xi − x)
et Y0 − (β0 + β1 x0 ) est gaussien.
Selon la proposition 11.1 S 2 est indépendant de β0 + β1 x0 et donc également
2
de Y0 − (β0 + β1 x0 ) et comme (n−2)S
; χ2 (n − 2) :
σ2
Y − (β0 + β1 x0 )
( 0
; t(n − 2).
−x)2
S 1 + n1 +  n(x0(x
2
i −x)
i=1

(n−2)

(n−2)

Cette v.a. peut être encadrée par [−t0,975 , t0,975 ] avec probabilité 0,95 ce qui
conduit immédiatement à l’intervalle de prédiction donné dans l’énoncé.
Notons que l’intervalle est d’autant plus précis que x0 est proche de x.

Exercice 11.5
n
Soit β'1 = i=1 ai Yi un estimateur linéaire quelconque de β1 . Comme E(β'1 ) =
n
n
ai (β0 + β1 xi ) il est sans biais, quels que soient les xi , si
i=1 ai E(Yi ) =
n
n
ni=1
n
2
2
2
i=1 ai = 0 et
i=1 ai xi = 1. Sa variance est
i=1 ai V (Yi ) = σ
i=1 ai .
L’estimateur de variance minimale parmi les estimateurs sans biais est tel que
n
n
n
2
i=1 ai soit minimal sous les contraintes
i=1 ai = 0 et
i=1 ai xi = 1.
Résolvons ce problème par le multiplicateur de Lagrange, soit en minimisant la
n
n
n
fonction de (c1 , c2 , · · · , cn , λ1 , λ2 ) : i=1 a2i − λ1 i=1 ai − λ2 ( i=1 ai xi − 1).

Corrigés des exercices

413

On résout le système suivant annulant les n + 2 dérivées partielles :
⎧
⎪
i − λ1 − λ2 xi = 0 (n équations pour i = 1, · · · , n)
⎨ 2a
n
.
i=1 ai = 0
⎪
⎩ n
a
x
=
1
i=1 i i

La somme des n premières équations donne −nλ1 − λ2 ni=1 xi = 0. En multin
pliant chacune d’elles par xi et en sommant on obtient encore 2 − λ1 i=1 xi −
n
λ2 i=1 x2i = 0. Ces deux équations permettre de déterminer λ1 et λ2 :
"

λ1 = −λ2 x
n
n
2 + λ2 (x i=1 xi − i=1 x2i ) = 0

"
⇐⇒

λ2 =

n

2

i=1 (xi −x)

2

λ1 = −  n

2x
2
i=1 (xi −x)

.

Puis, en substituant dans les n premières équations du système initial :
2x
2xi
− n
=0
2
2
i=1 (xi − x)
i=1 (xi − x)

2ai + n
soit ﬁnalement :

xi − x
.
2
i=1 (xi − x)

ai = n

Ainsi l’estimateur de variance minimale est :
n
n
(xi − x)Yi
(x − x)(Yi − Y )
i=1
n i
=
β'1∗ = i=1
n
2
2
(x
−
x)
i=1 i
i=1 (xi − x)
puisque

n

i=1 (xi − x)Y

= 0. Cet estimateur est bien celui des moindres carrés.

Exercice 11.6
La log-vraisemblance a été vue en section 11.2.2 :
n
n
1 
[yi − (β0 + β1 xi )]2 .
ln L(β0 , β1 , σ 2 ) = − (ln 2π + ln σ 2 ) − 2
2
2σ i=1
Le dénominateur du RVG est obtenu avec les estimations du MV de β0 , β1 et
σ 2 établis dans cette section :
n
n
1 
2 ) = − (ln 2π + ln σ
[yi − (β0 + β1 xi )]2
2) − 2
ln L(β0 , β1 , σ
2
2
σ i=1
n
n
2) −
= − (ln 2π + ln σ
2
2
n
car σ
2 = n1 i=1 [yi − (β0 + β1 xi )]2 .

414

Statistique - La théorie et ses applications

Pour le numérateur, on se place sous H0 : β1 = 0, c’est-à-dire que les Yi sont
i.i.d. d’espérance β0 et de variance σ 2 dont les estimations du MV sont respecn
tivement (voir exemple 6.20) y et s'2 = n1 i=1 (yi − y)2 . La log-vraisemblance
maximale est :
n
n
ln L(β0 , 0, s'2 ) = − (ln 2π + ln s'2 ) − .
2
2
Finalement :
n
(yi − y)2
n
n s'2
.
ln RV G = − ln 2 . = − ln i=1 2
2 σ

2
n
σ
Or la réalisation de la statistique F est (voir section 11.2.4) :

n
(n − 2) ni=1 (
(
yi − y)2
yi − y)2
=
,
f = i=1 2
2
s
n
σ
S 2 ayant été déﬁni comme égal à n
σ 2 /(n−2). Par la décomposition de la somme
des carrés totale on a :
n

i=1

d’où :

(
yi − y)2 =

n


(yi − y)2 −

i=1

f = (n − 2)

n


(yi − yi )2 =

i=1

 n

i=1 (yi −
n
σ2

n


(yi − y)2 − n
σ2

i=1
2

y)



−1

n
f
ln(
+ 1) .
2
n−2
Le RVG est une fonction décroissante de f et la région de rejet de la forme
−2 ln RV G < k correspond à une région f < k  du test classique. Toutefois la loi
asymptotique de −2 ln RV G n’est pas en correspondance avec la loi F (1, n − 2)
du test classique.

et :

ln RV G = −

Tables
Loi
Loi
Loi
Loi

de Gauss
de student
de Fisher
du khi-deux

Ces tables ont été reproduites avec l’aimable autorisation de Denis Labelle
et Alain Latour, de leur ouvrage polycopié Statistique Inférentielle, Université
du Québec à Montréal (UQAM)

416

Statistique - La théorie et ses applications

Loi normale

-4

-3

-2

-1

0

z

1

2

3

4

Loi N (0; 1) : Valeur de Pr {N (0; 1) ≤ z} en fonction de z
z

0,00

0,01

0,02

0,03

0,04

0,05

0,06

0,07

0,08

0,09

0,0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1,0
1,1
1,2
1,3
1,4
1,5
1,6
1,7
1,8
1,9
2,0
2,1
2,2
2,3
2,4
2,5
2,6
2,7
2,8
2,9
3,0
3,1
3,2

,5000
,5398
,5793
,6179
,6554
,6915
,7257
,7580
,7881
,8159
,8413
,8643
,8849
,9032
,9192
,9332
,9452
,9554
,9641
,9713
,9772
,9821
,9861
,9893
,9918
,9938
,9953
,9965
,9974
,9981
,9987
,9990
,9993

,5040
,5438
,5832
,6217
,6591
,6950
,7291
,7611
,7910
,8186
,8438
,8665
,8869
,9049
,9207
,9345
,9463
,9564
,9649
,9719
,9778
,9826
,9864
,9896
,9920
,9940
,9955
,9966
,9975
,9982
,9987
,9991
,9993

,5080
,5478
,5871
,6255
,6628
,6985
,7324
,7642
,7939
,8212
,8461
,8686
,8888
,9066
,9222
,9357
,9474
,9573
,9656
,9726
,9783
,9830
,9868
,9898
,9922
,9941
,9956
,9967
,9976
,9982
,9987
,9991
,9994

,5120
,5517
,5910
,6293
,6664
,7019
,7357
,7673
,7967
,8238
,8485
,8708
,8907
,9082
,9236
,9370
,9484
,9582
,9664
,9732
,9788
,9834
,9871
,9901
,9925
,9943
,9957
,9968
,9977
,9983
,9988
,9991
,9994

,5160
,5557
,5948
,6331
,6700
,7054
,7389
,7704
,7995
,8264
,8508
,8729
,8925
,9099
,9251
,9382
,9495
,9591
,9671
,9738
,9793
,9838
,9875
,9904
,9927
,9945
,9959
,9969
,9977
,9984
,9988
,9992
,9994

,5199
,5596
,5987
,6368
,6736
,7088
,7422
,7734
,8023
,8289
,8531
,8749
,8944
,9115
,9265
,9394
,9505
,9599
,9678
,9744
,9798
,9842
,9878
,9906
,9929
,9946
,9960
,9970
,9978
,9984
,9989
,9992
,9994

,5239
,5636
,6026
,6406
,6772
,7123
,7454
,7764
,8051
,8315
,8554
,8770
,8962
,9131
,9279
,9406
,9515
,9608
,9686
,9750
,9803
,9846
,9881
,9909
,9931
,9948
,9961
,9971
,9979
,9985
,9989
,9992
,9994

,5279
,5675
,6064
,6443
,6808
,7157
,7486
,7794
,8078
,8340
,8577
,8790
,8980
,9147
,9292
,9418
,9525
,9616
,9693
,9756
,9808
,9850
,9884
,9911
,9932
,9949
,9962
,9972
,9979
,9985
,9989
,9992
,9995

,5319
,5714
,6103
,6480
,6844
,7190
,7517
,7823
,8106
,8365
,8599
,8810
,8997
,9162
,9306
,9429
,9535
,9625
,9699
,9761
,9812
,9854
,9887
,9913
,9934
,9951
,9963
,9973
,9980
,9986
,9990
,9993
,9995

,5359
,5753
,6141
,6517
,6879
,7224
,7549
,7852
,8133
,8389
,8621
,8830
,9015
,9177
,9319
,9441
,9545
,9633
,9706
,9767
,9817
,9857
,9890
,9916
,9936
,9952
,9964
,9974
,9981
,9986
,9990
,9993
,9995

Tables

Loi de Student

α/2

-4

ν
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
50
70
90
∞

0,500
1,000
0,816
0,765
0,741
0,727
0,718
0,711
0,706
0,703
0,700
0,697
0,695
0,694
0,692
0,691
0,690
0,689
0,688
0,688
0,687
0,686
0,686
0,685
0,685
0,684
0,684
0,684
0,683
0,683
0,683
0,679
0,678
0,677
0,674

-3

α/2

-2

0,200
3,078
1,886
1,638
1,533
1,476
1,440
1,415
1,397
1,383
1,372
1,363
1,356
1,350
1,345
1,341
1,337
1,333
1,330
1,328
1,325
1,323
1,321
1,319
1,318
1,316
1,315
1,314
1,313
1,311
1,310
1,299
1,294
1,291
1,282

-1

0,100
6,314
2,920
2,353
2,132
2,015
1,943
1,895
1,860
1,833
1,812
1,796
1,782
1,771
1,761
1,753
1,746
1,740
1,734
1,729
1,725
1,721
1,717
1,714
1,711
1,708
1,706
1,703
1,701
1,699
1,697
1,676
1,667
1,662
1,645

0

1

α
0,050
12,706
4,303
3,182
2,776
2,571
2,447
2,365
2,306
2,262
2,228
2,201
2,179
2,160
2,145
2,131
2,120
2,110
2,101
2,093
2,086
2,080
2,074
2,069
2,064
2,060
2,056
2,052
2,048
2,045
2,042
2,009
1,994
1,987
1,960

2

3

0,020
31,821
6,965
4,541
3,747
3,365
3,143
2,998
2,896
2,821
2,764
2,718
2,681
2,650
2,624
2,602
2,583
2,567
2,552
2,539
2,528
2,518
2,508
2,500
2,492
2,485
2,479
2,473
2,467
2,462
2,457
2,403
2,381
2,368
2,326

4

0,010
63,656
9,925
5,841
4,604
4,032
3,707
3,499
3,355
3,250
3,169
3,106
3,055
3,012
2,977
2,947
2,921
2,898
2,878
2,861
2,845
2,831
2,819
2,807
2,797
2,787
2,779
2,771
2,763
2,756
2,750
2,678
2,648
2,632
2,576

417

418

Statistique - La théorie et ses applications

Loi de Fisher

α

0

1

2

3

4

5

6

Pr (Fν1 ,ν2 > c) = 0, 05
ν2
ν1

1

2

3

4

5

6

7

8

9

10

12

15

1
2
3
4
5

161
199
216
225
230

18,5
19,0
19,2
19,2
19,3

10,1
9,55
9,28
9,12
9,01

7,71
6,94
6,59
6,39
6,26

6,61
5,79
5,41
5,19
5,05

5,99
5,14
4,76
4,53
4,39

5,59
4,74
4,35
4,12
3,97

5,32
4,46
4,07
3,84
3,69

5,12
4,26
3,86
3,63
3,48

4,96
4,10
3,71
3,48
3,33

4,75
3,89
3,49
3,26
3,11

4,54
3,68
3,29
3,06
2,90

6
7
8
9
10

234
237
239
241
242

19,3
19,4
19,4
19,4
19,4

8,94
8,89
8,85
8,81
8,79

6,16
6,09
6,04
6,00
5,96

4,95
4,88
4,82
4,77
4,74

4,28
4,21
4,15
4,10
4,06

3,87
3,79
3,73
3,68
3,64

3,58
3,50
3,44
3,39
3,35

3,37
3,29
3,23
3,18
3,14

3,22
3,14
3,07
3,02
2,98

3,00
2,91
2,85
2,80
2,75

2,79
2,71
2,64
2,59
2,54

11
12
13
14
15

243
244
245
245
246

19,4
19,4
19,4
19,4
19,4

8,76
8,74
8,73
8,71
8,70

5,94
5,91
5,89
5,87
5,86

4,70
4,68
4,66
4,64
4,62

4,03
4,00
3,98
3,96
3,94

3,60
3,57
3,55
3,53
3,51

3,31
3,28
3,26
3,24
3,22

3,10
3,07
3,05
3,03
3,01

2,94
2,91
2,89
2,86
2,85

2,72
2,69
2,66
2,64
2,62

2,51
2,48
2,45
2,42
2,40

16
17
18
19
20

246
247
247
248
248

19,4
19,4
19,4
19,4
19,4

8,69
8,68
8,67
8,67
8,66

5,84
5,83
5,82
5,81
5,80

4,60
4,59
4,58
4,57
4,56

3,92
3,91
3,90
3,88
3,87

3,49
3,48
3,47
3,46
3,44

3,20
3,19
3,17
3,16
3,15

2,99
2,97
2,96
2,95
2,94

2,83
2,81
2,80
2,79
2,77

2,60
2,58
2,57
2,56
2,54

2,38
2,37
2,35
2,34
2,33

21
22
23
24
25

248
249
249
249
249

19,4
19,5
19,5
19,5
19,5

8,65
8,65
8,64
8,64
8,63

5,79
5,79
5,78
5,77
5,77

4,55
4,54
4,53
4,53
4,52

3,86
3,86
3,85
3,84
3,83

3,43
3,43
3,42
3,41
3,40

3,14
3,13
3,12
3,12
3,11

2,93
2,92
2,91
2,90
2,89

2,76
2,75
2,75
2,74
2,73

2,53
2,52
2,51
2,51
2,50

2,32
2,31
2,30
2,29
2,28

26
27
28
29
30

249
250
250
250
250

19,5
19,5
19,5
19,5
19,5

8,63
8,63
8,62
8,62
8,62

5,76
5,76
5,75
5,75
5,75

4,52
4,51
4,50
4,50
4,50

3,83
3,82
3,82
3,81
3,81

3,40
3,39
3,39
3,38
3,38

3,10
3,10
3,09
3,08
3,08

2,89
2,88
2,87
2,87
2,86

2,72
2,72
2,71
2,70
2,70

2,49
2,48
2,48
2,47
2,47

2,27
2,27
2,26
2,25
2,25

32
34
36
38
40

250
251
251
251
251

19,5
19,5
19,5
19,5
19,5

8,61
8,61
8,60
8,60
8,59

5,74
5,73
5,73
5,72
5,72

4,49
4,48
4,47
4,47
4,46

3,80
3,79
3,79
3,78
3,77

3,37
3,36
3,35
3,35
3,34

3,07
3,06
3,06
3,05
3,04

2,85
2,85
2,84
2,83
2,83

2,69
2,68
2,67
2,67
2,66

2,46
2,45
2,44
2,43
2,43

2,24
2,23
2,22
2,21
2,20

45
50
55
60
65

251
252
252
252
252

19,5
19,5
19,5
19,5
19,5

8,59
8,58
8,58
8,57
8,57

5,71
5,70
5,69
5,69
5,68

4,45
4,44
4,44
4,43
4,43

3,76
3,75
3,75
3,74
3,73

3,33
3,32
3,31
3,30
3,30

3,03
3,02
3,01
3,01
3,00

2,81
2,80
2,79
2,79
2,78

2,65
2,64
2,63
2,62
2,61

2,41
2,40
2,39
2,38
2,38

2,19
2,18
2,17
2,16
2,15

70
80

252
253

19,5
19,5

8,57
8,56

5,68
5,67

4,42
4,41

3,73
3,72

3,29
3,29

2,99
2,99

2,78
2,77

2,61
2,60

2,37
2,36

2,15
2,14

Tables

419

α

0

1

2

3

4

5

6

Pr (Fν1 ,ν2 > c) = 0, 05
ν2
ν1

18

20

22

24

26

28

30

40

50

60

100

200

1
2
3
4
5

4,41
3,55
3,16
2,93
2,77

4,35
3,49
3,10
2,87
2,71

4,30
3,44
3,05
2,82
2,66

4,26
3,40
3,01
2,78
2,62

4,23
3,37
2,98
2,74
2,59

4,20
3,34
2,95
2,71
2,56

4,17
3,32
2,92
2,69
2,53

4,08
3,23
2,84
2,61
2,45

4,03
3,18
2,79
2,56
2,40

4,00
3,15
2,76
2,53
2,37

3,94
3,09
2,70
2,46
2,31

3,89
3,04
2,65
2,42
2,26

6
7
8
9
10

2,66
2,58
2,51
2,46
2,41

2,60
2,51
2,45
2,39
2,35

2,55
2,46
2,40
2,34
2,30

2,51
2,42
2,36
2,30
2,25

2,47
2,39
2,32
2,27
2,22

2,45
2,36
2,29
2,24
2,19

2,42
2,33
2,27
2,21
2,16

2,34
2,25
2,18
2,12
2,08

2,29
2,20
2,13
2,07
2,03

2,25
2,17
2,10
2,04
1,99

2,19
2,10
2,03
1,97
1,93

2,14
2,06
1,98
1,93
1,88

11
12
13
14
15

2,37
2,34
2,31
2,29
2,27

2,31
2,28
2,25
2,22
2,20

2,26
2,23
2,20
2,17
2,15

2,22
2,18
2,15
2,13
2,11

2,18
2,15
2,12
2,09
2,07

2,15
2,12
2,09
2,06
2,04

2,13
2,09
2,06
2,04
2,01

2,04
2,00
1,97
1,95
1,92

1,99
1,95
1,92
1,89
1,87

1,95
1,92
1,89
1,86
1,84

1,89
1,85
1,82
1,79
1,77

1,84
1,80
1,77
1,74
1,72

16
17
18
19
20

2,25
2,23
2,22
2,20
2,19

2,18
2,17
2,15
2,14
2,12

2,13
2,11
2,10
2,08
2,07

2,09
2,07
2,05
2,04
2,03

2,05
2,03
2,02
2,00
1,99

2,02
2,00
1,99
1,97
1,96

1,99
1,98
1,96
1,95
1,93

1,90
1,89
1,87
1,85
1,84

1,85
1,83
1,81
1,80
1,78

1,82
1,80
1,78
1,76
1,75

1,75
1,73
1,71
1,69
1,68

1,69
1,67
1,66
1,64
1,62

21
22
23
24
25

2,18
2,17
2,16
2,15
2,14

2,11
2,10
2,09
2,08
2,07

2,06
2,05
2,04
2,03
2,02

2,01
2,00
1,99
1,98
1,97

1,98
1,97
1,96
1,95
1,94

1,95
1,93
1,92
1,91
1,91

1,92
1,91
1,90
1,89
1,88

1,83
1,81
1,80
1,79
1,78

1,77
1,76
1,75
1,74
1,73

1,73
1,72
1,71
1,70
1,69

1,66
1,65
1,64
1,63
1,62

1,61
1,60
1,58
1,57
1,56

26
27
28
29
30

2,13
2,13
2,12
2,11
2,11

2,07
2,06
2,05
2,05
2,04

2,01
2,00
2,00
1,99
1,98

1,97
1,96
1,95
1,95
1,94

1,93
1,92
1,91
1,91
1,90

1,90
1,89
1,88
1,88
1,87

1,87
1,86
1,85
1,85
1,84

1,77
1,77
1,76
1,75
1,74

1,72
1,71
1,70
1,69
1,69

1,68
1,67
1,66
1,66
1,65

1,61
1,60
1,59
1,58
1,57

1,55
1,54
1,53
1,52
1,52

32
34
36
38
40

2,10
2,09
2,08
2,07
2,06

2,03
2,02
2,01
2,00
1,99

1,97
1,96
1,95
1,95
1,94

1,93
1,92
1,91
1,90
1,89

1,89
1,88
1,87
1,86
1,85

1,86
1,85
1,84
1,83
1,82

1,83
1,82
1,81
1,80
1,79

1,73
1,72
1,71
1,70
1,69

1,67
1,66
1,65
1,64
1,63

1,64
1,62
1,61
1,60
1,59

1,56
1,55
1,54
1,52
1,52

1,50
1,49
1,48
1,47
1,46

45
50
55
60
65

2,05
2,04
2,03
2,02
2,01

1,98
1,97
1,96
1,95
1,94

1,92
1,91
1,90
1,89
1,88

1,88
1,86
1,85
1,84
1,83

1,84
1,82
1,81
1,80
1,79

1,80
1,79
1,78
1,77
1,76

1,77
1,76
1,75
1,74
1,73

1,67
1,66
1,65
1,64
1,63

1,61
1,60
1,59
1,58
1,57

1,57
1,56
1,55
1,53
1,52

1,49
1,48
1,46
1,45
1,44

1,43
1,41
1,40
1,39
1,37

70
80

2,00
1,99

1,93
1,92

1,88
1,86

1,83
1,82

1,79
1,78

1,75
1,74

1,72
1,71

1,62
1,61

1,56
1,54

1,52
1,50

1,43
1,41

1,36
1,35

420

Statistique - La théorie et ses applications

Loi du khi-deux

α
α
ν

0,995

0,975

0,95

0,9

0,1

0,05

0,025

0,005

1
2
3
4
5

0,000
0,010
0,072
0,207
0,412

0,001
0,051
0,216
0,484
0,831

0,004
0,103
0,352
0,711
1,145

0,016
0,211
0,584
1,064
1,610

2,706
4,605
6,251
7,779
9,236

3,841
5,991
7,815
9,488
11,070

5,024
7,378
9,348
11,143
12,832

7,879
10,597
12,838
14,860
16,750

6
7
8
9
10

0,676
0,989
1,344
1,735
2,156

1,237
1,690
2,180
2,700
3,247

1,635
2,167
2,733
3,325
3,940

2,204
2,833
3,490
4,168
4,865

10,645
12,017
13,362
14,684
15,987

12,592
14,067
15,507
16,919
18,307

14,449
16,013
17,535
19,023
20,483

18,548
20,278
21,955
23,589
25,188

11
12
13
14
15

2,603
3,074
3,565
4,075
4,601

3,816
4,404
5,009
5,629
6,262

4,575
5,226
5,892
6,571
7,261

5,578
6,304
7,041
7,790
8,547

17,275
18,549
19,812
21,064
22,307

19,675
21,026
22,362
23,685
24,996

21,920
23,337
24,736
26,119
27,488

26,757
28,300
29,819
31,319
32,801

16
17
18
19
20

5,142
5,697
6,265
6,844
7,434

6,908
7,564
8,231
8,907
9,591

7,962
8,672
9,390
10,117
10,851

9,312
10,085
10,865
11,651
12,443

23,542
24,769
25,989
27,204
28,412

26,296
27,587
28,869
30,144
31,410

28,845
30,191
31,526
32,852
34,170

34,267
35,718
37,156
38,582
39,997

21
22
23
24
25

8,034
8,643
9,260
9,886
10,520

10,283
10,982
11,689
12,401
13,120

11,591
12,338
13,091
13,848
14,611

13,240
14,041
14,848
15,659
16,473

29,615
30,813
32,007
33,196
34,382

32,671
33,924
35,172
36,415
37,652

35,479
36,781
38,076
39,364
40,646

41,401
42,796
44,181
45,558
46,928

26
27
28
29
30

11,160
11,808
12,461
13,121
13,787

13,844
14,573
15,308
16,047
16,791

15,379
16,151
16,928
17,708
18,493

17,292
18,114
18,939
19,768
20,599

35,563
36,741
37,916
39,087
40,256

38,885
40,113
41,337
42,557
43,773

41,923
43,195
44,461
45,722
46,979

48,290
49,645
50,994
52,335
53,672

40
50
60
70
80

20,707
27,991
35,534
43,275
51,172

24,433
32,357
40,482
48,758
57,153

26,509
34,764
43,188
51,739
60,391

29,051
37,689
46,459
55,329
64,278

51,805
63,167
74,397
85,527
96,578

55,758
67,505
79,082
90,531
101,879

59,342
71,420
83,298
95,023
106,629

66,766
79,490
91,952
104,215
116,321

90
100

59,196
67,328

65,647
74,222

69,126
77,929

73,291
82,358

107,565
118,498

113,145
124,342

118,136
129,561

128,299
140,170

Bibliographie
Agresti A (2002) Categorical data Analysis. Second edition, Wiley, New York
Bosq D, Lecoutre JP (1987) Théorie de l’estimation fonctionnelle. Economica,
Paris
Chap TL (1998) Applied Categorical Analysis. Wiley, New York
Chernoﬀ H, Lehmann EL (1954) The use of maximum likelihood estimates in
χ2 tests for goodness of ﬁt. Annals of Mathematical Statistics 25 : 579-86
Cleveland WS (1979) Robust Locally Weighted Regression. Journal of the American Statistical Association 74 : 829-36
Collomb G (1977) Quelques propriétés de la méthode du noyau pour l’estimation non-paramétrique de la régression en un point ﬁxé. Comptes Rendus de
l’Académie des Sciences de Paris tome 285, série A : 289-92
Cox DR, Hinkley DV (1979) Theoretical Statistics. Chapmann and Hall, London
Cramer H (1946) Mathematical Methods of Statistics. Princeton University
Press
Davison AC, Hinkley DV (1997) Bootstrap Methods and their Application.
Cambridge University Press
Deheuvels P (1977) Estimation Non Paramétrique de la Densité par histogrammes généralisés. Revue de Statistique Appliquée vol. XV : 5-42
Devroye L, Györﬁ L (1985) Nonparametric Density Estimation : The L1 View.
Wiley, New York
Droesbeke JJ, Fine J éd. (1996) Inférence non paramétrique, les statistiques de
rangs. Editions de l’Université de Bruxelles et Ellipses
Droesbeke JJ, Lejeune M, Saporta G éd. (2004) Données Catégorielles. Technip, Paris
Dodge Y (1999) Analyse de régression appliquée. Dunod, Paris
Efron B (1979) Bootstrap methods : another look at the jackknife. Annals of
Statistics 7 : 1-26
Fan J (1993) Local Linear Regression Smoothers and their Minimax Eﬃciency.
Annals of Statistics 21, 1 : 196-216
Friedman D, Diaconis P (1981) On the Histogram as a Density Estimator : L2
Theory. Zeitung fuer Wahrscheinlichkeitstheorie und Verwandte Gebiete 57 :
453-76
Gasser T, Müller HG (1984) Estimating Regression Functions and their Derivatives by the Kernel Method. Scandinavian Journal of Statistics 11 : 171-85
Gibbons JD (1985) Nonparametric Statistical Inference, Second edition. Marcel
Dekker, New York

422

Statistique - La théorie et ses applications

Haerdle W (1990) Applied Nonparametric Regression. Cambridge University
Press
Hodges JL, Lehmann EL (1956) The eﬃciency of some non parametric competitors of the t-test. Annals of Mathematical Statistics 27 : 324-55
Hodges JL, Lehmann EL (1963) Estimates of location based on rank tests.
Annals of Mathematical Statistics 34 : 598-611
Hosmer DW, Lemeshow S (2000) Applied Logistic Regression, Second edition.
Wiley, New York
Kiefer J, Wolfowitz J (1956) Consistency of the maximum likelihood estimator
in the presence of inﬁnitely many nuisance parameters. Annals of Mathematical
Statistics 27 : 887-906
Lecoutre JP, Tassi P (1987) Statistique non paramétrique et robustesse. Economica, Paris
Lehmann EL (1975) Nonparametrics : Statistical Methods based on Ranks.
Holden-Day, San Francisco
Lehmann EL (1986) Testing Statistical Hypotheses, Second edition. Wiley, New
York
Lehmann EL, Casella G. (1998) Theory of Point Estimation. Springer-Verlag,
New York
Lejeune M (1982) Estimation de densité à noyau variable. Rapport technique
No 1, Projet No 2.843-0.80 du Fonds National Suisse de la Recherche Scientiﬁque «Développement et implémentation de méthodes d’estimation non paramétrique»
Lejeune M (1983) Estimation non-paramétrique multivariée par noyaux. Rapport technique No 3, Projet No 2.843-0.80 du FNRS
Lejeune M (1984) Optimization in non Parametric Regression. Compstat 84,
Proceedings in Computational Statistics. Physica-Verlag, Wien : 421-26
Lejeune M (1985) Estimation Non Paramétrique par Noyaux : Régression Polynomiale Mobile. Revue de Statistique Appliquée vol. XXXIII, No 3 : 43-67
Lejeune M, Sarda P (1992) Smooth estimators of distribution and density functions. Computational Statistics & Data Analysis 14 : 451-71
Mann HB, Whitney DR (1947) On a test of whether one of the two random variables is stochastically larger than the other. Annals of Mathematical Statistics
18 : 50-60
Marron JS (1987) A Comparison of Cross-Validation Techniques in Density
Estimation. Annals of Statistics 15 : 152-62
Mosteller F, Tukey JW (1977) Data analysis and regression, a second course
in statistics. Addison-Wesley
Nadaraya EA (1964) On Estimating Regression. Theory of Probability and its
Applications 9 : 141-42

Bibliographie

423

Parzen E (1962) On Estimation of a Probability Density Function and Mode.
Annals of Mathematical Statistics 33 : 1065-76
Quenouille M (1956) Notes on biais in estimation. Biometrika 43 : 353-60
Rosenblatt M (1956) Remarks on Some Nonparamatric Estimates of a Density
Function. Annals of Mathematical Statistics 27 : 832-5
Saporta G (1990) Probabilités, Analyse des Données et Statistique. Technip,
Paris
Seber GAF (1977) Linear Regression Analysis. Wiley, New York
Shao J, Tu D (1995) The Jackknife and Bootstrap. Springer-Verlag, New York
Shao J (1999) Mathematical Statistics. Springer-Verlag, New York
Shapiro S, Wilk M (1965) An analysis of variance test for normality. Biometrika
52 : 591-611
Silverman BW (1987) The bootstrap : To smooth or not to smooth ? Biometrika
74,3 : 469-79
Simonoﬀ JS (1996) Smoothing Techniques in Statistics. Springer-Verlag, New
York
Watson GS (1964) Smooth Regression Analysis. Sankhya A 26 : 359-72
Wilcoxon F (1945) Individual comparisons of grouped data by ranking methods. Biometrics Bulletin 1 : 80-3

Index
A

en probabilité, 80
faible, 80
forte, 80
presque sûre, 80
uniforme, 194
Corrélation
de rangs, 282
de Spearman, 241, 282
linéaire, 34, 240, 300
empirique, 35, 72, 240
BAN, 128, 225
Correction de continuité, 85
Bande de conﬁance, 195
Couple de variables aléatoires, 28–37
Bayes T., 129
gaussien, 43, 240
Bernoulli J., 82
Biais, 70, 72, 99–100, 174, 180, 187, Covariance, 33–36
empirique, 72
191, 197
formule de décentrage, 33
Biweight, 185, 190, 315, 318
Cramer, 268
Bootstrap, 177–181
Cramer-Rao
borne (de), 114–118
inégalité (de), 114, 120
Capture-recapture, 133
Curtose,
19, 170
Caractéristique, 18, 68, 91
Caractère, 47
Centrage, 18, 71
Déviance, 253, 255, 309
Chance, 306
Classe exponentielle, 94–96, 108, 109, de Moivre, 65
Degré de liberté, 72, 74, 76
117, 119, 212, 216, 220
Distance interquartiles, 176
Coeﬃcient d’aplatissement, 19
Distribution, 30
Coeﬃcient d’asymétrie, 19
asymétrique, 60
Coeﬃcient de détermination, 241, 301
bootstrap, 179
Comptage, 54
d’échantillonnage, 69
Contrôle de qualité, 126, 235
d’une population, 68
Convergence, 79–86
empirique, 15, 21
avec probabilité 1, 80
théorique, 15, 21
en loi, 79
Droite de Henri, 271
en moyenne quadratique, 81
A posteriori, 68
A priori, 68
Analyse de variance, 297
modèle (d’), 305, 313
tableau (d’), 299, 302
Approximation
gaussienne, 82–86

B

C

D

425

426

Statistique - La théorie et ses applications

du maximum de vraisemblance, 122,
141, 158, 161, 227, 300
comportement asymptotique, 127
eﬃcace, 115, 117, 125
fonctionnel, 78, 172, 194
e.q.m., 101
minimax, 101
e.q.i.m., 189
sans biais, 99, 103, 110–121
Écart-type, 19, 173
UMVUE, 110–113, 119
de l’échantillon, 70, 173
Estimation,
92
empirique, 70
d’une
densité,
182–192
Échantillon
d’une
fonction
de répartition,
-s appariés, 149, 153, 234, 239, 274,
192–198
278
du bootstrap, 178
-s indépendants, 77, 152, 232
du maximum de vraisemblance, 122,
aléatoire, 37, 67–69
178
loi conjointe, 94
fonctionnelle, 167, 314
réalisé, 68
Étendue, 176, 184
Eﬀectif, 21
Événement, 1
Efron B., 172
complémentaire, 2
EMV, 122
incompatible, 2
Ensemble fondamental, 1
Équation(s) de vraisemblance, 123, 293, Ex aequo, 274, 278
Exhaustivité, 105–110
308
Expérience, 1, 67
Équiprobabilité, 4, 21
Erreur, 290, 293
absolue moyenne, 101
de deuxième espèce, 203
Famille
de première espèce, 203
exponentielle, 94
quadratique moyenne, 100–103, 118,
paramétrique de lois, 92
183
Fenêtre, 184, 315
intégrée, 189
largeur (de), 184, 198
Espérance mathématique, 15–24, 32, Fisher, R.A., 241
40
Fonction
Espace paramétrique, 92
caractéristique, 24, 37
Estimateur, 92
d’un couple de v.a., 32, 81
à noyau, 184, 314
d’un paramètre, 93, 114, 126, 128,
admissible, 101
129, 138
asymptotiquement eﬃcace, 127
d’une v.a., 11, 16, 81
asymptotiquement sans biais, 127
de densité conditionnelle, 30
BAN, 128, 130
de densité conjointe, 28, 41
bayésien, 128–131
de densité de probabilité, 8
convergent, 103–105
de perte, 227
de Hodges-Lehmann, 280
de probabilité, 6
des moindres carrés, 294, 299
de probabilité conditionnelle, 29
des moments, 97, 98, 104
de probabilité conjointe, 28
du jackknife, 174
de régression, 290, 300
Dualité test-IC, 242–244
Durée de vie, 56, 61, 65

E

F

Index
de répartition, 4
empirique, 78, 172, 193
de répartition conditionnelle, 30
de répartition conjointe, 28, 37
de vraisemblance, 122, 208, 220,
293
génératrice, 21–24, 36, 38, 80
gamma, 57
indicatrice, 46, 79
logistique, 306
logit, 306
mesurable, 3, 31
pivot, 138–139
puissance, 213
Formule
de Bayes, 129
de Stirling, 65
Fréchet M., 114
Fréquence, 21, 252
attendue, 254
relative, 21, 82, 84, 151, 181, 182
théorique, 254
Fractile, 11

G
Galton F., 290
Gauss-Markov, 300
Glivenko-Cantelli, 194
Goodness-of-ﬁt, 265
Gosset W.S., 75
Graunt J., 182

H
Histogramme, 169, 182, 189
Homoscédasticité, 300
Hypothèse
alternative, 202
bilatérale, 219–220
multiple, 202, 213–220
nulle, 202, 233
simple, 202
unilatérale, 214–219

I
i.i.d., 38

427

IC, 136
Indépendance, 3, 31, 33, 37
Individu, 67
Information de Fisher, 96, 114, 143
Intégrale de Riemann-Stieltjes, 17, 173
Intervalle de conﬁance, 137, 228
asymptotique, 140–143, 150, 153
largeur, 141, 145, 158
niveau, 136
par le jackknife, 175
pour la diﬀérence de deux moyennes,
147
pour la diﬀérence de deux proportions, 152
pour la médiane, 171
pour le rapport de deux variances,
149
pour un écart-type, 146, 170, 174
pour un quantile, 172
pour une caractéristique quelconque,
175, 178
pour une moyenne, 144, 168
pour une proportion, 150
pour une variance, 146, 170
procédure (d’), 136
conservatrice, 137, 155, 160
convergente, 159
uniformément plus précis, 158
Intervalle de prédiction, 297, 321
Invariance (du MV), 126

J
Jackknife, 173–177

K
Kolmogorov-Smirnov, 195, 266

L
Lehmann E.L., 274
Lissage, 185, 190, 196, 315
Log-vraisemblance, 123, 143
Logiciels, 74, 128, 142, 162, 226, 227
Loi
a posteriori, 129
a priori, 128

428

Statistique - La théorie et ses applications

asymptotique, 80
bêta, 63, 130
binomiale, 47, 53, 65
approximation gaussienne, 85
binomiale négative, 49, 85, 112
conditionnelle, 29, 37
conjointe, 43
continue uniforme, 9, 17, 54, 63
de Bernoulli, 46, 150
de Cauchy, 16, 75, 86, 104
de Fisher, 76
de Gauss, 57–60, 160
centrée-réduite, 41, 57
multivariée, 40, 128
de Gumbel, 62, 97
de Laplace, 25, 125
de Pareto, 61, 142
de Pascal, 49
de Poisson, 6, 51, 140, 156
approximation gaussienne, 85
de Raleigh, 132, 246
de Student, 74, 169
non centrale, 224, 231, 297
de Weibull, 61, 109
des grands nombres, 75, 81, 104
du khi-deux, 72, 85, 162
exponentielle, 22, 55, 61, 111, 139
exponentielle double, 25
géométrique, 23, 49
gamma, 56
hypergéométrique, 50, 85
limite, 80
lognormale, 60, 66
mère, 68, 84
gaussienne, 70
marginale, 29, 37, 43
multinomiale, 51, 252
normale, 57
uniforme discrète, 45
Lowess, 318

M
M-estimateur, 169
Médiane, 11, 169, 170, 176
empirique, 105, 125, 131, 170

Méthode
adaptative, 314
des moments, 96–98
des percentiles (bootstrap), 179
des quantiles, 153–157, 235
studentisée (bootstrap), 179
Marche aléatoire, 44, 64, 88
Matrice
d’information, 120, 128, 143, 162
des corrélations, 37
des variances-covariances, 37,
40–42, 51, 308
semi-déﬁnie positive, 120
Maximum, 77
Maximum de vraisemblance, 121–128,
172, 194
Mesures répétées, 149, 239
Minimum, 77
MISE, 189
Modèle
de localisation, 275
de position, 275
de régression, 289
explicatif, 289
gompit, 313
linéaire, 305
généralisé, 305
logistique, 290
logit, 313
probit, 313
Mode, 62, 181, 197, 198
Moindres carrés, 129, 294, 299
pondérés, 318
Moment, 18–22
croisé, 32
empirique, 72
empirique, 71, 79, 99, 103
centré, 71, 100
théorique, 71, 79, 99
Monte-Carlo (méthode de), 63, 178
Moyenne, 16, 168
α-tronquée, 169, 180
empirique, 38, 69, 81, 84, 168, 173
MV, 123

Index

N
n-échantillon, 68
Neyman J., 206
Neyman-Pearson (lemme de), 208
Niveau d’un test, 203, 213
Niveau de signiﬁcation, 203
Nombres au hasard, 55, 63
Nombres pseudo-aléatoires, 63
Normalité, 145, 148, 271
Noyau, 185, 315
intégré, 196
ordre (du), 191

O
o(.), 24
O(.), 188
Occurrence, 51, 55, 82
Odds ratio, 238, 312
Ondelettes, 192

P
P-valeur, 227
Paramètre
d’une loi, 92
de dimension k, 95, 118–121, 128,
143, 159
de nuisance, 224
de positionnement, 131
dimension, 92
Parzen E., 184
Pearson E.S., 206
Pearson K., 254
Percentile, 11
Phénomène aléatoire, 67
Plan d’expérience, 291, 304
Plus proches voisins, 192
Polygone des fréquences, 184
Population, 21, 38, 67, 168
virtuelle, 68, 232
Précision, 89
absolue, 151
relative, 151
Prédicteur, 289, 303
Probabilité
conditionnelle, 3

429

mesure de, 2, 5, 28
Processus
de Bernoulli, 47–48, 64, 82, 84
de Poisson, 51–56
Proportion, 63, 79, 150, 152, 235, 237
Pseudo-valeurs, 174
Puissance, 204

Q
QQ-plot, 271
Quantile, 9, 154, 170–172, 176, 271
d’une loi de Fisher, 76
Quartile, 13
Quenouille M., 172
Queues de distribution, 75, 169

R
Rééchantillonnage, 170, 172–181, 190
Réalisation, 67–68
Région d’acceptation, 203
Région de conﬁance, 143, 159–162
Région de rejet, 203
Régression
linéaire, 290, 292–305
logistique, 143, 305–313
multiple, 303–305
non paramétrique, 314–319
polynomiale locale, 318–319
simple, 289
Résidu, 294
Rang, 272
moyen, 278
Rapport
de deux v.a. gaussiennes, 75
de sommes de carrés, 77
de variances, 77
Rapport de vraisemblance, 208
généralisé, 220
monotone, 216
Rapport des chances, 238, 312
Reparamétrisation, 55, 93, 114, 117,
126
Risque, 203
alpha, 203
bêta, 203

430

Statistique - La théorie et ses applications

Tables, 74
Taille d’échantillon, 152
Taux de sondage, 39, 51
Tchebichev (inégalité de), 87
Test
bilatéral, 219
conservateur, 206
convergent, 206
d’adéquation, 265
d’ajustement, 264–271
d’homogénéité de populations, 257
Seuil d’un test, 213
d’indépendance, 239, 241, 259–264
Signiﬁcatif, 233, 297, 309
de Hosmer et Lemeshow, 313
Simulation, 63, 68
de Kolmogorov-Smirnov, 266, 270
Somme
de la médiane, 279
de carrés, 74
de localisation (deux lois),
de v.a., 36–38, 40
274–281
Somme des carrés
de Mann-Whitney, 275
des résidus, 294, 295
de McNemar, 239, 283
expliquée, 298
totale, 297
de normalité Shapiro-Wilk, 271
décomposition (de la), 297
de signiﬁcativité, 219
Sondage aléatoire, 38, 47, 67, 150, 239
de Student, 230, 234
avec remise, 39, 51
de Wald, 309
précision, 150
de Wilcoxon, 275, 278
sans remise, 39, 50, 86
des rangs signés, 278
Sondage stratiﬁé, 264
du khi-deux, 254, 256, 259, 265,
Splines, 192
268
Statistique, 68
du rapport de vraisemblance
complète, 113
généralisé, 220–226, 243, 252, 259
d’ordre, 77–78, 109, 272
simple, 208–213
de Pearson, 254, 261
du score, 310
de rang, 272
du signe, 273
de test, 203
exact de Fisher, 238, 262–264
descriptive, 21, 34, 36, 72
le plus puissant, 205, 208
du khi-deux, 254, 256, 258
minimax, 227
exhaustive, 105
plus puissant, 205
exhaustive minimale, 108, 212, 215
pour la diﬀérence de deux moyennes,
Suite de v.a., 37–39
232
i.i.d., 38
pour la diﬀérence de deux proporinﬁnie, 79
tions, 237, 262
Superpopulation, 181
pour le rapport de deux variances,
Support, 9
235
pour un quantile, 274
pour une corrélation, 240, 281
Tableau de contingence, 260
pour une loi multinomiale, 252

de deuxième espèce, 203
de première espèce, 203
Robustesse, 146, 149, 169, 231, 233,
300, 313
Rosenblatt M., 184, 197, 315
RPL, 318
RV, 208
RVG, 220

S

T

Index
pour une médiane, 273
pour une moyenne, 229, 273
pour une proportion, 235, 257
pour une variance, 231
randomisé, 209
sans biais, 204, 210, 214
UMP, 214
uniformément le plus puissant,
214–216
uniformément plus puissant, 214
unilatéral, 214
UPP, 214
UPP-sans biais, 219
Théorème
central limite, 82–86, 140, 145
de factorisation, 106, 124
de Rao-Blackwell, 113
Théorie de la décision, 227
Tirage aléatoire, 21, 34, 38
Tirage au hasard, 3, 21
Tribu borélienne, 5, 28
Tukey J., 172, 185, 197

U
UMVUE, 110
Unités statistiques, 67
Univers, 67, 69

431

V
Valeur
-s critiques, 220
aberrante, 169, 273
extrême, 61, 62, 169, 273
Validation croisée, 190
Variable aléatoire, 1–12
-s échangeables, 272
centrée, 18
centrée-réduite, 83
certaine, 18, 20, 80
fonctionnelle, 78
gaussienne, 57, 59
Variable catégorielle, 51, 251
Variables de contrôle, 149
Variance, 19, 169
de l’échantillon, 70, 73, 84, 86, 100,
169
empirique, 69, 70, 100, 175
pondérée, 148
formule de décentrage, 19, 71
Vecteur aléatoire, 37
gaussien, 41, 42
notation matricielle, 39
Vitesse de convergence, 183

